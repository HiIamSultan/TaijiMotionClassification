Logging terminal output to classification_log.txt...
n_estimators=100
hidden_dim=64, num_layers=3, batch_size=64, learning_rate=0.0001, epochs=200

========== Running Classification on Taiji_dataset_100.csv ==========

Processing Subject 1...
Top 32 discriminative features: [26 46 44 41 38 29 14 12 45 42 25 58 60 37 22 34 40 18 16 33  9 21 30 54
  1 56 52 32 55 28 13 59]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69347
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1685
		Class 12: 1646
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1703
		Class 17: 1655
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1629
	# of Testing Samples: 7740
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 199
		Class 12: 200
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 155
		Class 17: 197
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 189
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 69.91%
Precision: 0.7659 | Recall: 0.6991 | F1-Score: 0.6862


Running Deep Learning Classifier...
DataLoader: Training set - 1084 batches, Testing set - 121 batches
Epoch 1/200, Loss: 2141.4282
Epoch 2/200, Loss: 1182.0428
Epoch 3/200, Loss: 939.3070
Epoch 4/200, Loss: 794.6116
Epoch 5/200, Loss: 699.7710
Epoch 6/200, Loss: 637.3876
Epoch 7/200, Loss: 628.6747
Epoch 8/200, Loss: 621.8517
Epoch 9/200, Loss: 615.5720
Epoch 10/200, Loss: 609.1960
Epoch 11/200, Loss: 603.7531
Epoch 12/200, Loss: 602.8156
Epoch 13/200, Loss: 602.0811
Epoch 14/200, Loss: 601.3951
Epoch 15/200, Loss: 600.8437
Epoch 16/200, Loss: 600.2201
Epoch 17/200, Loss: 600.3564
Epoch 18/200, Loss: 599.9991
Epoch 19/200, Loss: 599.9762
Epoch 20/200, Loss: 600.1515
Epoch 21/200, Loss: 600.0168
Epoch 22/200, Loss: 600.0729
Epoch 23/200, Loss: 599.8988
Epoch 24/200, Loss: 599.8672
Epoch 25/200, Loss: 599.9649
Epoch 26/200, Loss: 599.8886
Epoch 27/200, Loss: 599.8065
Epoch 28/200, Loss: 599.8282
Epoch 29/200, Loss: 600.0594
Epoch 30/200, Loss: 599.8428
Epoch 31/200, Loss: 599.9381
Epoch 32/200, Loss: 599.8390
Epoch 33/200, Loss: 599.8221
Epoch 34/200, Loss: 599.8779
Epoch 35/200, Loss: 599.9292
Epoch 36/200, Loss: 599.9001
Epoch 37/200, Loss: 599.9581
Epoch 38/200, Loss: 599.8460
Epoch 39/200, Loss: 599.8745
Epoch 40/200, Loss: 599.7932
Epoch 41/200, Loss: 599.9119
Epoch 42/200, Loss: 599.9228
Epoch 43/200, Loss: 599.9262
Epoch 44/200, Loss: 599.8587
Epoch 45/200, Loss: 599.8082
Epoch 46/200, Loss: 599.9113
Epoch 47/200, Loss: 599.8017
Epoch 48/200, Loss: 599.8527
Epoch 49/200, Loss: 599.9801
Epoch 50/200, Loss: 599.8423
Epoch 51/200, Loss: 599.8165
Epoch 52/200, Loss: 599.9807
Epoch 53/200, Loss: 599.9166
Epoch 54/200, Loss: 599.8660
Epoch 55/200, Loss: 599.9716
Epoch 56/200, Loss: 599.8263
Epoch 57/200, Loss: 599.9544
Epoch 58/200, Loss: 599.8550
Epoch 59/200, Loss: 599.8862
Epoch 60/200, Loss: 599.8417
Epoch 61/200, Loss: 599.9170
Epoch 62/200, Loss: 599.8807
Epoch 63/200, Loss: 599.8841
Epoch 64/200, Loss: 599.8630
Epoch 65/200, Loss: 599.7890
Epoch 66/200, Loss: 599.9927
Epoch 67/200, Loss: 599.8165
Epoch 68/200, Loss: 599.8330
Epoch 69/200, Loss: 599.9567
Epoch 70/200, Loss: 599.8048
Epoch 71/200, Loss: 599.8396
Epoch 72/200, Loss: 599.9896
Epoch 73/200, Loss: 599.8849
Epoch 74/200, Loss: 599.8947
Epoch 75/200, Loss: 599.8958
Epoch 76/200, Loss: 599.8849
Epoch 77/200, Loss: 599.9600
Epoch 78/200, Loss: 599.9162
Epoch 79/200, Loss: 599.8074
Epoch 80/200, Loss: 599.9136
Epoch 81/200, Loss: 599.7801
Epoch 82/200, Loss: 599.8616
Epoch 83/200, Loss: 599.8563
Epoch 84/200, Loss: 599.9196
Epoch 85/200, Loss: 599.8803
Epoch 86/200, Loss: 599.9417
Epoch 87/200, Loss: 599.9013
Epoch 88/200, Loss: 599.9513
Epoch 89/200, Loss: 599.9281
Epoch 90/200, Loss: 599.7907
Epoch 91/200, Loss: 600.0116
Epoch 92/200, Loss: 599.8936
Epoch 93/200, Loss: 599.9314
Epoch 94/200, Loss: 599.8709
Epoch 95/200, Loss: 599.8252
Epoch 96/200, Loss: 599.9300
Epoch 97/200, Loss: 599.9081
Epoch 98/200, Loss: 599.8712
Epoch 99/200, Loss: 600.0836
Epoch 100/200, Loss: 599.8575
Epoch 101/200, Loss: 599.9536
Epoch 102/200, Loss: 599.9623
Epoch 103/200, Loss: 599.8778
Epoch 104/200, Loss: 599.8760
Epoch 105/200, Loss: 599.8366
Epoch 106/200, Loss: 599.8708
Epoch 107/200, Loss: 599.8439
Epoch 108/200, Loss: 599.8619
Epoch 109/200, Loss: 599.7519
Epoch 110/200, Loss: 599.9734
Epoch 111/200, Loss: 599.8275
Epoch 112/200, Loss: 600.0760
Epoch 113/200, Loss: 599.9734
Epoch 114/200, Loss: 599.8945
Epoch 115/200, Loss: 599.9689
Epoch 116/200, Loss: 599.9341
Epoch 117/200, Loss: 599.8835
Epoch 118/200, Loss: 600.0824
Epoch 119/200, Loss: 599.9364
Epoch 120/200, Loss: 599.8335
Epoch 121/200, Loss: 599.9283
Epoch 122/200, Loss: 599.8450
Epoch 123/200, Loss: 599.9918
Epoch 124/200, Loss: 599.8989
Epoch 125/200, Loss: 599.8809
Epoch 126/200, Loss: 600.0315
Epoch 127/200, Loss: 599.9108
Epoch 128/200, Loss: 599.9622
Epoch 129/200, Loss: 599.8085
Epoch 130/200, Loss: 599.8619
Epoch 131/200, Loss: 599.7639
Epoch 132/200, Loss: 599.7785
Epoch 133/200, Loss: 599.9345
Epoch 134/200, Loss: 599.8926
Epoch 135/200, Loss: 599.8007
Epoch 136/200, Loss: 599.9434
Epoch 137/200, Loss: 599.8867
Epoch 138/200, Loss: 599.9532
Epoch 139/200, Loss: 599.9712
Epoch 140/200, Loss: 599.9220
Epoch 141/200, Loss: 599.8849
Epoch 142/200, Loss: 599.9700
Epoch 143/200, Loss: 600.0066
Epoch 144/200, Loss: 599.8605
Epoch 145/200, Loss: 599.9154
Epoch 146/200, Loss: 599.9648
Epoch 147/200, Loss: 599.9727
Epoch 148/200, Loss: 599.8498
Epoch 149/200, Loss: 599.9270
Epoch 150/200, Loss: 599.8597
Epoch 151/200, Loss: 599.8435
Epoch 152/200, Loss: 599.7406
Epoch 153/200, Loss: 599.9113
Epoch 154/200, Loss: 599.9752
Epoch 155/200, Loss: 599.9375
Epoch 156/200, Loss: 599.8672
Epoch 157/200, Loss: 599.8876
Epoch 158/200, Loss: 599.8544
Epoch 159/200, Loss: 599.9309
Epoch 160/200, Loss: 600.0018
Epoch 161/200, Loss: 599.9793
Epoch 162/200, Loss: 599.9018
Epoch 163/200, Loss: 600.0748
Epoch 164/200, Loss: 599.9045
Epoch 165/200, Loss: 599.8562
Epoch 166/200, Loss: 599.8176
Epoch 167/200, Loss: 599.8213
Epoch 168/200, Loss: 599.8557
Epoch 169/200, Loss: 599.9083
Epoch 170/200, Loss: 599.9486
Epoch 171/200, Loss: 599.8546
Epoch 172/200, Loss: 599.7791
Epoch 173/200, Loss: 599.9211
Epoch 174/200, Loss: 599.8890
Epoch 175/200, Loss: 599.9420
Epoch 176/200, Loss: 600.0564
Epoch 177/200, Loss: 599.7838
Epoch 178/200, Loss: 600.0019
Epoch 179/200, Loss: 599.9134
Epoch 180/200, Loss: 599.9518
Epoch 181/200, Loss: 599.8384
Epoch 182/200, Loss: 599.9312
Epoch 183/200, Loss: 599.8577
Epoch 184/200, Loss: 599.9120
Epoch 185/200, Loss: 599.8556
Epoch 186/200, Loss: 599.8454
Epoch 187/200, Loss: 599.8025
Epoch 188/200, Loss: 599.8929
Epoch 189/200, Loss: 599.8058
Epoch 190/200, Loss: 599.9748
Epoch 191/200, Loss: 599.8042
Epoch 192/200, Loss: 599.8109
Epoch 193/200, Loss: 599.8516
Epoch 194/200, Loss: 599.9816
Epoch 195/200, Loss: 599.8616
Epoch 196/200, Loss: 599.8705
Epoch 197/200, Loss: 599.7987
Epoch 198/200, Loss: 599.8514
Epoch 199/200, Loss: 599.9875
Epoch 200/200, Loss: 599.9745
Train Accuracy: 82.76% | Test Accuracy: 75.71%
Precision: 0.7635 | Recall: 0.7571 | F1-Score: 0.7407
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 28.32%
Precision: 0.2869 | Recall: 0.2832 | F1-Score: 0.2696


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1084 batches, Testing set - 121 batches
Epoch 1/200, Loss: 3445.0618
Epoch 2/200, Loss: 2675.4808
Epoch 3/200, Loss: 2581.2779
Epoch 4/200, Loss: 2533.5382
Epoch 5/200, Loss: 2498.0967
Epoch 6/200, Loss: 2452.5200
Epoch 7/200, Loss: 2448.0838
Epoch 8/200, Loss: 2444.5149
Epoch 9/200, Loss: 2440.8810
Epoch 10/200, Loss: 2437.3771
Epoch 11/200, Loss: 2433.0414
Epoch 12/200, Loss: 2432.2462
Epoch 13/200, Loss: 2431.9978
Epoch 14/200, Loss: 2431.6555
Epoch 15/200, Loss: 2431.2036
Epoch 16/200, Loss: 2430.5719
Epoch 17/200, Loss: 2430.6043
Epoch 18/200, Loss: 2430.5634
Epoch 19/200, Loss: 2430.5820
Epoch 20/200, Loss: 2430.3415
Epoch 21/200, Loss: 2430.4030
Epoch 22/200, Loss: 2430.4941
Epoch 23/200, Loss: 2430.3681
Epoch 24/200, Loss: 2430.3687
Epoch 25/200, Loss: 2430.3822
Epoch 26/200, Loss: 2430.4961
Epoch 27/200, Loss: 2430.3525
Epoch 28/200, Loss: 2430.4883
Epoch 29/200, Loss: 2430.4696
Epoch 30/200, Loss: 2430.4276
Epoch 31/200, Loss: 2430.4805
Epoch 32/200, Loss: 2430.5229
Epoch 33/200, Loss: 2430.3359
Epoch 34/200, Loss: 2430.5040
Epoch 35/200, Loss: 2430.5199
Epoch 36/200, Loss: 2430.3221
Epoch 37/200, Loss: 2430.4668
Epoch 38/200, Loss: 2430.2057
Epoch 39/200, Loss: 2430.4519
Epoch 40/200, Loss: 2430.3458
Epoch 41/200, Loss: 2430.4750
Epoch 42/200, Loss: 2430.4462
Epoch 43/200, Loss: 2430.3479
Epoch 44/200, Loss: 2430.4287
Epoch 45/200, Loss: 2430.4345
Epoch 46/200, Loss: 2430.5355
Epoch 47/200, Loss: 2430.5449
Epoch 48/200, Loss: 2430.5444
Epoch 49/200, Loss: 2430.5143
Epoch 50/200, Loss: 2430.5750
Epoch 51/200, Loss: 2430.2885
Epoch 52/200, Loss: 2430.5942
Epoch 53/200, Loss: 2430.4169
Epoch 54/200, Loss: 2430.4248
Epoch 55/200, Loss: 2430.3339
Epoch 56/200, Loss: 2430.4798
Epoch 57/200, Loss: 2430.4205
Epoch 58/200, Loss: 2430.4834
Epoch 59/200, Loss: 2430.2987
Epoch 60/200, Loss: 2430.4465
Epoch 61/200, Loss: 2430.4962
Epoch 62/200, Loss: 2430.3336
Epoch 63/200, Loss: 2430.2136
Epoch 64/200, Loss: 2430.3219
Epoch 65/200, Loss: 2430.3283
Epoch 66/200, Loss: 2430.4853
Epoch 67/200, Loss: 2430.2698
Epoch 68/200, Loss: 2430.3285
Epoch 69/200, Loss: 2430.3930
Epoch 70/200, Loss: 2430.3857
Epoch 71/200, Loss: 2430.4594
Epoch 72/200, Loss: 2430.3179
Epoch 73/200, Loss: 2430.4034
Epoch 74/200, Loss: 2430.4044
Epoch 75/200, Loss: 2430.4217
Epoch 76/200, Loss: 2430.4838
Epoch 77/200, Loss: 2430.4384
Epoch 78/200, Loss: 2430.5479
Epoch 79/200, Loss: 2430.3563
Epoch 80/200, Loss: 2430.5273
Epoch 81/200, Loss: 2430.3070
Epoch 82/200, Loss: 2430.4601
Epoch 83/200, Loss: 2430.5493
Epoch 84/200, Loss: 2430.4494
Epoch 85/200, Loss: 2430.3710
Epoch 86/200, Loss: 2430.2986
Epoch 87/200, Loss: 2430.3892
Epoch 88/200, Loss: 2430.3443
Epoch 89/200, Loss: 2430.2813
Epoch 90/200, Loss: 2430.3785
Epoch 91/200, Loss: 2430.4154
Epoch 92/200, Loss: 2430.3632
Epoch 93/200, Loss: 2430.3565
Epoch 94/200, Loss: 2430.3551
Epoch 95/200, Loss: 2430.4183
Epoch 96/200, Loss: 2430.4963
Epoch 97/200, Loss: 2430.4130
Epoch 98/200, Loss: 2430.5193
Epoch 99/200, Loss: 2430.3805
Epoch 100/200, Loss: 2430.3152
Epoch 101/200, Loss: 2430.5782
Epoch 102/200, Loss: 2430.3822
Epoch 103/200, Loss: 2430.4468
Epoch 104/200, Loss: 2430.5405
Epoch 105/200, Loss: 2430.5365
Epoch 106/200, Loss: 2430.3753
Epoch 107/200, Loss: 2430.3180
Epoch 108/200, Loss: 2430.4921
Epoch 109/200, Loss: 2430.3712
Epoch 110/200, Loss: 2430.2859
Epoch 111/200, Loss: 2430.3447
Epoch 112/200, Loss: 2430.4101
Epoch 113/200, Loss: 2430.3902
Epoch 114/200, Loss: 2430.3196
Epoch 115/200, Loss: 2430.3458
Epoch 116/200, Loss: 2430.3584
Epoch 117/200, Loss: 2430.4456
Epoch 118/200, Loss: 2430.5615
Epoch 119/200, Loss: 2430.3513
Epoch 120/200, Loss: 2430.3478
Epoch 121/200, Loss: 2430.5430
Epoch 122/200, Loss: 2430.2857
Epoch 123/200, Loss: 2430.3866
Epoch 124/200, Loss: 2430.3675
Epoch 125/200, Loss: 2430.4410
Epoch 126/200, Loss: 2430.2914
Epoch 127/200, Loss: 2430.5478
Epoch 128/200, Loss: 2430.4158
Epoch 129/200, Loss: 2430.3632
Epoch 130/200, Loss: 2430.4224
Epoch 131/200, Loss: 2430.3646
Epoch 132/200, Loss: 2430.2743
Epoch 133/200, Loss: 2430.4523
Epoch 134/200, Loss: 2430.4652
Epoch 135/200, Loss: 2430.4472
Epoch 136/200, Loss: 2430.3403
Epoch 137/200, Loss: 2430.4533
Epoch 138/200, Loss: 2430.4204
Epoch 139/200, Loss: 2430.6099
Epoch 140/200, Loss: 2430.3843
Epoch 141/200, Loss: 2430.4751
Epoch 142/200, Loss: 2430.2588
Epoch 143/200, Loss: 2430.3128
Epoch 144/200, Loss: 2430.3132
Epoch 145/200, Loss: 2430.4497
Epoch 146/200, Loss: 2430.3520
Epoch 147/200, Loss: 2430.3207
Epoch 148/200, Loss: 2430.5379
Epoch 149/200, Loss: 2430.3006
Epoch 150/200, Loss: 2430.3857
Epoch 151/200, Loss: 2430.4462
Epoch 152/200, Loss: 2430.3077
Epoch 153/200, Loss: 2430.4115
Epoch 154/200, Loss: 2430.4131
Epoch 155/200, Loss: 2430.3580
Epoch 156/200, Loss: 2430.3717
Epoch 157/200, Loss: 2430.4560
Epoch 158/200, Loss: 2430.5037
Epoch 159/200, Loss: 2430.3080
Epoch 160/200, Loss: 2430.4697
Epoch 161/200, Loss: 2430.4735
Epoch 162/200, Loss: 2430.3374
Epoch 163/200, Loss: 2430.4213
Epoch 164/200, Loss: 2430.3832
Epoch 165/200, Loss: 2430.4416
Epoch 166/200, Loss: 2430.4215
Epoch 167/200, Loss: 2430.5384
Epoch 168/200, Loss: 2430.3254
Epoch 169/200, Loss: 2430.5596
Epoch 170/200, Loss: 2430.3677
Epoch 171/200, Loss: 2430.3383
Epoch 172/200, Loss: 2430.5357
Epoch 173/200, Loss: 2430.3821
Epoch 174/200, Loss: 2430.3411
Epoch 175/200, Loss: 2430.4772
Epoch 176/200, Loss: 2430.3012
Epoch 177/200, Loss: 2430.3077
Epoch 178/200, Loss: 2430.4938
Epoch 179/200, Loss: 2430.3334
Epoch 180/200, Loss: 2430.5309
Epoch 181/200, Loss: 2430.3422
Epoch 182/200, Loss: 2430.4679
Epoch 183/200, Loss: 2430.3131
Epoch 184/200, Loss: 2430.3974
Epoch 185/200, Loss: 2430.4218
Epoch 186/200, Loss: 2430.5628
Epoch 187/200, Loss: 2430.4629
Epoch 188/200, Loss: 2430.4087
Epoch 189/200, Loss: 2430.4522
Epoch 190/200, Loss: 2430.3776
Epoch 191/200, Loss: 2430.3427
Epoch 192/200, Loss: 2430.5078
Epoch 193/200, Loss: 2430.4853
Epoch 194/200, Loss: 2430.2920
Epoch 195/200, Loss: 2430.3187
Epoch 196/200, Loss: 2430.4866
Epoch 197/200, Loss: 2430.3630
Epoch 198/200, Loss: 2430.3258
Epoch 199/200, Loss: 2430.4312
Epoch 200/200, Loss: 2430.3494
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 28.16% | Test Accuracy: 28.50%
Precision: 0.1909 | Recall: 0.2850 | F1-Score: 0.2062

Processing Subject 2...
Top 32 discriminative features: [26 46 44 41 38 29 14 12 45 58 60 22 18 16 42 25 37  9 21 40 33 54 30 56
 52  1 55 28 48 50 34 59]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69314
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1684
		Class 12: 1658
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1659
		Class 17: 1666
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1618
	# of Testing Samples: 7773
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 200
		Class 12: 188
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 199
		Class 17: 186
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 200
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 62.87%
Precision: 0.7068 | Recall: 0.6287 | F1-Score: 0.6311


Running Deep Learning Classifier...
DataLoader: Training set - 1084 batches, Testing set - 122 batches
Epoch 1/200, Loss: 2099.3496
Epoch 2/200, Loss: 1143.7399
Epoch 3/200, Loss: 902.7739
Epoch 4/200, Loss: 760.1007
Epoch 5/200, Loss: 669.2451
Epoch 6/200, Loss: 606.7290
Epoch 7/200, Loss: 598.2699
Epoch 8/200, Loss: 593.2466
Epoch 9/200, Loss: 584.8668
Epoch 10/200, Loss: 578.7423
Epoch 11/200, Loss: 573.1886
Epoch 12/200, Loss: 572.2108
Epoch 13/200, Loss: 573.6184
Epoch 14/200, Loss: 571.1333
Epoch 15/200, Loss: 570.4427
Epoch 16/200, Loss: 570.2598
Epoch 17/200, Loss: 569.9277
Epoch 18/200, Loss: 570.8350
Epoch 19/200, Loss: 569.5260
Epoch 20/200, Loss: 569.5025
Epoch 21/200, Loss: 569.4161
Epoch 22/200, Loss: 570.7135
Epoch 23/200, Loss: 569.4156
Epoch 24/200, Loss: 570.2325
Epoch 25/200, Loss: 573.5589
Epoch 26/200, Loss: 569.5902
Epoch 27/200, Loss: 572.2428
Epoch 28/200, Loss: 569.9048
Epoch 29/200, Loss: 569.6953
Epoch 30/200, Loss: 569.3826
Epoch 31/200, Loss: 570.7372
Epoch 32/200, Loss: 569.6013
Epoch 33/200, Loss: 569.4006
Epoch 34/200, Loss: 569.5289
Epoch 35/200, Loss: 569.7944
Epoch 36/200, Loss: 570.6073
Epoch 37/200, Loss: 569.9438
Epoch 38/200, Loss: 569.5513
Epoch 39/200, Loss: 570.0941
Epoch 40/200, Loss: 569.3881
Epoch 41/200, Loss: 570.4674
Epoch 42/200, Loss: 571.6334
Epoch 43/200, Loss: 571.2725
Epoch 44/200, Loss: 569.4897
Epoch 45/200, Loss: 569.7577
Epoch 46/200, Loss: 569.7211
Epoch 47/200, Loss: 570.0646
Epoch 48/200, Loss: 571.2654
Epoch 49/200, Loss: 569.9763
Epoch 50/200, Loss: 569.4137
Epoch 51/200, Loss: 570.1647
Epoch 52/200, Loss: 569.6583
Epoch 53/200, Loss: 569.6150
Epoch 54/200, Loss: 569.4622
Epoch 55/200, Loss: 570.0993
Epoch 56/200, Loss: 569.5989
Epoch 57/200, Loss: 569.9700
Epoch 58/200, Loss: 569.4364
Epoch 59/200, Loss: 569.7583
Epoch 60/200, Loss: 570.2776
Epoch 61/200, Loss: 569.6115
Epoch 62/200, Loss: 569.6566
Epoch 63/200, Loss: 570.4053
Epoch 64/200, Loss: 569.8337
Epoch 65/200, Loss: 570.2866
Epoch 66/200, Loss: 569.8236
Epoch 67/200, Loss: 570.0007
Epoch 68/200, Loss: 569.3934
Epoch 69/200, Loss: 570.7289
Epoch 70/200, Loss: 570.1581
Epoch 71/200, Loss: 569.3853
Epoch 72/200, Loss: 569.4894
Epoch 73/200, Loss: 569.3823
Epoch 74/200, Loss: 569.5567
Epoch 75/200, Loss: 570.8418
Epoch 76/200, Loss: 570.9144
Epoch 77/200, Loss: 570.2007
Epoch 78/200, Loss: 569.6075
Epoch 79/200, Loss: 569.4836
Epoch 80/200, Loss: 570.2586
Epoch 81/200, Loss: 569.6364
Epoch 82/200, Loss: 569.6922
Epoch 83/200, Loss: 569.6178
Epoch 84/200, Loss: 569.6371
Epoch 85/200, Loss: 569.8180
Epoch 86/200, Loss: 569.4778
Epoch 87/200, Loss: 570.7784
Epoch 88/200, Loss: 569.3690
Epoch 89/200, Loss: 569.3710
Epoch 90/200, Loss: 569.3786
Epoch 91/200, Loss: 569.6727
Epoch 92/200, Loss: 569.7380
Epoch 93/200, Loss: 570.0536
Epoch 94/200, Loss: 569.5076
Epoch 95/200, Loss: 569.3755
Epoch 96/200, Loss: 569.3655
Epoch 97/200, Loss: 571.3961
Epoch 98/200, Loss: 570.1470
Epoch 99/200, Loss: 570.0087
Epoch 100/200, Loss: 569.4586
Epoch 101/200, Loss: 569.6280
Epoch 102/200, Loss: 569.7794
Epoch 103/200, Loss: 569.3903
Epoch 104/200, Loss: 570.1444
Epoch 105/200, Loss: 569.8842
Epoch 106/200, Loss: 569.6059
Epoch 107/200, Loss: 570.7932
Epoch 108/200, Loss: 570.0097
Epoch 109/200, Loss: 569.6708
Epoch 110/200, Loss: 570.0350
Epoch 111/200, Loss: 569.7240
Epoch 112/200, Loss: 569.9902
Epoch 113/200, Loss: 570.3326
Epoch 114/200, Loss: 569.3900
Epoch 115/200, Loss: 570.5159
Epoch 116/200, Loss: 569.5646
Epoch 117/200, Loss: 569.7216
Epoch 118/200, Loss: 571.6455
Epoch 119/200, Loss: 570.8570
Epoch 120/200, Loss: 569.3651
Epoch 121/200, Loss: 570.8982
Epoch 122/200, Loss: 571.5144
Epoch 123/200, Loss: 569.3922
Epoch 124/200, Loss: 569.3834
Epoch 125/200, Loss: 570.5991
Epoch 126/200, Loss: 570.3720
Epoch 127/200, Loss: 569.4699
Epoch 128/200, Loss: 569.4124
Epoch 129/200, Loss: 569.4292
Epoch 130/200, Loss: 569.9208
Epoch 131/200, Loss: 569.6387
Epoch 132/200, Loss: 570.0699
Epoch 133/200, Loss: 570.1835
Epoch 134/200, Loss: 570.1396
Epoch 135/200, Loss: 569.6705
Epoch 136/200, Loss: 569.3932
Epoch 137/200, Loss: 569.5343
Epoch 138/200, Loss: 570.1501
Epoch 139/200, Loss: 569.5869
Epoch 140/200, Loss: 569.5170
Epoch 141/200, Loss: 570.0403
Epoch 142/200, Loss: 570.4067
Epoch 143/200, Loss: 569.6559
Epoch 144/200, Loss: 569.3687
Epoch 145/200, Loss: 569.5168
Epoch 146/200, Loss: 569.3707
Epoch 147/200, Loss: 571.3715
Epoch 148/200, Loss: 569.3878
Epoch 149/200, Loss: 569.6940
Epoch 150/200, Loss: 569.7118
Epoch 151/200, Loss: 569.4098
Epoch 152/200, Loss: 569.9211
Epoch 153/200, Loss: 570.5135
Epoch 154/200, Loss: 569.3645
Epoch 155/200, Loss: 569.8679
Epoch 156/200, Loss: 570.3623
Epoch 157/200, Loss: 569.4348
Epoch 158/200, Loss: 571.0885
Epoch 159/200, Loss: 571.1118
Epoch 160/200, Loss: 569.4007
Epoch 161/200, Loss: 569.9684
Epoch 162/200, Loss: 569.7061
Epoch 163/200, Loss: 569.3979
Epoch 164/200, Loss: 569.3907
Epoch 165/200, Loss: 569.4973
Epoch 166/200, Loss: 569.7283
Epoch 167/200, Loss: 569.5294
Epoch 168/200, Loss: 569.4243
Epoch 169/200, Loss: 569.3660
Epoch 170/200, Loss: 570.0584
Epoch 171/200, Loss: 569.3834
Epoch 172/200, Loss: 571.7634
Epoch 173/200, Loss: 569.3860
Epoch 174/200, Loss: 569.4289
Epoch 175/200, Loss: 570.3466
Epoch 176/200, Loss: 569.3785
Epoch 177/200, Loss: 569.5554
Epoch 178/200, Loss: 570.0006
Epoch 179/200, Loss: 569.8577
Epoch 180/200, Loss: 569.9571
Epoch 181/200, Loss: 569.3835
Epoch 182/200, Loss: 569.3929
Epoch 183/200, Loss: 569.5591
Epoch 184/200, Loss: 570.4639
Epoch 185/200, Loss: 569.4203
Epoch 186/200, Loss: 569.6771
Epoch 187/200, Loss: 569.3819
Epoch 188/200, Loss: 569.7859
Epoch 189/200, Loss: 569.4094
Epoch 190/200, Loss: 570.4607
Epoch 191/200, Loss: 569.6908
Epoch 192/200, Loss: 571.6192
Epoch 193/200, Loss: 570.1339
Epoch 194/200, Loss: 569.9291
Epoch 195/200, Loss: 569.3973
Epoch 196/200, Loss: 569.4555
Epoch 197/200, Loss: 570.2967
Epoch 198/200, Loss: 570.5003
Epoch 199/200, Loss: 570.3011
Epoch 200/200, Loss: 570.3370
Train Accuracy: 84.06% | Test Accuracy: 63.48%
Precision: 0.6317 | Recall: 0.6348 | F1-Score: 0.6099
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 19.73%
Precision: 0.2044 | Recall: 0.1973 | F1-Score: 0.1941


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1084 batches, Testing set - 122 batches
Epoch 1/200, Loss: 3516.2299
Epoch 2/200, Loss: 2974.2117
Epoch 3/200, Loss: 2858.4055
Epoch 4/200, Loss: 2798.7347
Epoch 5/200, Loss: 2765.2817
Epoch 6/200, Loss: 2713.6646
Epoch 7/200, Loss: 2709.3268
Epoch 8/200, Loss: 2705.4125
Epoch 9/200, Loss: 2701.3368
Epoch 10/200, Loss: 2698.4322
Epoch 11/200, Loss: 2693.0485
Epoch 12/200, Loss: 2695.2539
Epoch 13/200, Loss: 2692.0919
Epoch 14/200, Loss: 2692.6606
Epoch 15/200, Loss: 2690.4772
Epoch 16/200, Loss: 2691.2264
Epoch 17/200, Loss: 2690.6497
Epoch 18/200, Loss: 2690.4979
Epoch 19/200, Loss: 2690.5827
Epoch 20/200, Loss: 2690.0818
Epoch 21/200, Loss: 2689.9789
Epoch 22/200, Loss: 2690.2458
Epoch 23/200, Loss: 2689.8055
Epoch 24/200, Loss: 2690.0526
Epoch 25/200, Loss: 2689.3657
Epoch 26/200, Loss: 2689.7572
Epoch 27/200, Loss: 2691.5162
Epoch 28/200, Loss: 2690.1513
Epoch 29/200, Loss: 2691.2797
Epoch 30/200, Loss: 2689.0961
Epoch 31/200, Loss: 2689.9091
Epoch 32/200, Loss: 2689.4578
Epoch 33/200, Loss: 2691.1511
Epoch 34/200, Loss: 2690.4499
Epoch 35/200, Loss: 2690.6585
Epoch 36/200, Loss: 2689.5804
Epoch 37/200, Loss: 2690.5717
Epoch 38/200, Loss: 2690.0942
Epoch 39/200, Loss: 2690.4064
Epoch 40/200, Loss: 2690.3337
Epoch 41/200, Loss: 2691.0355
Epoch 42/200, Loss: 2690.4314
Epoch 43/200, Loss: 2689.2691
Epoch 44/200, Loss: 2690.3283
Epoch 45/200, Loss: 2690.3146
Epoch 46/200, Loss: 2691.9835
Epoch 47/200, Loss: 2689.9675
Epoch 48/200, Loss: 2690.8874
Epoch 49/200, Loss: 2689.8194
Epoch 50/200, Loss: 2689.7642
Epoch 51/200, Loss: 2692.2338
Epoch 52/200, Loss: 2689.8017
Epoch 53/200, Loss: 2689.7468
Epoch 54/200, Loss: 2690.3530
Epoch 55/200, Loss: 2690.6011
Epoch 56/200, Loss: 2690.3880
Epoch 57/200, Loss: 2693.2487
Epoch 58/200, Loss: 2690.3947
Epoch 59/200, Loss: 2691.6617
Epoch 60/200, Loss: 2689.1136
Epoch 61/200, Loss: 2689.4574
Epoch 62/200, Loss: 2689.4682
Epoch 63/200, Loss: 2690.2811
Epoch 64/200, Loss: 2689.3716
Epoch 65/200, Loss: 2690.2568
Epoch 66/200, Loss: 2689.2890
Epoch 67/200, Loss: 2690.5874
Epoch 68/200, Loss: 2690.4681
Epoch 69/200, Loss: 2689.9750
Epoch 70/200, Loss: 2691.3528
Epoch 71/200, Loss: 2689.5517
Epoch 72/200, Loss: 2690.3426
Epoch 73/200, Loss: 2690.0431
Epoch 74/200, Loss: 2689.8806
Epoch 75/200, Loss: 2691.0950
Epoch 76/200, Loss: 2691.0637
Epoch 77/200, Loss: 2689.7980
Epoch 78/200, Loss: 2689.9412
Epoch 79/200, Loss: 2689.7231
Epoch 80/200, Loss: 2691.7093
Epoch 81/200, Loss: 2690.5177
Epoch 82/200, Loss: 2689.9881
Epoch 83/200, Loss: 2691.4088
Epoch 84/200, Loss: 2689.6921
Epoch 85/200, Loss: 2689.9878
Epoch 86/200, Loss: 2689.8922
Epoch 87/200, Loss: 2689.9466
Epoch 88/200, Loss: 2691.2411
Epoch 89/200, Loss: 2690.5809
Epoch 90/200, Loss: 2689.5022
Epoch 91/200, Loss: 2690.2217
Epoch 92/200, Loss: 2691.7984
Epoch 93/200, Loss: 2690.1978
Epoch 94/200, Loss: 2689.7968
Epoch 95/200, Loss: 2689.3822
Epoch 96/200, Loss: 2690.8936
Epoch 97/200, Loss: 2689.7545
Epoch 98/200, Loss: 2689.3723
Epoch 99/200, Loss: 2690.2655
Epoch 100/200, Loss: 2690.3190
Epoch 101/200, Loss: 2690.3457
Epoch 102/200, Loss: 2690.7272
Epoch 103/200, Loss: 2689.7758
Epoch 104/200, Loss: 2690.8119
Epoch 105/200, Loss: 2689.9115
Epoch 106/200, Loss: 2690.9439
Epoch 107/200, Loss: 2690.0954
Epoch 108/200, Loss: 2689.6146
Epoch 109/200, Loss: 2690.1131
Epoch 110/200, Loss: 2689.8722
Epoch 111/200, Loss: 2691.1017
Epoch 112/200, Loss: 2690.1027
Epoch 113/200, Loss: 2690.6780
Epoch 114/200, Loss: 2689.7873
Epoch 115/200, Loss: 2690.1558
Epoch 116/200, Loss: 2690.1558
Epoch 117/200, Loss: 2690.2973
Epoch 118/200, Loss: 2689.7233
Epoch 119/200, Loss: 2689.2164
Epoch 120/200, Loss: 2690.3097
Epoch 121/200, Loss: 2689.6868
Epoch 122/200, Loss: 2689.8346
Epoch 123/200, Loss: 2689.8893
Epoch 124/200, Loss: 2689.0828
Epoch 125/200, Loss: 2691.2048
Epoch 126/200, Loss: 2690.0704
Epoch 127/200, Loss: 2691.5164
Epoch 128/200, Loss: 2690.2952
Epoch 129/200, Loss: 2690.4710
Epoch 130/200, Loss: 2690.0762
Epoch 131/200, Loss: 2689.8469
Epoch 132/200, Loss: 2690.5186
Epoch 133/200, Loss: 2691.1406
Epoch 134/200, Loss: 2690.3313
Epoch 135/200, Loss: 2689.9514
Epoch 136/200, Loss: 2691.4413
Epoch 137/200, Loss: 2690.5270
Epoch 138/200, Loss: 2689.5597
Epoch 139/200, Loss: 2690.4600
Epoch 140/200, Loss: 2689.6328
Epoch 141/200, Loss: 2690.5997
Epoch 142/200, Loss: 2689.5484
Epoch 143/200, Loss: 2689.8387
Epoch 144/200, Loss: 2689.9970
Epoch 145/200, Loss: 2689.5665
Epoch 146/200, Loss: 2690.8120
Epoch 147/200, Loss: 2689.9775
Epoch 148/200, Loss: 2690.5765
Epoch 149/200, Loss: 2689.9288
Epoch 150/200, Loss: 2688.7761
Epoch 151/200, Loss: 2690.5771
Epoch 152/200, Loss: 2690.8829
Epoch 153/200, Loss: 2689.7072
Epoch 154/200, Loss: 2690.0477
Epoch 155/200, Loss: 2689.7650
Epoch 156/200, Loss: 2690.4764
Epoch 157/200, Loss: 2689.0883
Epoch 158/200, Loss: 2691.0981
Epoch 159/200, Loss: 2689.9290
Epoch 160/200, Loss: 2690.2936
Epoch 161/200, Loss: 2689.3080
Epoch 162/200, Loss: 2689.8738
Epoch 163/200, Loss: 2690.0866
Epoch 164/200, Loss: 2690.1353
Epoch 165/200, Loss: 2689.5900
Epoch 166/200, Loss: 2689.9565
Epoch 167/200, Loss: 2690.1835
Epoch 168/200, Loss: 2690.4354
Epoch 169/200, Loss: 2690.4181
Epoch 170/200, Loss: 2690.4588
Epoch 171/200, Loss: 2690.0615
Epoch 172/200, Loss: 2689.5972
Epoch 173/200, Loss: 2690.1522
Epoch 174/200, Loss: 2690.3332
Epoch 175/200, Loss: 2689.7211
Epoch 176/200, Loss: 2690.0863
Epoch 177/200, Loss: 2690.5941
Epoch 178/200, Loss: 2689.5917
Epoch 179/200, Loss: 2690.8995
Epoch 180/200, Loss: 2689.3040
Epoch 181/200, Loss: 2690.3313
Epoch 182/200, Loss: 2690.8075
Epoch 183/200, Loss: 2690.9064
Epoch 184/200, Loss: 2690.3908
Epoch 185/200, Loss: 2689.7672
Epoch 186/200, Loss: 2689.2719
Epoch 187/200, Loss: 2690.0736
Epoch 188/200, Loss: 2689.6196
Epoch 189/200, Loss: 2688.9696
Epoch 190/200, Loss: 2691.0452
Epoch 191/200, Loss: 2690.2769
Epoch 192/200, Loss: 2689.1441
Epoch 193/200, Loss: 2690.1122
Epoch 194/200, Loss: 2690.8829
Epoch 195/200, Loss: 2690.1067
Epoch 196/200, Loss: 2690.1797
Epoch 197/200, Loss: 2690.7443
Epoch 198/200, Loss: 2689.0299
Epoch 199/200, Loss: 2689.2167
Epoch 200/200, Loss: 2688.5544
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 25.06% | Test Accuracy: 18.77%
Precision: 0.1721 | Recall: 0.1877 | F1-Score: 0.1592

Processing Subject 3...
Top 32 discriminative features: [26 46 44 38 41 29 14 12 45 22 58 60 42 18 16  9 40 21 54 56 33 25 37 52
  1 30 55 34 28 59 48 10]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69381
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1684
		Class 12: 1660
		Class 13: 1789
		Class 14: 1800
		Class 15: 1800
		Class 16: 1658
		Class 17: 1652
		Class 18: 1765
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1800
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1780
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1652
	# of Testing Samples: 7706
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 200
		Class 12: 186
		Class 13: 200
		Class 14: 200
		Class 15: 193
		Class 16: 200
		Class 17: 200
		Class 18: 178
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 189
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 194
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 166
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 66.92%
Precision: 0.7806 | Recall: 0.6692 | F1-Score: 0.6818


Running Deep Learning Classifier...
DataLoader: Training set - 1085 batches, Testing set - 121 batches
Epoch 1/200, Loss: 2104.2862
Epoch 2/200, Loss: 1055.6599
Epoch 3/200, Loss: 836.7454
Epoch 4/200, Loss: 711.1030
Epoch 5/200, Loss: 627.4817
Epoch 6/200, Loss: 571.3845
Epoch 7/200, Loss: 563.0645
Epoch 8/200, Loss: 557.9989
Epoch 9/200, Loss: 550.6041
Epoch 10/200, Loss: 545.1342
Epoch 11/200, Loss: 539.4559
Epoch 12/200, Loss: 538.4876
Epoch 13/200, Loss: 537.8167
Epoch 14/200, Loss: 537.2157
Epoch 15/200, Loss: 536.7087
Epoch 16/200, Loss: 535.9507
Epoch 17/200, Loss: 536.0127
Epoch 18/200, Loss: 535.9291
Epoch 19/200, Loss: 536.1524
Epoch 20/200, Loss: 535.6816
Epoch 21/200, Loss: 535.5717
Epoch 22/200, Loss: 535.8106
Epoch 23/200, Loss: 535.7954
Epoch 24/200, Loss: 535.6381
Epoch 25/200, Loss: 535.8024
Epoch 26/200, Loss: 535.7243
Epoch 27/200, Loss: 535.8488
Epoch 28/200, Loss: 535.8298
Epoch 29/200, Loss: 535.8911
Epoch 30/200, Loss: 536.1613
Epoch 31/200, Loss: 535.7128
Epoch 32/200, Loss: 535.6876
Epoch 33/200, Loss: 535.6965
Epoch 34/200, Loss: 535.6878
Epoch 35/200, Loss: 535.6100
Epoch 36/200, Loss: 536.8043
Epoch 37/200, Loss: 535.5527
Epoch 38/200, Loss: 535.9938
Epoch 39/200, Loss: 535.6459
Epoch 40/200, Loss: 535.8082
Epoch 41/200, Loss: 535.7818
Epoch 42/200, Loss: 535.6861
Epoch 43/200, Loss: 536.0084
Epoch 44/200, Loss: 535.8956
Epoch 45/200, Loss: 535.8501
Epoch 46/200, Loss: 535.4915
Epoch 47/200, Loss: 535.7631
Epoch 48/200, Loss: 535.9597
Epoch 49/200, Loss: 535.8645
Epoch 50/200, Loss: 535.6347
Epoch 51/200, Loss: 535.7262
Epoch 52/200, Loss: 535.4826
Epoch 53/200, Loss: 535.8304
Epoch 54/200, Loss: 535.9632
Epoch 55/200, Loss: 536.2048
Epoch 56/200, Loss: 535.4965
Epoch 57/200, Loss: 536.2957
Epoch 58/200, Loss: 535.7423
Epoch 59/200, Loss: 535.8172
Epoch 60/200, Loss: 535.7955
Epoch 61/200, Loss: 536.3253
Epoch 62/200, Loss: 536.7503
Epoch 63/200, Loss: 537.1026
Epoch 64/200, Loss: 535.9684
Epoch 65/200, Loss: 536.1059
Epoch 66/200, Loss: 535.5359
Epoch 67/200, Loss: 536.2630
Epoch 68/200, Loss: 535.9600
Epoch 69/200, Loss: 535.7976
Epoch 70/200, Loss: 535.7511
Epoch 71/200, Loss: 535.8327
Epoch 72/200, Loss: 536.4539
Epoch 73/200, Loss: 536.1582
Epoch 74/200, Loss: 536.2491
Epoch 75/200, Loss: 535.8259
Epoch 76/200, Loss: 535.5683
Epoch 77/200, Loss: 536.2418
Epoch 78/200, Loss: 535.6005
Epoch 79/200, Loss: 535.7836
Epoch 80/200, Loss: 535.7493
Epoch 81/200, Loss: 536.1420
Epoch 82/200, Loss: 535.7968
Epoch 83/200, Loss: 535.8450
Epoch 84/200, Loss: 535.5460
Epoch 85/200, Loss: 536.2288
Epoch 86/200, Loss: 535.6921
Epoch 87/200, Loss: 535.5174
Epoch 88/200, Loss: 536.1209
Epoch 89/200, Loss: 536.0398
Epoch 90/200, Loss: 535.6371
Epoch 91/200, Loss: 535.9264
Epoch 92/200, Loss: 536.9170
Epoch 93/200, Loss: 536.0749
Epoch 94/200, Loss: 536.3819
Epoch 95/200, Loss: 535.6896
Epoch 96/200, Loss: 536.6650
Epoch 97/200, Loss: 536.2663
Epoch 98/200, Loss: 535.9041
Epoch 99/200, Loss: 535.8944
Epoch 100/200, Loss: 535.8118
Epoch 101/200, Loss: 535.6973
Epoch 102/200, Loss: 535.9277
Epoch 103/200, Loss: 535.8226
Epoch 104/200, Loss: 535.9191
Epoch 105/200, Loss: 535.8133
Epoch 106/200, Loss: 536.3095
Epoch 107/200, Loss: 535.6635
Epoch 108/200, Loss: 537.1668
Epoch 109/200, Loss: 535.6909
Epoch 110/200, Loss: 535.6578
Epoch 111/200, Loss: 535.6972
Epoch 112/200, Loss: 535.7577
Epoch 113/200, Loss: 536.9023
Epoch 114/200, Loss: 535.9790
Epoch 115/200, Loss: 535.9327
Epoch 116/200, Loss: 535.7054
Epoch 117/200, Loss: 536.1751
Epoch 118/200, Loss: 536.1005
Epoch 119/200, Loss: 535.8096
Epoch 120/200, Loss: 536.2318
Epoch 121/200, Loss: 535.9513
Epoch 122/200, Loss: 536.2303
Epoch 123/200, Loss: 535.8765
Epoch 124/200, Loss: 535.6959
Epoch 125/200, Loss: 536.3608
Epoch 126/200, Loss: 535.9571
Epoch 127/200, Loss: 535.9019
Epoch 128/200, Loss: 535.9164
Epoch 129/200, Loss: 535.5360
Epoch 130/200, Loss: 535.5713
Epoch 131/200, Loss: 536.3360
Epoch 132/200, Loss: 535.9578
Epoch 133/200, Loss: 535.7025
Epoch 134/200, Loss: 535.6766
Epoch 135/200, Loss: 535.6904
Epoch 136/200, Loss: 536.0685
Epoch 137/200, Loss: 535.6768
Epoch 138/200, Loss: 535.7120
Epoch 139/200, Loss: 535.7917
Epoch 140/200, Loss: 537.0384
Epoch 141/200, Loss: 535.6958
Epoch 142/200, Loss: 536.5882
Epoch 143/200, Loss: 535.7330
Epoch 144/200, Loss: 536.1718
Epoch 145/200, Loss: 535.7597
Epoch 146/200, Loss: 536.0027
Epoch 147/200, Loss: 536.0082
Epoch 148/200, Loss: 536.2378
Epoch 149/200, Loss: 536.1090
Epoch 150/200, Loss: 535.5544
Epoch 151/200, Loss: 537.0287
Epoch 152/200, Loss: 535.8772
Epoch 153/200, Loss: 536.1727
Epoch 154/200, Loss: 535.5969
Epoch 155/200, Loss: 535.8715
Epoch 156/200, Loss: 535.8216
Epoch 157/200, Loss: 535.5655
Epoch 158/200, Loss: 536.3171
Epoch 159/200, Loss: 535.5393
Epoch 160/200, Loss: 535.8205
Epoch 161/200, Loss: 535.6768
Epoch 162/200, Loss: 536.1570
Epoch 163/200, Loss: 535.7827
Epoch 164/200, Loss: 535.5181
Epoch 165/200, Loss: 535.7504
Epoch 166/200, Loss: 535.8169
Epoch 167/200, Loss: 535.7507
Epoch 168/200, Loss: 536.2314
Epoch 169/200, Loss: 535.7714
Epoch 170/200, Loss: 535.9081
Epoch 171/200, Loss: 536.2824
Epoch 172/200, Loss: 536.1769
Epoch 173/200, Loss: 535.8419
Epoch 174/200, Loss: 535.6671
Epoch 175/200, Loss: 535.9598
Epoch 176/200, Loss: 537.0094
Epoch 177/200, Loss: 536.2565
Epoch 178/200, Loss: 535.7036
Epoch 179/200, Loss: 535.7746
Epoch 180/200, Loss: 535.8357
Epoch 181/200, Loss: 535.5518
Epoch 182/200, Loss: 535.6663
Epoch 183/200, Loss: 536.9863
Epoch 184/200, Loss: 535.8268
Epoch 185/200, Loss: 535.5836
Epoch 186/200, Loss: 536.0613
Epoch 187/200, Loss: 536.6916
Epoch 188/200, Loss: 537.4043
Epoch 189/200, Loss: 535.9057
Epoch 190/200, Loss: 536.4829
Epoch 191/200, Loss: 535.4998
Epoch 192/200, Loss: 535.4804
Epoch 193/200, Loss: 536.3673
Epoch 194/200, Loss: 536.0085
Epoch 195/200, Loss: 536.3621
Epoch 196/200, Loss: 535.6689
Epoch 197/200, Loss: 536.0459
Epoch 198/200, Loss: 536.4308
Epoch 199/200, Loss: 535.9561
Epoch 200/200, Loss: 535.7502
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 84.92% | Test Accuracy: 60.94%
Precision: 0.6281 | Recall: 0.6094 | F1-Score: 0.5970
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 21.85%
Precision: 0.2087 | Recall: 0.2185 | F1-Score: 0.2074


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1085 batches, Testing set - 121 batches
Epoch 1/200, Loss: 3206.4991
Epoch 2/200, Loss: 2700.0672
Epoch 3/200, Loss: 2551.7879
Epoch 4/200, Loss: 2452.2350
Epoch 5/200, Loss: 2383.3175
Epoch 6/200, Loss: 2333.9229
Epoch 7/200, Loss: 2326.7214
Epoch 8/200, Loss: 2320.1628
Epoch 9/200, Loss: 2314.8525
Epoch 10/200, Loss: 2310.1465
Epoch 11/200, Loss: 2306.0985
Epoch 12/200, Loss: 2305.9508
Epoch 13/200, Loss: 2304.9325
Epoch 14/200, Loss: 2305.1446
Epoch 15/200, Loss: 2304.2957
Epoch 16/200, Loss: 2303.2242
Epoch 17/200, Loss: 2304.2351
Epoch 18/200, Loss: 2303.5363
Epoch 19/200, Loss: 2303.4117
Epoch 20/200, Loss: 2303.9860
Epoch 21/200, Loss: 2303.0272
Epoch 22/200, Loss: 2303.1431
Epoch 23/200, Loss: 2303.5199
Epoch 24/200, Loss: 2303.3537
Epoch 25/200, Loss: 2302.9058
Epoch 26/200, Loss: 2304.1692
Epoch 27/200, Loss: 2303.0336
Epoch 28/200, Loss: 2303.1385
Epoch 29/200, Loss: 2303.4091
Epoch 30/200, Loss: 2303.0274
Epoch 31/200, Loss: 2302.8180
Epoch 32/200, Loss: 2303.1617
Epoch 33/200, Loss: 2304.2670
Epoch 34/200, Loss: 2302.6685
Epoch 35/200, Loss: 2303.9176
Epoch 36/200, Loss: 2303.3719
Epoch 37/200, Loss: 2304.1031
Epoch 38/200, Loss: 2303.5628
Epoch 39/200, Loss: 2302.8351
Epoch 40/200, Loss: 2303.3513
Epoch 41/200, Loss: 2303.7140
Epoch 42/200, Loss: 2303.4296
Epoch 43/200, Loss: 2303.2652
Epoch 44/200, Loss: 2302.6445
Epoch 45/200, Loss: 2302.9695
Epoch 46/200, Loss: 2302.5454
Epoch 47/200, Loss: 2303.9979
Epoch 48/200, Loss: 2303.1869
Epoch 49/200, Loss: 2302.9315
Epoch 50/200, Loss: 2303.3218
Epoch 51/200, Loss: 2303.0391
Epoch 52/200, Loss: 2302.8926
Epoch 53/200, Loss: 2302.8836
Epoch 54/200, Loss: 2303.1380
Epoch 55/200, Loss: 2303.3683
Epoch 56/200, Loss: 2302.7361
Epoch 57/200, Loss: 2302.5369
Epoch 58/200, Loss: 2302.8951
Epoch 59/200, Loss: 2303.7741
Epoch 60/200, Loss: 2303.2414
Epoch 61/200, Loss: 2303.1240
Epoch 62/200, Loss: 2303.0090
Epoch 63/200, Loss: 2302.7632
Epoch 64/200, Loss: 2302.6509
Epoch 65/200, Loss: 2303.0127
Epoch 66/200, Loss: 2303.5836
Epoch 67/200, Loss: 2302.8549
Epoch 68/200, Loss: 2304.0261
Epoch 69/200, Loss: 2303.3851
Epoch 70/200, Loss: 2303.8979
Epoch 71/200, Loss: 2303.4037
Epoch 72/200, Loss: 2303.0803
Epoch 73/200, Loss: 2303.1864
Epoch 74/200, Loss: 2303.6094
Epoch 75/200, Loss: 2302.7265
Epoch 76/200, Loss: 2303.6911
Epoch 77/200, Loss: 2303.2925
Epoch 78/200, Loss: 2302.9658
Epoch 79/200, Loss: 2303.1913
Epoch 80/200, Loss: 2302.7361
Epoch 81/200, Loss: 2303.3938
Epoch 82/200, Loss: 2303.1287
Epoch 83/200, Loss: 2302.9694
Epoch 84/200, Loss: 2302.9511
Epoch 85/200, Loss: 2303.3157
Epoch 86/200, Loss: 2302.9301
Epoch 87/200, Loss: 2302.9001
Epoch 88/200, Loss: 2303.6016
Epoch 89/200, Loss: 2303.2537
Epoch 90/200, Loss: 2303.0678
Epoch 91/200, Loss: 2303.1557
Epoch 92/200, Loss: 2303.7491
Epoch 93/200, Loss: 2303.0553
Epoch 94/200, Loss: 2302.8531
Epoch 95/200, Loss: 2303.1259
Epoch 96/200, Loss: 2302.3121
Epoch 97/200, Loss: 2303.4054
Epoch 98/200, Loss: 2303.1524
Epoch 99/200, Loss: 2303.4971
Epoch 100/200, Loss: 2303.1617
Epoch 101/200, Loss: 2303.4205
Epoch 102/200, Loss: 2303.0192
Epoch 103/200, Loss: 2303.1430
Epoch 104/200, Loss: 2303.5182
Epoch 105/200, Loss: 2303.7263
Epoch 106/200, Loss: 2304.2451
Epoch 107/200, Loss: 2302.7909
Epoch 108/200, Loss: 2303.0359
Epoch 109/200, Loss: 2302.8878
Epoch 110/200, Loss: 2303.7690
Epoch 111/200, Loss: 2303.2810
Epoch 112/200, Loss: 2303.6318
Epoch 113/200, Loss: 2303.0407
Epoch 114/200, Loss: 2303.5843
Epoch 115/200, Loss: 2303.5087
Epoch 116/200, Loss: 2302.4676
Epoch 117/200, Loss: 2302.9642
Epoch 118/200, Loss: 2302.9983
Epoch 119/200, Loss: 2302.7334
Epoch 120/200, Loss: 2302.5018
Epoch 121/200, Loss: 2303.6097
Epoch 122/200, Loss: 2303.6203
Epoch 123/200, Loss: 2303.6772
Epoch 124/200, Loss: 2302.8101
Epoch 125/200, Loss: 2303.2768
Epoch 126/200, Loss: 2304.4553
Epoch 127/200, Loss: 2302.9316
Epoch 128/200, Loss: 2303.7318
Epoch 129/200, Loss: 2303.4404
Epoch 130/200, Loss: 2303.2576
Epoch 131/200, Loss: 2303.3622
Epoch 132/200, Loss: 2302.9049
Epoch 133/200, Loss: 2303.0140
Epoch 134/200, Loss: 2303.2867
Epoch 135/200, Loss: 2302.9423
Epoch 136/200, Loss: 2303.0421
Epoch 137/200, Loss: 2302.9620
Epoch 138/200, Loss: 2303.6558
Epoch 139/200, Loss: 2302.7595
Epoch 140/200, Loss: 2303.5951
Epoch 141/200, Loss: 2303.3978
Epoch 142/200, Loss: 2303.1068
Epoch 143/200, Loss: 2303.3036
Epoch 144/200, Loss: 2303.1405
Epoch 145/200, Loss: 2303.0262
Epoch 146/200, Loss: 2303.3075
Epoch 147/200, Loss: 2303.6188
Epoch 148/200, Loss: 2303.2712
Epoch 149/200, Loss: 2303.5179
Epoch 150/200, Loss: 2303.2542
Epoch 151/200, Loss: 2303.2163
Epoch 152/200, Loss: 2302.8340
Epoch 153/200, Loss: 2302.5125
Epoch 154/200, Loss: 2303.2843
Epoch 155/200, Loss: 2303.6583
Epoch 156/200, Loss: 2303.6166
Epoch 157/200, Loss: 2302.8380
Epoch 158/200, Loss: 2302.5816
Epoch 159/200, Loss: 2303.7488
Epoch 160/200, Loss: 2303.1507
Epoch 161/200, Loss: 2303.3897
Epoch 162/200, Loss: 2302.9148
Epoch 163/200, Loss: 2302.9898
Epoch 164/200, Loss: 2303.9094
Epoch 165/200, Loss: 2303.8370
Epoch 166/200, Loss: 2302.8865
Epoch 167/200, Loss: 2303.3563
Epoch 168/200, Loss: 2302.9023
Epoch 169/200, Loss: 2303.3586
Epoch 170/200, Loss: 2303.3105
Epoch 171/200, Loss: 2303.2863
Epoch 172/200, Loss: 2303.6313
Epoch 173/200, Loss: 2303.2072
Epoch 174/200, Loss: 2303.0167
Epoch 175/200, Loss: 2303.1289
Epoch 176/200, Loss: 2303.1847
Epoch 177/200, Loss: 2303.0437
Epoch 178/200, Loss: 2303.5442
Epoch 179/200, Loss: 2303.4066
Epoch 180/200, Loss: 2303.8690
Epoch 181/200, Loss: 2303.4829
Epoch 182/200, Loss: 2302.9671
Epoch 183/200, Loss: 2303.1350
Epoch 184/200, Loss: 2303.7482
Epoch 185/200, Loss: 2304.2687
Epoch 186/200, Loss: 2303.4393
Epoch 187/200, Loss: 2302.6028
Epoch 188/200, Loss: 2302.8537
Epoch 189/200, Loss: 2303.1596
Epoch 190/200, Loss: 2302.8690
Epoch 191/200, Loss: 2303.3349
Epoch 192/200, Loss: 2303.3842
Epoch 193/200, Loss: 2302.9867
Epoch 194/200, Loss: 2303.7828
Epoch 195/200, Loss: 2303.2588
Epoch 196/200, Loss: 2303.5223
Epoch 197/200, Loss: 2302.7534
Epoch 198/200, Loss: 2302.4990
Epoch 199/200, Loss: 2302.6424
Epoch 200/200, Loss: 2302.9346
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 32.22% | Test Accuracy: 22.59%
Precision: 0.1752 | Recall: 0.2259 | F1-Score: 0.1779

Processing Subject 4...
Top 32 discriminative features: [26 46 44 41 38 29 14 12 45 42 58 60 22  9 40 18 16 33 21 25 37 52 30 54
 56  1 55 34 28 48 50 59]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69440
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1719
		Class 12: 1694
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1682
		Class 17: 1698
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1618
	# of Testing Samples: 7647
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 165
		Class 12: 152
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 176
		Class 17: 154
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 200
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 80.27%
Precision: 0.8223 | Recall: 0.8027 | F1-Score: 0.7945


Running Deep Learning Classifier...
DataLoader: Training set - 1085 batches, Testing set - 120 batches
Epoch 1/200, Loss: 2264.2390
Epoch 2/200, Loss: 1213.9980
Epoch 3/200, Loss: 948.6418
Epoch 4/200, Loss: 799.4481
Epoch 5/200, Loss: 703.8382
Epoch 6/200, Loss: 640.0855
Epoch 7/200, Loss: 631.1902
Epoch 8/200, Loss: 624.5011
Epoch 9/200, Loss: 617.7501
Epoch 10/200, Loss: 611.0925
Epoch 11/200, Loss: 605.4765
Epoch 12/200, Loss: 604.4549
Epoch 13/200, Loss: 603.7676
Epoch 14/200, Loss: 603.0939
Epoch 15/200, Loss: 602.4468
Epoch 16/200, Loss: 601.8076
Epoch 17/200, Loss: 601.7100
Epoch 18/200, Loss: 601.6360
Epoch 19/200, Loss: 601.5653
Epoch 20/200, Loss: 601.4989
Epoch 21/200, Loss: 601.4207
Epoch 22/200, Loss: 601.4169
Epoch 23/200, Loss: 601.4128
Epoch 24/200, Loss: 601.4089
Epoch 25/200, Loss: 601.4052
Epoch 26/200, Loss: 601.3996
Epoch 27/200, Loss: 601.3995
Epoch 28/200, Loss: 601.3995
Epoch 29/200, Loss: 601.3994
Epoch 30/200, Loss: 601.3994
Epoch 31/200, Loss: 601.3993
Epoch 32/200, Loss: 601.3993
Epoch 33/200, Loss: 601.3993
Epoch 34/200, Loss: 601.3993
Epoch 35/200, Loss: 601.3993
Epoch 36/200, Loss: 601.3993
Epoch 37/200, Loss: 601.3993
Epoch 38/200, Loss: 601.3993
Epoch 39/200, Loss: 601.3993
Epoch 40/200, Loss: 601.3993
Epoch 41/200, Loss: 601.3993
Epoch 42/200, Loss: 601.3993
Epoch 43/200, Loss: 601.3993
Epoch 44/200, Loss: 601.3993
Epoch 45/200, Loss: 601.3993
Epoch 46/200, Loss: 601.3993
Epoch 47/200, Loss: 601.3993
Epoch 48/200, Loss: 601.3993
Epoch 49/200, Loss: 601.3993
Epoch 50/200, Loss: 601.3993
Epoch 51/200, Loss: 601.3993
Epoch 52/200, Loss: 601.3993
Epoch 53/200, Loss: 601.3993
Epoch 54/200, Loss: 601.3993
Epoch 55/200, Loss: 601.3993
Epoch 56/200, Loss: 601.3993
Epoch 57/200, Loss: 601.3993
Epoch 58/200, Loss: 601.3993
Epoch 59/200, Loss: 601.3993
Epoch 60/200, Loss: 601.3993
Epoch 61/200, Loss: 601.3993
Epoch 62/200, Loss: 601.3993
Epoch 63/200, Loss: 601.3993
Epoch 64/200, Loss: 601.3993
Epoch 65/200, Loss: 601.3993
Epoch 66/200, Loss: 601.3993
Epoch 67/200, Loss: 601.3993
Epoch 68/200, Loss: 601.3993
Epoch 69/200, Loss: 601.3993
Epoch 70/200, Loss: 601.3993
Epoch 71/200, Loss: 601.3993
Epoch 72/200, Loss: 601.3993
Epoch 73/200, Loss: 601.3993
Epoch 74/200, Loss: 601.3993
Epoch 75/200, Loss: 601.3993
Epoch 76/200, Loss: 601.3993
Epoch 77/200, Loss: 601.3993
Epoch 78/200, Loss: 601.3993
Epoch 79/200, Loss: 601.3993
Epoch 80/200, Loss: 601.3993
Epoch 81/200, Loss: 601.3993
Epoch 82/200, Loss: 601.3993
Epoch 83/200, Loss: 601.3993
Epoch 84/200, Loss: 601.3993
Epoch 85/200, Loss: 601.3993
Epoch 86/200, Loss: 601.3993
Epoch 87/200, Loss: 601.3993
Epoch 88/200, Loss: 601.3993
Epoch 89/200, Loss: 601.3993
Epoch 90/200, Loss: 601.3993
Epoch 91/200, Loss: 601.3993
Epoch 92/200, Loss: 601.3993
Epoch 93/200, Loss: 601.3993
Epoch 94/200, Loss: 601.3993
Epoch 95/200, Loss: 601.3993
Epoch 96/200, Loss: 601.3993
Epoch 97/200, Loss: 601.3993
Epoch 98/200, Loss: 601.3993
Epoch 99/200, Loss: 601.3993
Epoch 100/200, Loss: 601.3993
Epoch 101/200, Loss: 601.3993
Epoch 102/200, Loss: 601.3993
Epoch 103/200, Loss: 601.3993
Epoch 104/200, Loss: 601.3993
Epoch 105/200, Loss: 601.3993
Epoch 106/200, Loss: 601.3993
Epoch 107/200, Loss: 601.3993
Epoch 108/200, Loss: 601.3993
Epoch 109/200, Loss: 601.3993
Epoch 110/200, Loss: 601.3993
Epoch 111/200, Loss: 601.3993
Epoch 112/200, Loss: 601.3993
Epoch 113/200, Loss: 601.3993
Epoch 114/200, Loss: 601.3993
Epoch 115/200, Loss: 601.3993
Epoch 116/200, Loss: 601.3993
Epoch 117/200, Loss: 601.3993
Epoch 118/200, Loss: 601.3993
Epoch 119/200, Loss: 601.3993
Epoch 120/200, Loss: 601.3993
Epoch 121/200, Loss: 601.3993
Epoch 122/200, Loss: 601.3993
Epoch 123/200, Loss: 601.3993
Epoch 124/200, Loss: 601.3993
Epoch 125/200, Loss: 601.3993
Epoch 126/200, Loss: 601.3993
Epoch 127/200, Loss: 601.3993
Epoch 128/200, Loss: 601.3993
Epoch 129/200, Loss: 601.3993
Epoch 130/200, Loss: 601.3993
Epoch 131/200, Loss: 601.3993
Epoch 132/200, Loss: 601.3993
Epoch 133/200, Loss: 601.3993
Epoch 134/200, Loss: 601.3993
Epoch 135/200, Loss: 601.3993
Epoch 136/200, Loss: 601.3993
Epoch 137/200, Loss: 601.3993
Epoch 138/200, Loss: 601.3993
Epoch 139/200, Loss: 601.3993
Epoch 140/200, Loss: 601.3993
Epoch 141/200, Loss: 601.3993
Epoch 142/200, Loss: 601.3993
Epoch 143/200, Loss: 601.3993
Epoch 144/200, Loss: 601.3993
Epoch 145/200, Loss: 601.3993
Epoch 146/200, Loss: 601.3993
Epoch 147/200, Loss: 601.3993
Epoch 148/200, Loss: 601.3993
Epoch 149/200, Loss: 601.3993
Epoch 150/200, Loss: 601.3993
Epoch 151/200, Loss: 601.3993
Epoch 152/200, Loss: 601.3993
Epoch 153/200, Loss: 601.3993
Epoch 154/200, Loss: 601.3993
Epoch 155/200, Loss: 601.3993
Epoch 156/200, Loss: 601.3993
Epoch 157/200, Loss: 601.3993
Epoch 158/200, Loss: 601.3993
Epoch 159/200, Loss: 601.3993
Epoch 160/200, Loss: 601.3993
Epoch 161/200, Loss: 601.3993
Epoch 162/200, Loss: 601.3993
Epoch 163/200, Loss: 601.3993
Epoch 164/200, Loss: 601.3993
Epoch 165/200, Loss: 601.3993
Epoch 166/200, Loss: 601.3993
Epoch 167/200, Loss: 601.3993
Epoch 168/200, Loss: 601.3993
Epoch 169/200, Loss: 601.3993
Epoch 170/200, Loss: 601.3993
Epoch 171/200, Loss: 601.3993
Epoch 172/200, Loss: 601.3993
Epoch 173/200, Loss: 601.3993
Epoch 174/200, Loss: 601.3993
Epoch 175/200, Loss: 601.3993
Epoch 176/200, Loss: 601.3993
Epoch 177/200, Loss: 601.3993
Epoch 178/200, Loss: 601.3993
Epoch 179/200, Loss: 601.3993
Epoch 180/200, Loss: 601.3993
Epoch 181/200, Loss: 601.3993
Epoch 182/200, Loss: 601.3993
Epoch 183/200, Loss: 601.3993
Epoch 184/200, Loss: 601.3993
Epoch 185/200, Loss: 601.3993
Epoch 186/200, Loss: 601.3993
Epoch 187/200, Loss: 601.3993
Epoch 188/200, Loss: 601.3993
Epoch 189/200, Loss: 601.3993
Epoch 190/200, Loss: 601.3993
Epoch 191/200, Loss: 601.3993
Epoch 192/200, Loss: 601.3993
Epoch 193/200, Loss: 601.3993
Epoch 194/200, Loss: 601.3993
Epoch 195/200, Loss: 601.3993
Epoch 196/200, Loss: 601.3993
Epoch 197/200, Loss: 601.3993
Epoch 198/200, Loss: 601.3993
Epoch 199/200, Loss: 601.3993
Epoch 200/200, Loss: 601.3993
Train Accuracy: 83.33% | Test Accuracy: 68.20%
Precision: 0.6828 | Recall: 0.6820 | F1-Score: 0.6615
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 20.48%
Precision: 0.2081 | Recall: 0.2048 | F1-Score: 0.2014


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1085 batches, Testing set - 120 batches
Epoch 1/200, Loss: 3698.5791
Epoch 2/200, Loss: 3048.6691
Epoch 3/200, Loss: 2857.0141
Epoch 4/200, Loss: 2762.9807
Epoch 5/200, Loss: 2707.3040
Epoch 6/200, Loss: 2645.1496
Epoch 7/200, Loss: 2638.9817
Epoch 8/200, Loss: 2634.3863
Epoch 9/200, Loss: 2629.8829
Epoch 10/200, Loss: 2625.6757
Epoch 11/200, Loss: 2619.2738
Epoch 12/200, Loss: 2618.1986
Epoch 13/200, Loss: 2617.7181
Epoch 14/200, Loss: 2617.2610
Epoch 15/200, Loss: 2616.8317
Epoch 16/200, Loss: 2616.1650
Epoch 17/200, Loss: 2616.0463
Epoch 18/200, Loss: 2615.9811
Epoch 19/200, Loss: 2615.9204
Epoch 20/200, Loss: 2615.8739
Epoch 21/200, Loss: 2615.7711
Epoch 22/200, Loss: 2615.7698
Epoch 23/200, Loss: 2615.7662
Epoch 24/200, Loss: 2615.7641
Epoch 25/200, Loss: 2615.7625
Epoch 26/200, Loss: 2615.7538
Epoch 27/200, Loss: 2615.7538
Epoch 28/200, Loss: 2615.7538
Epoch 29/200, Loss: 2615.7538
Epoch 30/200, Loss: 2615.7537
Epoch 31/200, Loss: 2615.7536
Epoch 32/200, Loss: 2615.7537
Epoch 33/200, Loss: 2615.7536
Epoch 34/200, Loss: 2615.7536
Epoch 35/200, Loss: 2615.7536
Epoch 36/200, Loss: 2615.7537
Epoch 37/200, Loss: 2615.7536
Epoch 38/200, Loss: 2615.7537
Epoch 39/200, Loss: 2615.7537
Epoch 40/200, Loss: 2615.7537
Epoch 41/200, Loss: 2615.7536
Epoch 42/200, Loss: 2615.7536
Epoch 43/200, Loss: 2615.7536
Epoch 44/200, Loss: 2615.7536
Epoch 45/200, Loss: 2615.7536
Epoch 46/200, Loss: 2615.7536
Epoch 47/200, Loss: 2615.7536
Epoch 48/200, Loss: 2615.7536
Epoch 49/200, Loss: 2615.7536
Epoch 50/200, Loss: 2615.7537
Epoch 51/200, Loss: 2615.7536
Epoch 52/200, Loss: 2615.7537
Epoch 53/200, Loss: 2615.7536
Epoch 54/200, Loss: 2615.7536
Epoch 55/200, Loss: 2615.7536
Epoch 56/200, Loss: 2615.7536
Epoch 57/200, Loss: 2615.7536
Epoch 58/200, Loss: 2615.7537
Epoch 59/200, Loss: 2615.7537
Epoch 60/200, Loss: 2615.7536
Epoch 61/200, Loss: 2615.7537
Epoch 62/200, Loss: 2615.7536
Epoch 63/200, Loss: 2615.7536
Epoch 64/200, Loss: 2615.7536
Epoch 65/200, Loss: 2615.7536
Epoch 66/200, Loss: 2615.7536
Epoch 67/200, Loss: 2615.7536
Epoch 68/200, Loss: 2615.7536
Epoch 69/200, Loss: 2615.7537
Epoch 70/200, Loss: 2615.7536
Epoch 71/200, Loss: 2615.7536
Epoch 72/200, Loss: 2615.7537
Epoch 73/200, Loss: 2615.7536
Epoch 74/200, Loss: 2615.7536
Epoch 75/200, Loss: 2615.7537
Epoch 76/200, Loss: 2615.7536
Epoch 77/200, Loss: 2615.7537
Epoch 78/200, Loss: 2615.7536
Epoch 79/200, Loss: 2615.7536
Epoch 80/200, Loss: 2615.7537
Epoch 81/200, Loss: 2615.7536
Epoch 82/200, Loss: 2615.7536
Epoch 83/200, Loss: 2615.7536
Epoch 84/200, Loss: 2615.7536
Epoch 85/200, Loss: 2615.7536
Epoch 86/200, Loss: 2615.7537
Epoch 87/200, Loss: 2615.7536
Epoch 88/200, Loss: 2615.7537
Epoch 89/200, Loss: 2615.7536
Epoch 90/200, Loss: 2615.7536
Epoch 91/200, Loss: 2615.7536
Epoch 92/200, Loss: 2615.7536
Epoch 93/200, Loss: 2615.7536
Epoch 94/200, Loss: 2615.7536
Epoch 95/200, Loss: 2615.7536
Epoch 96/200, Loss: 2615.7536
Epoch 97/200, Loss: 2615.7536
Epoch 98/200, Loss: 2615.7537
Epoch 99/200, Loss: 2615.7537
Epoch 100/200, Loss: 2615.7537
Epoch 101/200, Loss: 2615.7536
Epoch 102/200, Loss: 2615.7536
Epoch 103/200, Loss: 2615.7537
Epoch 104/200, Loss: 2615.7536
Epoch 105/200, Loss: 2615.7536
Epoch 106/200, Loss: 2615.7536
Epoch 107/200, Loss: 2615.7537
Epoch 108/200, Loss: 2615.7536
Epoch 109/200, Loss: 2615.7536
Epoch 110/200, Loss: 2615.7536
Epoch 111/200, Loss: 2615.7536
Epoch 112/200, Loss: 2615.7537
Epoch 113/200, Loss: 2615.7536
Epoch 114/200, Loss: 2615.7537
Epoch 115/200, Loss: 2615.7536
Epoch 116/200, Loss: 2615.7536
Epoch 117/200, Loss: 2615.7536
Epoch 118/200, Loss: 2615.7536
Epoch 119/200, Loss: 2615.7537
Epoch 120/200, Loss: 2615.7536
Epoch 121/200, Loss: 2615.7536
Epoch 122/200, Loss: 2615.7537
Epoch 123/200, Loss: 2615.7537
Epoch 124/200, Loss: 2615.7537
Epoch 125/200, Loss: 2615.7537
Epoch 126/200, Loss: 2615.7537
Epoch 127/200, Loss: 2615.7536
Epoch 128/200, Loss: 2615.7537
Epoch 129/200, Loss: 2615.7536
Epoch 130/200, Loss: 2615.7536
Epoch 131/200, Loss: 2615.7536
Epoch 132/200, Loss: 2615.7536
Epoch 133/200, Loss: 2615.7536
Epoch 134/200, Loss: 2615.7536
Epoch 135/200, Loss: 2615.7537
Epoch 136/200, Loss: 2615.7537
Epoch 137/200, Loss: 2615.7536
Epoch 138/200, Loss: 2615.7536
Epoch 139/200, Loss: 2615.7537
Epoch 140/200, Loss: 2615.7537
Epoch 141/200, Loss: 2615.7536
Epoch 142/200, Loss: 2615.7536
Epoch 143/200, Loss: 2615.7536
Epoch 144/200, Loss: 2615.7536
Epoch 145/200, Loss: 2615.7537
Epoch 146/200, Loss: 2615.7536
Epoch 147/200, Loss: 2615.7536
Epoch 148/200, Loss: 2615.7536
Epoch 149/200, Loss: 2615.7536
Epoch 150/200, Loss: 2615.7536
Epoch 151/200, Loss: 2615.7536
Epoch 152/200, Loss: 2615.7536
Epoch 153/200, Loss: 2615.7536
Epoch 154/200, Loss: 2615.7536
Epoch 155/200, Loss: 2615.7537
Epoch 156/200, Loss: 2615.7536
Epoch 157/200, Loss: 2615.7536
Epoch 158/200, Loss: 2615.7537
Epoch 159/200, Loss: 2615.7536
Epoch 160/200, Loss: 2615.7537
Epoch 161/200, Loss: 2615.7536
Epoch 162/200, Loss: 2615.7536
Epoch 163/200, Loss: 2615.7537
Epoch 164/200, Loss: 2615.7537
Epoch 165/200, Loss: 2615.7536
Epoch 166/200, Loss: 2615.7536
Epoch 167/200, Loss: 2615.7536
Epoch 168/200, Loss: 2615.7537
Epoch 169/200, Loss: 2615.7536
Epoch 170/200, Loss: 2615.7537
Epoch 171/200, Loss: 2615.7536
Epoch 172/200, Loss: 2615.7537
Epoch 173/200, Loss: 2615.7536
Epoch 174/200, Loss: 2615.7536
Epoch 175/200, Loss: 2615.7536
Epoch 176/200, Loss: 2615.7537
Epoch 177/200, Loss: 2615.7536
Epoch 178/200, Loss: 2615.7537
Epoch 179/200, Loss: 2615.7536
Epoch 180/200, Loss: 2615.7536
Epoch 181/200, Loss: 2615.7536
Epoch 182/200, Loss: 2615.7536
Epoch 183/200, Loss: 2615.7536
Epoch 184/200, Loss: 2615.7536
Epoch 185/200, Loss: 2615.7536
Epoch 186/200, Loss: 2615.7536
Epoch 187/200, Loss: 2615.7537
Epoch 188/200, Loss: 2615.7536
Epoch 189/200, Loss: 2615.7537
Epoch 190/200, Loss: 2615.7536
Epoch 191/200, Loss: 2615.7536
Epoch 192/200, Loss: 2615.7536
Epoch 193/200, Loss: 2615.7536
Epoch 194/200, Loss: 2615.7536
Epoch 195/200, Loss: 2615.7536
Epoch 196/200, Loss: 2615.7536
Epoch 197/200, Loss: 2615.7537
Epoch 198/200, Loss: 2615.7536
Epoch 199/200, Loss: 2615.7536
Epoch 200/200, Loss: 2615.7536
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 26.42% | Test Accuracy: 23.13%
Precision: 0.1633 | Recall: 0.2313 | F1-Score: 0.1728

Processing Subject 5...
Top 32 discriminative features: [26 46 44 41 38 29 14 12 58 45 60 42 18 16 22 40 25 37  9 33 21 52 54 56
 30  1 55 34 28 59 48 50]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69312
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1684
		Class 12: 1652
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1658
		Class 17: 1652
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1637
	# of Testing Samples: 7775
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 200
		Class 12: 194
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 200
		Class 17: 200
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 181
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 68.55%
Precision: 0.6936 | Recall: 0.6855 | F1-Score: 0.6572


Running Deep Learning Classifier...
DataLoader: Training set - 1083 batches, Testing set - 122 batches
Epoch 1/200, Loss: 2222.9138
Epoch 2/200, Loss: 1209.4902
Epoch 3/200, Loss: 948.7857
Epoch 4/200, Loss: 804.5540
Epoch 5/200, Loss: 711.6667
Epoch 6/200, Loss: 651.1279
Epoch 7/200, Loss: 642.2615
Epoch 8/200, Loss: 635.3474
Epoch 9/200, Loss: 628.7776
Epoch 10/200, Loss: 622.4964
Epoch 11/200, Loss: 616.6793
Epoch 12/200, Loss: 615.7769
Epoch 13/200, Loss: 615.1176
Epoch 14/200, Loss: 614.4957
Epoch 15/200, Loss: 613.8745
Epoch 16/200, Loss: 613.1898
Epoch 17/200, Loss: 613.1197
Epoch 18/200, Loss: 613.0531
Epoch 19/200, Loss: 612.9884
Epoch 20/200, Loss: 612.9255
Epoch 21/200, Loss: 612.8498
Epoch 22/200, Loss: 612.8463
Epoch 23/200, Loss: 612.8425
Epoch 24/200, Loss: 612.8387
Epoch 25/200, Loss: 612.8351
Epoch 26/200, Loss: 612.8298
Epoch 27/200, Loss: 612.8297
Epoch 28/200, Loss: 612.8297
Epoch 29/200, Loss: 612.8297
Epoch 30/200, Loss: 612.8296
Epoch 31/200, Loss: 612.8295
Epoch 32/200, Loss: 612.8295
Epoch 33/200, Loss: 612.8295
Epoch 34/200, Loss: 612.8295
Epoch 35/200, Loss: 612.8295
Epoch 36/200, Loss: 612.8295
Epoch 37/200, Loss: 612.8295
Epoch 38/200, Loss: 612.8295
Epoch 39/200, Loss: 612.8295
Epoch 40/200, Loss: 612.8295
Epoch 41/200, Loss: 612.8295
Epoch 42/200, Loss: 612.8295
Epoch 43/200, Loss: 612.8295
Epoch 44/200, Loss: 612.8295
Epoch 45/200, Loss: 612.8295
Epoch 46/200, Loss: 612.8295
Epoch 47/200, Loss: 612.8295
Epoch 48/200, Loss: 612.8295
Epoch 49/200, Loss: 612.8295
Epoch 50/200, Loss: 612.8295
Epoch 51/200, Loss: 612.8295
Epoch 52/200, Loss: 612.8295
Epoch 53/200, Loss: 612.8295
Epoch 54/200, Loss: 612.8295
Epoch 55/200, Loss: 612.8295
Epoch 56/200, Loss: 612.8295
Epoch 57/200, Loss: 612.8295
Epoch 58/200, Loss: 612.8295
Epoch 59/200, Loss: 612.8295
Epoch 60/200, Loss: 612.8295
Epoch 61/200, Loss: 612.8295
Epoch 62/200, Loss: 612.8295
Epoch 63/200, Loss: 612.8295
Epoch 64/200, Loss: 612.8295
Epoch 65/200, Loss: 612.8295
Epoch 66/200, Loss: 612.8295
Epoch 67/200, Loss: 612.8295
Epoch 68/200, Loss: 612.8295
Epoch 69/200, Loss: 612.8295
Epoch 70/200, Loss: 612.8295
Epoch 71/200, Loss: 612.8295
Epoch 72/200, Loss: 612.8295
Epoch 73/200, Loss: 612.8295
Epoch 74/200, Loss: 612.8295
Epoch 75/200, Loss: 612.8295
Epoch 76/200, Loss: 612.8295
Epoch 77/200, Loss: 612.8295
Epoch 78/200, Loss: 612.8295
Epoch 79/200, Loss: 612.8295
Epoch 80/200, Loss: 612.8295
Epoch 81/200, Loss: 612.8295
Epoch 82/200, Loss: 612.8295
Epoch 83/200, Loss: 612.8295
Epoch 84/200, Loss: 612.8295
Epoch 85/200, Loss: 612.8295
Epoch 86/200, Loss: 612.8295
Epoch 87/200, Loss: 612.8295
Epoch 88/200, Loss: 612.8295
Epoch 89/200, Loss: 612.8295
Epoch 90/200, Loss: 612.8295
Epoch 91/200, Loss: 612.8295
Epoch 92/200, Loss: 612.8295
Epoch 93/200, Loss: 612.8295
Epoch 94/200, Loss: 612.8295
Epoch 95/200, Loss: 612.8295
Epoch 96/200, Loss: 612.8295
Epoch 97/200, Loss: 612.8295
Epoch 98/200, Loss: 612.8295
Epoch 99/200, Loss: 612.8295
Epoch 100/200, Loss: 612.8295
Epoch 101/200, Loss: 612.8295
Epoch 102/200, Loss: 612.8295
Epoch 103/200, Loss: 612.8295
Epoch 104/200, Loss: 612.8295
Epoch 105/200, Loss: 612.8295
Epoch 106/200, Loss: 612.8295
Epoch 107/200, Loss: 612.8295
Epoch 108/200, Loss: 612.8295
Epoch 109/200, Loss: 612.8295
Epoch 110/200, Loss: 612.8295
Epoch 111/200, Loss: 612.8295
Epoch 112/200, Loss: 612.8295
Epoch 113/200, Loss: 612.8295
Epoch 114/200, Loss: 612.8295
Epoch 115/200, Loss: 612.8295
Epoch 116/200, Loss: 612.8295
Epoch 117/200, Loss: 612.8295
Epoch 118/200, Loss: 612.8295
Epoch 119/200, Loss: 612.8295
Epoch 120/200, Loss: 612.8295
Epoch 121/200, Loss: 612.8295
Epoch 122/200, Loss: 612.8295
Epoch 123/200, Loss: 612.8295
Epoch 124/200, Loss: 612.8295
Epoch 125/200, Loss: 612.8295
Epoch 126/200, Loss: 612.8295
Epoch 127/200, Loss: 612.8295
Epoch 128/200, Loss: 612.8295
Epoch 129/200, Loss: 612.8295
Epoch 130/200, Loss: 612.8295
Epoch 131/200, Loss: 612.8295
Epoch 132/200, Loss: 612.8295
Epoch 133/200, Loss: 612.8295
Epoch 134/200, Loss: 612.8295
Epoch 135/200, Loss: 612.8295
Epoch 136/200, Loss: 612.8295
Epoch 137/200, Loss: 612.8295
Epoch 138/200, Loss: 612.8295
Epoch 139/200, Loss: 612.8295
Epoch 140/200, Loss: 612.8295
Epoch 141/200, Loss: 612.8295
Epoch 142/200, Loss: 612.8295
Epoch 143/200, Loss: 612.8295
Epoch 144/200, Loss: 612.8295
Epoch 145/200, Loss: 612.8295
Epoch 146/200, Loss: 612.8295
Epoch 147/200, Loss: 612.8295
Epoch 148/200, Loss: 612.8295
Epoch 149/200, Loss: 612.8295
Epoch 150/200, Loss: 612.8295
Epoch 151/200, Loss: 612.8295
Epoch 152/200, Loss: 612.8295
Epoch 153/200, Loss: 612.8295
Epoch 154/200, Loss: 612.8295
Epoch 155/200, Loss: 612.8295
Epoch 156/200, Loss: 612.8295
Epoch 157/200, Loss: 612.8295
Epoch 158/200, Loss: 612.8295
Epoch 159/200, Loss: 612.8295
Epoch 160/200, Loss: 612.8295
Epoch 161/200, Loss: 612.8295
Epoch 162/200, Loss: 612.8295
Epoch 163/200, Loss: 612.8295
Epoch 164/200, Loss: 612.8295
Epoch 165/200, Loss: 612.8295
Epoch 166/200, Loss: 612.8295
Epoch 167/200, Loss: 612.8295
Epoch 168/200, Loss: 612.8295
Epoch 169/200, Loss: 612.8295
Epoch 170/200, Loss: 612.8295
Epoch 171/200, Loss: 612.8295
Epoch 172/200, Loss: 612.8295
Epoch 173/200, Loss: 612.8295
Epoch 174/200, Loss: 612.8295
Epoch 175/200, Loss: 612.8295
Epoch 176/200, Loss: 612.8295
Epoch 177/200, Loss: 612.8295
Epoch 178/200, Loss: 612.8295
Epoch 179/200, Loss: 612.8295
Epoch 180/200, Loss: 612.8295
Epoch 181/200, Loss: 612.8295
Epoch 182/200, Loss: 612.8295
Epoch 183/200, Loss: 612.8295
Epoch 184/200, Loss: 612.8295
Epoch 185/200, Loss: 612.8295
Epoch 186/200, Loss: 612.8295
Epoch 187/200, Loss: 612.8295
Epoch 188/200, Loss: 612.8295
Epoch 189/200, Loss: 612.8295
Epoch 190/200, Loss: 612.8295
Epoch 191/200, Loss: 612.8295
Epoch 192/200, Loss: 612.8295
Epoch 193/200, Loss: 612.8295
Epoch 194/200, Loss: 612.8295
Epoch 195/200, Loss: 612.8295
Epoch 196/200, Loss: 612.8295
Epoch 197/200, Loss: 612.8295
Epoch 198/200, Loss: 612.8295
Epoch 199/200, Loss: 612.8295
Epoch 200/200, Loss: 612.8295
Train Accuracy: 82.56% | Test Accuracy: 67.46%
Precision: 0.6683 | Recall: 0.6746 | F1-Score: 0.6444
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 24.09%
Precision: 0.2386 | Recall: 0.2409 | F1-Score: 0.2355


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1083 batches, Testing set - 122 batches
Epoch 1/200, Loss: 3518.1494
Epoch 2/200, Loss: 3040.7982
Epoch 3/200, Loss: 2864.9292
Epoch 4/200, Loss: 2777.9545
Epoch 5/200, Loss: 2718.1031
Epoch 6/200, Loss: 2653.1241
Epoch 7/200, Loss: 2646.4060
Epoch 8/200, Loss: 2640.7126
Epoch 9/200, Loss: 2635.3582
Epoch 10/200, Loss: 2630.8933
Epoch 11/200, Loss: 2624.6155
Epoch 12/200, Loss: 2623.5280
Epoch 13/200, Loss: 2623.1516
Epoch 14/200, Loss: 2622.5427
Epoch 15/200, Loss: 2621.9929
Epoch 16/200, Loss: 2621.4299
Epoch 17/200, Loss: 2621.2714
Epoch 18/200, Loss: 2621.2115
Epoch 19/200, Loss: 2621.1532
Epoch 20/200, Loss: 2621.1011
Epoch 21/200, Loss: 2621.0063
Epoch 22/200, Loss: 2621.0031
Epoch 23/200, Loss: 2621.0005
Epoch 24/200, Loss: 2620.9975
Epoch 25/200, Loss: 2620.9950
Epoch 26/200, Loss: 2620.9873
Epoch 27/200, Loss: 2620.9872
Epoch 28/200, Loss: 2620.9872
Epoch 29/200, Loss: 2620.9872
Epoch 30/200, Loss: 2620.9872
Epoch 31/200, Loss: 2620.9871
Epoch 32/200, Loss: 2620.9871
Epoch 33/200, Loss: 2620.9871
Epoch 34/200, Loss: 2620.9871
Epoch 35/200, Loss: 2620.9871
Epoch 36/200, Loss: 2620.9871
Epoch 37/200, Loss: 2620.9871
Epoch 38/200, Loss: 2620.9871
Epoch 39/200, Loss: 2620.9871
Epoch 40/200, Loss: 2620.9871
Epoch 41/200, Loss: 2620.9871
Epoch 42/200, Loss: 2620.9871
Epoch 43/200, Loss: 2620.9871
Epoch 44/200, Loss: 2620.9871
Epoch 45/200, Loss: 2620.9871
Epoch 46/200, Loss: 2620.9871
Epoch 47/200, Loss: 2620.9871
Epoch 48/200, Loss: 2620.9871
Epoch 49/200, Loss: 2620.9871
Epoch 50/200, Loss: 2620.9871
Epoch 51/200, Loss: 2620.9871
Epoch 52/200, Loss: 2620.9871
Epoch 53/200, Loss: 2620.9871
Epoch 54/200, Loss: 2620.9871
Epoch 55/200, Loss: 2620.9871
Epoch 56/200, Loss: 2620.9871
Epoch 57/200, Loss: 2620.9871
Epoch 58/200, Loss: 2620.9871
Epoch 59/200, Loss: 2620.9871
Epoch 60/200, Loss: 2620.9871
Epoch 61/200, Loss: 2620.9871
Epoch 62/200, Loss: 2620.9871
Epoch 63/200, Loss: 2620.9871
Epoch 64/200, Loss: 2620.9871
Epoch 65/200, Loss: 2620.9871
Epoch 66/200, Loss: 2620.9871
Epoch 67/200, Loss: 2620.9871
Epoch 68/200, Loss: 2620.9871
Epoch 69/200, Loss: 2620.9871
Epoch 70/200, Loss: 2620.9871
Epoch 71/200, Loss: 2620.9871
Epoch 72/200, Loss: 2620.9871
Epoch 73/200, Loss: 2620.9871
Epoch 74/200, Loss: 2620.9871
Epoch 75/200, Loss: 2620.9871
Epoch 76/200, Loss: 2620.9871
Epoch 77/200, Loss: 2620.9871
Epoch 78/200, Loss: 2620.9871
Epoch 79/200, Loss: 2620.9871
Epoch 80/200, Loss: 2620.9871
Epoch 81/200, Loss: 2620.9871
Epoch 82/200, Loss: 2620.9871
Epoch 83/200, Loss: 2620.9871
Epoch 84/200, Loss: 2620.9871
Epoch 85/200, Loss: 2620.9871
Epoch 86/200, Loss: 2620.9871
Epoch 87/200, Loss: 2620.9871
Epoch 88/200, Loss: 2620.9871
Epoch 89/200, Loss: 2620.9871
Epoch 90/200, Loss: 2620.9871
Epoch 91/200, Loss: 2620.9871
Epoch 92/200, Loss: 2620.9871
Epoch 93/200, Loss: 2620.9871
Epoch 94/200, Loss: 2620.9871
Epoch 95/200, Loss: 2620.9871
Epoch 96/200, Loss: 2620.9871
Epoch 97/200, Loss: 2620.9871
Epoch 98/200, Loss: 2620.9871
Epoch 99/200, Loss: 2620.9871
Epoch 100/200, Loss: 2620.9871
Epoch 101/200, Loss: 2620.9871
Epoch 102/200, Loss: 2620.9871
Epoch 103/200, Loss: 2620.9871
Epoch 104/200, Loss: 2620.9871
Epoch 105/200, Loss: 2620.9871
Epoch 106/200, Loss: 2620.9871
Epoch 107/200, Loss: 2620.9871
Epoch 108/200, Loss: 2620.9871
Epoch 109/200, Loss: 2620.9871
Epoch 110/200, Loss: 2620.9871
Epoch 111/200, Loss: 2620.9871
Epoch 112/200, Loss: 2620.9871
Epoch 113/200, Loss: 2620.9871
Epoch 114/200, Loss: 2620.9871
Epoch 115/200, Loss: 2620.9871
Epoch 116/200, Loss: 2620.9871
Epoch 117/200, Loss: 2620.9871
Epoch 118/200, Loss: 2620.9871
Epoch 119/200, Loss: 2620.9871
Epoch 120/200, Loss: 2620.9871
Epoch 121/200, Loss: 2620.9871
Epoch 122/200, Loss: 2620.9871
Epoch 123/200, Loss: 2620.9871
Epoch 124/200, Loss: 2620.9871
Epoch 125/200, Loss: 2620.9871
Epoch 126/200, Loss: 2620.9871
Epoch 127/200, Loss: 2620.9871
Epoch 128/200, Loss: 2620.9871
Epoch 129/200, Loss: 2620.9871
Epoch 130/200, Loss: 2620.9871
Epoch 131/200, Loss: 2620.9871
Epoch 132/200, Loss: 2620.9871
Epoch 133/200, Loss: 2620.9871
Epoch 134/200, Loss: 2620.9871
Epoch 135/200, Loss: 2620.9871
Epoch 136/200, Loss: 2620.9871
Epoch 137/200, Loss: 2620.9871
Epoch 138/200, Loss: 2620.9871
Epoch 139/200, Loss: 2620.9871
Epoch 140/200, Loss: 2620.9871
Epoch 141/200, Loss: 2620.9871
Epoch 142/200, Loss: 2620.9871
Epoch 143/200, Loss: 2620.9871
Epoch 144/200, Loss: 2620.9871
Epoch 145/200, Loss: 2620.9871
Epoch 146/200, Loss: 2620.9871
Epoch 147/200, Loss: 2620.9871
Epoch 148/200, Loss: 2620.9871
Epoch 149/200, Loss: 2620.9871
Epoch 150/200, Loss: 2620.9871
Epoch 151/200, Loss: 2620.9871
Epoch 152/200, Loss: 2620.9871
Epoch 153/200, Loss: 2620.9871
Epoch 154/200, Loss: 2620.9871
Epoch 155/200, Loss: 2620.9871
Epoch 156/200, Loss: 2620.9871
Epoch 157/200, Loss: 2620.9871
Epoch 158/200, Loss: 2620.9871
Epoch 159/200, Loss: 2620.9871
Epoch 160/200, Loss: 2620.9871
Epoch 161/200, Loss: 2620.9871
Epoch 162/200, Loss: 2620.9871
Epoch 163/200, Loss: 2620.9871
Epoch 164/200, Loss: 2620.9871
Epoch 165/200, Loss: 2620.9871
Epoch 166/200, Loss: 2620.9871
Epoch 167/200, Loss: 2620.9871
Epoch 168/200, Loss: 2620.9871
Epoch 169/200, Loss: 2620.9871
Epoch 170/200, Loss: 2620.9871
Epoch 171/200, Loss: 2620.9871
Epoch 172/200, Loss: 2620.9871
Epoch 173/200, Loss: 2620.9871
Epoch 174/200, Loss: 2620.9871
Epoch 175/200, Loss: 2620.9871
Epoch 176/200, Loss: 2620.9871
Epoch 177/200, Loss: 2620.9871
Epoch 178/200, Loss: 2620.9871
Epoch 179/200, Loss: 2620.9871
Epoch 180/200, Loss: 2620.9871
Epoch 181/200, Loss: 2620.9871
Epoch 182/200, Loss: 2620.9871
Epoch 183/200, Loss: 2620.9871
Epoch 184/200, Loss: 2620.9871
Epoch 185/200, Loss: 2620.9871
Epoch 186/200, Loss: 2620.9871
Epoch 187/200, Loss: 2620.9871
Epoch 188/200, Loss: 2620.9871
Epoch 189/200, Loss: 2620.9871
Epoch 190/200, Loss: 2620.9871
Epoch 191/200, Loss: 2620.9871
Epoch 192/200, Loss: 2620.9871
Epoch 193/200, Loss: 2620.9871
Epoch 194/200, Loss: 2620.9871
Epoch 195/200, Loss: 2620.9871
Epoch 196/200, Loss: 2620.9871
Epoch 197/200, Loss: 2620.9871
Epoch 198/200, Loss: 2620.9871
Epoch 199/200, Loss: 2620.9871
Epoch 200/200, Loss: 2620.9871
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 26.83% | Test Accuracy: 25.75%
Precision: 0.1818 | Recall: 0.2575 | F1-Score: 0.2016

Processing Subject 6...
Top 32 discriminative features: [26 46 44 41 29 38 14 37 25 12 45 42 22 33 18 58 16 60  9 40 21 30 52 55
 54 56  1 28 59 34 49 13]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69357
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1687
		Class 12: 1646
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1658
		Class 17: 1667
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1789
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1655
	# of Testing Samples: 7730
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 197
		Class 12: 200
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 200
		Class 17: 185
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 185
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 163
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 68.25%
Precision: 0.7295 | Recall: 0.6825 | F1-Score: 0.6689


Running Deep Learning Classifier...
DataLoader: Training set - 1084 batches, Testing set - 121 batches
Epoch 1/200, Loss: 2117.9846
Epoch 2/200, Loss: 1123.6756
Epoch 3/200, Loss: 880.0983
Epoch 4/200, Loss: 750.9159
Epoch 5/200, Loss: 669.4825
Epoch 6/200, Loss: 614.5052
Epoch 7/200, Loss: 606.6539
Epoch 8/200, Loss: 600.4873
Epoch 9/200, Loss: 594.5870
Epoch 10/200, Loss: 589.0481
Epoch 11/200, Loss: 584.0350
Epoch 12/200, Loss: 583.0827
Epoch 13/200, Loss: 582.4721
Epoch 14/200, Loss: 581.9297
Epoch 15/200, Loss: 581.3360
Epoch 16/200, Loss: 580.7754
Epoch 17/200, Loss: 580.6998
Epoch 18/200, Loss: 580.7273
Epoch 19/200, Loss: 580.6101
Epoch 20/200, Loss: 580.5156
Epoch 21/200, Loss: 580.5217
Epoch 22/200, Loss: 580.4992
Epoch 23/200, Loss: 580.5253
Epoch 24/200, Loss: 580.5417
Epoch 25/200, Loss: 580.4598
Epoch 26/200, Loss: 580.5152
Epoch 27/200, Loss: 580.5081
Epoch 28/200, Loss: 580.5084
Epoch 29/200, Loss: 580.4387
Epoch 30/200, Loss: 580.4880
Epoch 31/200, Loss: 580.5432
Epoch 32/200, Loss: 580.4661
Epoch 33/200, Loss: 580.4515
Epoch 34/200, Loss: 580.4963
Epoch 35/200, Loss: 580.5011
Epoch 36/200, Loss: 580.4710
Epoch 37/200, Loss: 580.4827
Epoch 38/200, Loss: 580.4900
Epoch 39/200, Loss: 580.4942
Epoch 40/200, Loss: 580.4189
Epoch 41/200, Loss: 580.4631
Epoch 42/200, Loss: 580.4550
Epoch 43/200, Loss: 580.4206
Epoch 44/200, Loss: 580.4495
Epoch 45/200, Loss: 580.4434
Epoch 46/200, Loss: 580.4084
Epoch 47/200, Loss: 580.4616
Epoch 48/200, Loss: 580.4455
Epoch 49/200, Loss: 580.5392
Epoch 50/200, Loss: 580.4747
Epoch 51/200, Loss: 580.5035
Epoch 52/200, Loss: 580.4697
Epoch 53/200, Loss: 580.4812
Epoch 54/200, Loss: 580.4503
Epoch 55/200, Loss: 580.4561
Epoch 56/200, Loss: 580.5011
Epoch 57/200, Loss: 580.4624
Epoch 58/200, Loss: 580.4960
Epoch 59/200, Loss: 580.4892
Epoch 60/200, Loss: 580.5652
Epoch 61/200, Loss: 580.4107
Epoch 62/200, Loss: 580.4672
Epoch 63/200, Loss: 580.4605
Epoch 64/200, Loss: 580.4946
Epoch 65/200, Loss: 580.5325
Epoch 66/200, Loss: 580.4603
Epoch 67/200, Loss: 580.5006
Epoch 68/200, Loss: 580.4529
Epoch 69/200, Loss: 580.4376
Epoch 70/200, Loss: 580.4333
Epoch 71/200, Loss: 580.4297
Epoch 72/200, Loss: 580.5915
Epoch 73/200, Loss: 580.4897
Epoch 74/200, Loss: 580.4860
Epoch 75/200, Loss: 580.4755
Epoch 76/200, Loss: 580.4667
Epoch 77/200, Loss: 580.4438
Epoch 78/200, Loss: 580.5064
Epoch 79/200, Loss: 580.4679
Epoch 80/200, Loss: 580.5334
Epoch 81/200, Loss: 580.4789
Epoch 82/200, Loss: 580.4529
Epoch 83/200, Loss: 580.5146
Epoch 84/200, Loss: 580.4289
Epoch 85/200, Loss: 580.4682
Epoch 86/200, Loss: 580.5677
Epoch 87/200, Loss: 580.4962
Epoch 88/200, Loss: 580.5368
Epoch 89/200, Loss: 580.4513
Epoch 90/200, Loss: 580.5054
Epoch 91/200, Loss: 580.4797
Epoch 92/200, Loss: 580.5019
Epoch 93/200, Loss: 580.4565
Epoch 94/200, Loss: 580.4627
Epoch 95/200, Loss: 580.5016
Epoch 96/200, Loss: 580.4679
Epoch 97/200, Loss: 580.4709
Epoch 98/200, Loss: 580.4613
Epoch 99/200, Loss: 580.4357
Epoch 100/200, Loss: 580.4532
Epoch 101/200, Loss: 580.4431
Epoch 102/200, Loss: 580.4500
Epoch 103/200, Loss: 580.4857
Epoch 104/200, Loss: 580.4394
Epoch 105/200, Loss: 580.4756
Epoch 106/200, Loss: 580.5396
Epoch 107/200, Loss: 580.4943
Epoch 108/200, Loss: 580.4638
Epoch 109/200, Loss: 580.5291
Epoch 110/200, Loss: 580.4440
Epoch 111/200, Loss: 580.5456
Epoch 112/200, Loss: 580.4808
Epoch 113/200, Loss: 580.4870
Epoch 114/200, Loss: 580.4701
Epoch 115/200, Loss: 580.4293
Epoch 116/200, Loss: 580.4992
Epoch 117/200, Loss: 580.4909
Epoch 118/200, Loss: 580.4853
Epoch 119/200, Loss: 580.4953
Epoch 120/200, Loss: 580.4515
Epoch 121/200, Loss: 580.4622
Epoch 122/200, Loss: 580.4706
Epoch 123/200, Loss: 580.5137
Epoch 124/200, Loss: 580.4631
Epoch 125/200, Loss: 580.4217
Epoch 126/200, Loss: 580.4450
Epoch 127/200, Loss: 580.4709
Epoch 128/200, Loss: 580.4847
Epoch 129/200, Loss: 580.4054
Epoch 130/200, Loss: 580.4798
Epoch 131/200, Loss: 580.4983
Epoch 132/200, Loss: 580.4852
Epoch 133/200, Loss: 580.4252
Epoch 134/200, Loss: 580.3995
Epoch 135/200, Loss: 580.4924
Epoch 136/200, Loss: 580.4639
Epoch 137/200, Loss: 580.5007
Epoch 138/200, Loss: 580.4876
Epoch 139/200, Loss: 580.4402
Epoch 140/200, Loss: 580.5214
Epoch 141/200, Loss: 580.4561
Epoch 142/200, Loss: 580.5024
Epoch 143/200, Loss: 580.5021
Epoch 144/200, Loss: 580.4648
Epoch 145/200, Loss: 580.4438
Epoch 146/200, Loss: 580.4611
Epoch 147/200, Loss: 580.4164
Epoch 148/200, Loss: 580.4851
Epoch 149/200, Loss: 580.4582
Epoch 150/200, Loss: 580.4337
Epoch 151/200, Loss: 580.4745
Epoch 152/200, Loss: 580.4727
Epoch 153/200, Loss: 580.5007
Epoch 154/200, Loss: 580.5106
Epoch 155/200, Loss: 580.4572
Epoch 156/200, Loss: 580.4960
Epoch 157/200, Loss: 580.5083
Epoch 158/200, Loss: 580.4848
Epoch 159/200, Loss: 580.4163
Epoch 160/200, Loss: 580.4783
Epoch 161/200, Loss: 580.4576
Epoch 162/200, Loss: 580.4364
Epoch 163/200, Loss: 580.4903
Epoch 164/200, Loss: 580.5102
Epoch 165/200, Loss: 580.4873
Epoch 166/200, Loss: 580.4805
Epoch 167/200, Loss: 580.5110
Epoch 168/200, Loss: 580.4469
Epoch 169/200, Loss: 580.4627
Epoch 170/200, Loss: 580.4664
Epoch 171/200, Loss: 580.4865
Epoch 172/200, Loss: 580.4364
Epoch 173/200, Loss: 580.5106
Epoch 174/200, Loss: 580.5437
Epoch 175/200, Loss: 580.5376
Epoch 176/200, Loss: 580.4349
Epoch 177/200, Loss: 580.5257
Epoch 178/200, Loss: 580.4873
Epoch 179/200, Loss: 580.4468
Epoch 180/200, Loss: 580.5140
Epoch 181/200, Loss: 580.4601
Epoch 182/200, Loss: 580.5029
Epoch 183/200, Loss: 580.4393
Epoch 184/200, Loss: 580.5271
Epoch 185/200, Loss: 580.4816
Epoch 186/200, Loss: 580.5139
Epoch 187/200, Loss: 580.4430
Epoch 188/200, Loss: 580.4705
Epoch 189/200, Loss: 580.5555
Epoch 190/200, Loss: 580.4998
Epoch 191/200, Loss: 580.4831
Epoch 192/200, Loss: 580.5261
Epoch 193/200, Loss: 580.5148
Epoch 194/200, Loss: 580.6036
Epoch 195/200, Loss: 580.5461
Epoch 196/200, Loss: 580.5328
Epoch 197/200, Loss: 580.4609
Epoch 198/200, Loss: 580.4579
Epoch 199/200, Loss: 580.4493
Epoch 200/200, Loss: 580.5169
Train Accuracy: 83.87% | Test Accuracy: 72.70%
Precision: 0.7302 | Recall: 0.7270 | F1-Score: 0.7108
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 26.83%
Precision: 0.2780 | Recall: 0.2683 | F1-Score: 0.2680


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1084 batches, Testing set - 121 batches
Epoch 1/200, Loss: 3484.9882
Epoch 2/200, Loss: 2798.1720
Epoch 3/200, Loss: 2581.6618
Epoch 4/200, Loss: 2491.6456
Epoch 5/200, Loss: 2438.2794
Epoch 6/200, Loss: 2382.4963
Epoch 7/200, Loss: 2377.9333
Epoch 8/200, Loss: 2373.7398
Epoch 9/200, Loss: 2369.6342
Epoch 10/200, Loss: 2365.6102
Epoch 11/200, Loss: 2361.0021
Epoch 12/200, Loss: 2359.9163
Epoch 13/200, Loss: 2359.4654
Epoch 14/200, Loss: 2358.9350
Epoch 15/200, Loss: 2358.6412
Epoch 16/200, Loss: 2358.0698
Epoch 17/200, Loss: 2357.9358
Epoch 18/200, Loss: 2357.9120
Epoch 19/200, Loss: 2357.9044
Epoch 20/200, Loss: 2357.8167
Epoch 21/200, Loss: 2357.7963
Epoch 22/200, Loss: 2357.7062
Epoch 23/200, Loss: 2357.7270
Epoch 24/200, Loss: 2357.6364
Epoch 25/200, Loss: 2357.7630
Epoch 26/200, Loss: 2357.7711
Epoch 27/200, Loss: 2357.7363
Epoch 28/200, Loss: 2357.7314
Epoch 29/200, Loss: 2357.7406
Epoch 30/200, Loss: 2357.7349
Epoch 31/200, Loss: 2357.7389
Epoch 32/200, Loss: 2357.7350
Epoch 33/200, Loss: 2357.7579
Epoch 34/200, Loss: 2357.7455
Epoch 35/200, Loss: 2357.7370
Epoch 36/200, Loss: 2357.7374
Epoch 37/200, Loss: 2357.7294
Epoch 38/200, Loss: 2357.7840
Epoch 39/200, Loss: 2357.6473
Epoch 40/200, Loss: 2357.6987
Epoch 41/200, Loss: 2357.7482
Epoch 42/200, Loss: 2357.7113
Epoch 43/200, Loss: 2357.6965
Epoch 44/200, Loss: 2357.7694
Epoch 45/200, Loss: 2357.8025
Epoch 46/200, Loss: 2357.7046
Epoch 47/200, Loss: 2357.7222
Epoch 48/200, Loss: 2357.7732
Epoch 49/200, Loss: 2357.6840
Epoch 50/200, Loss: 2357.7224
Epoch 51/200, Loss: 2357.6757
Epoch 52/200, Loss: 2357.7135
Epoch 53/200, Loss: 2357.7684
Epoch 54/200, Loss: 2357.8598
Epoch 55/200, Loss: 2357.7203
Epoch 56/200, Loss: 2357.6891
Epoch 57/200, Loss: 2357.7063
Epoch 58/200, Loss: 2357.7910
Epoch 59/200, Loss: 2357.7669
Epoch 60/200, Loss: 2357.7913
Epoch 61/200, Loss: 2357.7639
Epoch 62/200, Loss: 2357.7577
Epoch 63/200, Loss: 2357.7335
Epoch 64/200, Loss: 2357.7023
Epoch 65/200, Loss: 2357.7716
Epoch 66/200, Loss: 2357.7918
Epoch 67/200, Loss: 2357.7627
Epoch 68/200, Loss: 2357.7086
Epoch 69/200, Loss: 2357.7232
Epoch 70/200, Loss: 2357.7625
Epoch 71/200, Loss: 2357.6812
Epoch 72/200, Loss: 2357.7532
Epoch 73/200, Loss: 2357.8392
Epoch 74/200, Loss: 2357.7751
Epoch 75/200, Loss: 2357.7435
Epoch 76/200, Loss: 2357.7335
Epoch 77/200, Loss: 2357.7292
Epoch 78/200, Loss: 2357.7863
Epoch 79/200, Loss: 2357.6920
Epoch 80/200, Loss: 2357.7887
Epoch 81/200, Loss: 2357.7629
Epoch 82/200, Loss: 2357.8290
Epoch 83/200, Loss: 2357.7361
Epoch 84/200, Loss: 2357.8070
Epoch 85/200, Loss: 2357.6901
Epoch 86/200, Loss: 2357.7762
Epoch 87/200, Loss: 2357.8556
Epoch 88/200, Loss: 2357.7876
Epoch 89/200, Loss: 2357.6574
Epoch 90/200, Loss: 2357.7368
Epoch 91/200, Loss: 2357.7363
Epoch 92/200, Loss: 2357.7544
Epoch 93/200, Loss: 2357.7442
Epoch 94/200, Loss: 2357.6483
Epoch 95/200, Loss: 2357.7979
Epoch 96/200, Loss: 2357.7072
Epoch 97/200, Loss: 2357.8327
Epoch 98/200, Loss: 2357.7560
Epoch 99/200, Loss: 2357.6992
Epoch 100/200, Loss: 2357.7181
Epoch 101/200, Loss: 2357.7499
Epoch 102/200, Loss: 2357.6887
Epoch 103/200, Loss: 2357.6745
Epoch 104/200, Loss: 2357.7871
Epoch 105/200, Loss: 2357.6846
Epoch 106/200, Loss: 2357.7014
Epoch 107/200, Loss: 2357.7317
Epoch 108/200, Loss: 2357.7377
Epoch 109/200, Loss: 2357.7842
Epoch 110/200, Loss: 2357.7808
Epoch 111/200, Loss: 2357.7093
Epoch 112/200, Loss: 2357.6899
Epoch 113/200, Loss: 2357.6431
Epoch 114/200, Loss: 2357.7222
Epoch 115/200, Loss: 2357.7373
Epoch 116/200, Loss: 2357.7478
Epoch 117/200, Loss: 2357.6856
Epoch 118/200, Loss: 2357.7719
Epoch 119/200, Loss: 2357.7913
Epoch 120/200, Loss: 2357.7761
Epoch 121/200, Loss: 2357.7330
Epoch 122/200, Loss: 2357.7071
Epoch 123/200, Loss: 2357.7523
Epoch 124/200, Loss: 2357.7616
Epoch 125/200, Loss: 2357.7942
Epoch 126/200, Loss: 2357.6208
Epoch 127/200, Loss: 2357.7597
Epoch 128/200, Loss: 2357.7129
Epoch 129/200, Loss: 2357.6718
Epoch 130/200, Loss: 2357.8122
Epoch 131/200, Loss: 2357.7167
Epoch 132/200, Loss: 2357.6423
Epoch 133/200, Loss: 2357.7226
Epoch 134/200, Loss: 2357.6825
Epoch 135/200, Loss: 2357.7561
Epoch 136/200, Loss: 2357.7566
Epoch 137/200, Loss: 2357.7649
Epoch 138/200, Loss: 2357.7485
Epoch 139/200, Loss: 2357.7954
Epoch 140/200, Loss: 2357.6734
Epoch 141/200, Loss: 2357.6925
Epoch 142/200, Loss: 2357.7855
Epoch 143/200, Loss: 2357.6919
Epoch 144/200, Loss: 2357.7149
Epoch 145/200, Loss: 2357.7698
Epoch 146/200, Loss: 2357.6812
Epoch 147/200, Loss: 2357.7116
Epoch 148/200, Loss: 2357.7415
Epoch 149/200, Loss: 2357.7368
Epoch 150/200, Loss: 2357.7165
Epoch 151/200, Loss: 2357.6969
Epoch 152/200, Loss: 2357.7263
Epoch 153/200, Loss: 2357.7654
Epoch 154/200, Loss: 2357.7358
Epoch 155/200, Loss: 2357.7657
Epoch 156/200, Loss: 2357.6993
Epoch 157/200, Loss: 2357.7178
Epoch 158/200, Loss: 2357.7505
Epoch 159/200, Loss: 2357.7252
Epoch 160/200, Loss: 2357.7377
Epoch 161/200, Loss: 2357.6725
Epoch 162/200, Loss: 2357.6696
Epoch 163/200, Loss: 2357.7705
Epoch 164/200, Loss: 2357.8086
Epoch 165/200, Loss: 2357.7257
Epoch 166/200, Loss: 2357.7810
Epoch 167/200, Loss: 2357.7916
Epoch 168/200, Loss: 2357.6759
Epoch 169/200, Loss: 2357.7278
Epoch 170/200, Loss: 2357.7207
Epoch 171/200, Loss: 2357.6885
Epoch 172/200, Loss: 2357.6954
Epoch 173/200, Loss: 2357.7725
Epoch 174/200, Loss: 2357.7976
Epoch 175/200, Loss: 2357.7554
Epoch 176/200, Loss: 2357.7647
Epoch 177/200, Loss: 2357.7754
Epoch 178/200, Loss: 2357.7560
Epoch 179/200, Loss: 2357.7149
Epoch 180/200, Loss: 2357.7080
Epoch 181/200, Loss: 2357.7162
Epoch 182/200, Loss: 2357.7229
Epoch 183/200, Loss: 2357.6886
Epoch 184/200, Loss: 2357.7794
Epoch 185/200, Loss: 2357.7769
Epoch 186/200, Loss: 2357.7641
Epoch 187/200, Loss: 2357.7372
Epoch 188/200, Loss: 2357.7185
Epoch 189/200, Loss: 2357.6987
Epoch 190/200, Loss: 2357.7558
Epoch 191/200, Loss: 2357.6611
Epoch 192/200, Loss: 2357.7071
Epoch 193/200, Loss: 2357.7709
Epoch 194/200, Loss: 2357.7999
Epoch 195/200, Loss: 2357.7463
Epoch 196/200, Loss: 2357.7364
Epoch 197/200, Loss: 2357.7341
Epoch 198/200, Loss: 2357.7537
Epoch 199/200, Loss: 2357.7058
Epoch 200/200, Loss: 2357.7123
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 29.04% | Test Accuracy: 23.30%
Precision: 0.1774 | Recall: 0.2330 | F1-Score: 0.1697

Processing Subject 7...
Top 32 discriminative features: [26 46 44 41 29 38 14 12 45 42 18 58 60 16 22 25 37 40 33  9 21 30 52 54
 56 34  1 55 28 59 48 50]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69287
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1684
		Class 12: 1646
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1658
		Class 17: 1652
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1618
	# of Testing Samples: 7800
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 200
		Class 12: 200
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 200
		Class 17: 200
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 200
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 77.18%
Precision: 0.7942 | Recall: 0.7718 | F1-Score: 0.7657


Running Deep Learning Classifier...
DataLoader: Training set - 1083 batches, Testing set - 122 batches
Epoch 1/200, Loss: 2365.5021
Epoch 2/200, Loss: 1274.8302
Epoch 3/200, Loss: 998.5407
Epoch 4/200, Loss: 831.1418
Epoch 5/200, Loss: 725.9766
Epoch 6/200, Loss: 659.3946
Epoch 7/200, Loss: 650.3011
Epoch 8/200, Loss: 643.2519
Epoch 9/200, Loss: 636.2949
Epoch 10/200, Loss: 629.9945
Epoch 11/200, Loss: 623.8757
Epoch 12/200, Loss: 622.9072
Epoch 13/200, Loss: 622.2228
Epoch 14/200, Loss: 621.5827
Epoch 15/200, Loss: 621.0710
Epoch 16/200, Loss: 620.2675
Epoch 17/200, Loss: 620.1702
Epoch 18/200, Loss: 620.0908
Epoch 19/200, Loss: 620.0352
Epoch 20/200, Loss: 619.9235
Epoch 21/200, Loss: 619.8777
Epoch 22/200, Loss: 619.8428
Epoch 23/200, Loss: 619.8873
Epoch 24/200, Loss: 619.9215
Epoch 25/200, Loss: 619.8433
Epoch 26/200, Loss: 619.8925
Epoch 27/200, Loss: 619.9158
Epoch 28/200, Loss: 619.8948
Epoch 29/200, Loss: 619.8807
Epoch 30/200, Loss: 619.8021
Epoch 31/200, Loss: 620.0438
Epoch 32/200, Loss: 619.8615
Epoch 33/200, Loss: 619.8646
Epoch 34/200, Loss: 619.8747
Epoch 35/200, Loss: 619.8889
Epoch 36/200, Loss: 619.9767
Epoch 37/200, Loss: 619.8282
Epoch 38/200, Loss: 619.9061
Epoch 39/200, Loss: 619.8625
Epoch 40/200, Loss: 619.7938
Epoch 41/200, Loss: 619.8191
Epoch 42/200, Loss: 619.9423
Epoch 43/200, Loss: 619.8102
Epoch 44/200, Loss: 619.7706
Epoch 45/200, Loss: 619.8360
Epoch 46/200, Loss: 619.9257
Epoch 47/200, Loss: 619.9162
Epoch 48/200, Loss: 619.8922
Epoch 49/200, Loss: 619.8749
Epoch 50/200, Loss: 619.8817
Epoch 51/200, Loss: 619.8182
Epoch 52/200, Loss: 619.9162
Epoch 53/200, Loss: 619.8867
Epoch 54/200, Loss: 619.8261
Epoch 55/200, Loss: 619.9200
Epoch 56/200, Loss: 619.8247
Epoch 57/200, Loss: 619.8686
Epoch 58/200, Loss: 620.0426
Epoch 59/200, Loss: 619.8333
Epoch 60/200, Loss: 619.9256
Epoch 61/200, Loss: 619.8367
Epoch 62/200, Loss: 619.8099
Epoch 63/200, Loss: 619.9483
Epoch 64/200, Loss: 619.8899
Epoch 65/200, Loss: 619.8710
Epoch 66/200, Loss: 619.9062
Epoch 67/200, Loss: 619.8708
Epoch 68/200, Loss: 619.8395
Epoch 69/200, Loss: 619.8641
Epoch 70/200, Loss: 619.8922
Epoch 71/200, Loss: 619.8351
Epoch 72/200, Loss: 619.9341
Epoch 73/200, Loss: 619.8150
Epoch 74/200, Loss: 619.8880
Epoch 75/200, Loss: 619.8459
Epoch 76/200, Loss: 619.9116
Epoch 77/200, Loss: 619.9815
Epoch 78/200, Loss: 619.8941
Epoch 79/200, Loss: 619.8233
Epoch 80/200, Loss: 619.9870
Epoch 81/200, Loss: 619.8961
Epoch 82/200, Loss: 619.8532
Epoch 83/200, Loss: 619.9454
Epoch 84/200, Loss: 619.9049
Epoch 85/200, Loss: 619.8803
Epoch 86/200, Loss: 619.8403
Epoch 87/200, Loss: 619.9742
Epoch 88/200, Loss: 619.8629
Epoch 89/200, Loss: 619.7774
Epoch 90/200, Loss: 619.7843
Epoch 91/200, Loss: 619.8832
Epoch 92/200, Loss: 619.8141
Epoch 93/200, Loss: 619.8958
Epoch 94/200, Loss: 619.7957
Epoch 95/200, Loss: 619.8615
Epoch 96/200, Loss: 619.8850
Epoch 97/200, Loss: 619.7621
Epoch 98/200, Loss: 619.8234
Epoch 99/200, Loss: 619.8205
Epoch 100/200, Loss: 619.7895
Epoch 101/200, Loss: 619.8533
Epoch 102/200, Loss: 619.8224
Epoch 103/200, Loss: 619.8605
Epoch 104/200, Loss: 619.8275
Epoch 105/200, Loss: 619.8491
Epoch 106/200, Loss: 620.0323
Epoch 107/200, Loss: 619.8412
Epoch 108/200, Loss: 619.8835
Epoch 109/200, Loss: 619.8647
Epoch 110/200, Loss: 619.7559
Epoch 111/200, Loss: 619.8802
Epoch 112/200, Loss: 619.8555
Epoch 113/200, Loss: 619.9121
Epoch 114/200, Loss: 619.8346
Epoch 115/200, Loss: 619.8923
Epoch 116/200, Loss: 619.8787
Epoch 117/200, Loss: 619.9725
Epoch 118/200, Loss: 619.9569
Epoch 119/200, Loss: 619.8638
Epoch 120/200, Loss: 619.8611
Epoch 121/200, Loss: 619.9150
Epoch 122/200, Loss: 619.8590
Epoch 123/200, Loss: 619.8293
Epoch 124/200, Loss: 619.8131
Epoch 125/200, Loss: 619.8612
Epoch 126/200, Loss: 619.9113
Epoch 127/200, Loss: 619.9254
Epoch 128/200, Loss: 619.9670
Epoch 129/200, Loss: 619.9220
Epoch 130/200, Loss: 619.8696
Epoch 131/200, Loss: 619.8374
Epoch 132/200, Loss: 619.9051
Epoch 133/200, Loss: 619.9158
Epoch 134/200, Loss: 619.9051
Epoch 135/200, Loss: 619.8184
Epoch 136/200, Loss: 619.8506
Epoch 137/200, Loss: 619.8821
Epoch 138/200, Loss: 619.9287
Epoch 139/200, Loss: 619.8439
Epoch 140/200, Loss: 619.7756
Epoch 141/200, Loss: 619.9122
Epoch 142/200, Loss: 619.8550
Epoch 143/200, Loss: 619.9035
Epoch 144/200, Loss: 619.7960
Epoch 145/200, Loss: 619.8934
Epoch 146/200, Loss: 619.8492
Epoch 147/200, Loss: 619.8669
Epoch 148/200, Loss: 619.8257
Epoch 149/200, Loss: 619.8928
Epoch 150/200, Loss: 619.9136
Epoch 151/200, Loss: 619.8021
Epoch 152/200, Loss: 619.8011
Epoch 153/200, Loss: 619.8389
Epoch 154/200, Loss: 619.8681
Epoch 155/200, Loss: 619.8163
Epoch 156/200, Loss: 619.8542
Epoch 157/200, Loss: 619.8777
Epoch 158/200, Loss: 619.8404
Epoch 159/200, Loss: 619.8209
Epoch 160/200, Loss: 619.9804
Epoch 161/200, Loss: 619.9059
Epoch 162/200, Loss: 620.0196
Epoch 163/200, Loss: 619.8696
Epoch 164/200, Loss: 619.7834
Epoch 165/200, Loss: 619.9464
Epoch 166/200, Loss: 619.9331
Epoch 167/200, Loss: 619.9060
Epoch 168/200, Loss: 619.9546
Epoch 169/200, Loss: 619.8397
Epoch 170/200, Loss: 619.7836
Epoch 171/200, Loss: 619.8019
Epoch 172/200, Loss: 619.9700
Epoch 173/200, Loss: 619.8693
Epoch 174/200, Loss: 619.8189
Epoch 175/200, Loss: 619.8293
Epoch 176/200, Loss: 619.9042
Epoch 177/200, Loss: 619.8154
Epoch 178/200, Loss: 619.9086
Epoch 179/200, Loss: 619.8666
Epoch 180/200, Loss: 619.9170
Epoch 181/200, Loss: 619.8709
Epoch 182/200, Loss: 619.9367
Epoch 183/200, Loss: 619.9094
Epoch 184/200, Loss: 619.8451
Epoch 185/200, Loss: 619.9205
Epoch 186/200, Loss: 619.8079
Epoch 187/200, Loss: 619.8593
Epoch 188/200, Loss: 619.8868
Epoch 189/200, Loss: 619.8170
Epoch 190/200, Loss: 619.9656
Epoch 191/200, Loss: 619.9552
Epoch 192/200, Loss: 619.8418
Epoch 193/200, Loss: 619.9411
Epoch 194/200, Loss: 619.8498
Epoch 195/200, Loss: 619.8134
Epoch 196/200, Loss: 619.8456
Epoch 197/200, Loss: 619.8548
Epoch 198/200, Loss: 619.9186
Epoch 199/200, Loss: 619.8103
Epoch 200/200, Loss: 619.8904
Train Accuracy: 82.80% | Test Accuracy: 81.41%
Precision: 0.8214 | Recall: 0.8141 | F1-Score: 0.8065
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 25.05%
Precision: 0.2349 | Recall: 0.2505 | F1-Score: 0.2391


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1083 batches, Testing set - 122 batches
Epoch 1/200, Loss: 4055.5560
Epoch 2/200, Loss: 3097.9937
Epoch 3/200, Loss: 3003.4459
Epoch 4/200, Loss: 2932.7306
Epoch 5/200, Loss: 2876.0916
Epoch 6/200, Loss: 2810.4326
Epoch 7/200, Loss: 2802.4757
Epoch 8/200, Loss: 2797.3169
Epoch 9/200, Loss: 2791.8124
Epoch 10/200, Loss: 2786.2388
Epoch 11/200, Loss: 2779.9953
Epoch 12/200, Loss: 2778.7185
Epoch 13/200, Loss: 2778.3475
Epoch 14/200, Loss: 2777.6869
Epoch 15/200, Loss: 2777.2170
Epoch 16/200, Loss: 2776.6093
Epoch 17/200, Loss: 2776.4370
Epoch 18/200, Loss: 2776.4667
Epoch 19/200, Loss: 2776.3637
Epoch 20/200, Loss: 2776.3581
Epoch 21/200, Loss: 2776.2368
Epoch 22/200, Loss: 2776.2745
Epoch 23/200, Loss: 2776.1448
Epoch 24/200, Loss: 2776.1037
Epoch 25/200, Loss: 2776.2512
Epoch 26/200, Loss: 2776.2656
Epoch 27/200, Loss: 2776.0763
Epoch 28/200, Loss: 2776.2136
Epoch 29/200, Loss: 2776.0957
Epoch 30/200, Loss: 2776.1485
Epoch 31/200, Loss: 2776.2368
Epoch 32/200, Loss: 2776.2302
Epoch 33/200, Loss: 2776.2579
Epoch 34/200, Loss: 2776.2541
Epoch 35/200, Loss: 2776.1729
Epoch 36/200, Loss: 2776.1972
Epoch 37/200, Loss: 2776.2213
Epoch 38/200, Loss: 2776.1608
Epoch 39/200, Loss: 2776.1244
Epoch 40/200, Loss: 2776.1588
Epoch 41/200, Loss: 2776.2345
Epoch 42/200, Loss: 2776.2018
Epoch 43/200, Loss: 2776.1850
Epoch 44/200, Loss: 2776.1056
Epoch 45/200, Loss: 2776.1728
Epoch 46/200, Loss: 2776.1483
Epoch 47/200, Loss: 2776.1644
Epoch 48/200, Loss: 2776.1970
Epoch 49/200, Loss: 2776.1827
Epoch 50/200, Loss: 2776.2214
Epoch 51/200, Loss: 2776.1676
Epoch 52/200, Loss: 2776.1939
Epoch 53/200, Loss: 2776.2721
Epoch 54/200, Loss: 2776.2486
Epoch 55/200, Loss: 2776.0815
Epoch 56/200, Loss: 2776.2166
Epoch 57/200, Loss: 2776.2384
Epoch 58/200, Loss: 2776.1804
Epoch 59/200, Loss: 2776.2417
Epoch 60/200, Loss: 2776.0444
Epoch 61/200, Loss: 2776.2037
Epoch 62/200, Loss: 2776.3274
Epoch 63/200, Loss: 2776.1969
Epoch 64/200, Loss: 2776.1334
Epoch 65/200, Loss: 2776.1516
Epoch 66/200, Loss: 2776.1493
Epoch 67/200, Loss: 2776.1207
Epoch 68/200, Loss: 2776.1858
Epoch 69/200, Loss: 2776.2634
Epoch 70/200, Loss: 2776.1550
Epoch 71/200, Loss: 2776.0992
Epoch 72/200, Loss: 2776.1396
Epoch 73/200, Loss: 2776.1357
Epoch 74/200, Loss: 2776.1503
Epoch 75/200, Loss: 2776.1504
Epoch 76/200, Loss: 2776.2072
Epoch 77/200, Loss: 2776.1957
Epoch 78/200, Loss: 2776.3031
Epoch 79/200, Loss: 2776.2332
Epoch 80/200, Loss: 2776.3530
Epoch 81/200, Loss: 2776.1277
Epoch 82/200, Loss: 2776.1587
Epoch 83/200, Loss: 2776.2267
Epoch 84/200, Loss: 2776.1320
Epoch 85/200, Loss: 2776.1524
Epoch 86/200, Loss: 2776.1724
Epoch 87/200, Loss: 2776.1347
Epoch 88/200, Loss: 2776.3021
Epoch 89/200, Loss: 2776.1493
Epoch 90/200, Loss: 2776.2437
Epoch 91/200, Loss: 2776.1479
Epoch 92/200, Loss: 2776.0954
Epoch 93/200, Loss: 2776.1640
Epoch 94/200, Loss: 2776.1239
Epoch 95/200, Loss: 2776.2183
Epoch 96/200, Loss: 2776.2381
Epoch 97/200, Loss: 2776.2085
Epoch 98/200, Loss: 2776.1739
Epoch 99/200, Loss: 2776.2321
Epoch 100/200, Loss: 2776.1373
Epoch 101/200, Loss: 2776.1928
Epoch 102/200, Loss: 2776.2194
Epoch 103/200, Loss: 2776.1502
Epoch 104/200, Loss: 2776.1597
Epoch 105/200, Loss: 2776.2268
Epoch 106/200, Loss: 2776.1138
Epoch 107/200, Loss: 2776.1756
Epoch 108/200, Loss: 2776.2123
Epoch 109/200, Loss: 2776.2310
Epoch 110/200, Loss: 2776.1358
Epoch 111/200, Loss: 2776.0586
Epoch 112/200, Loss: 2776.1635
Epoch 113/200, Loss: 2776.1196
Epoch 114/200, Loss: 2776.2125
Epoch 115/200, Loss: 2776.2090
Epoch 116/200, Loss: 2776.1202
Epoch 117/200, Loss: 2776.2645
Epoch 118/200, Loss: 2776.2206
Epoch 119/200, Loss: 2776.1805
Epoch 120/200, Loss: 2776.1918
Epoch 121/200, Loss: 2776.1034
Epoch 122/200, Loss: 2776.2325
Epoch 123/200, Loss: 2776.1199
Epoch 124/200, Loss: 2776.1214
Epoch 125/200, Loss: 2776.2007
Epoch 126/200, Loss: 2776.1802
Epoch 127/200, Loss: 2776.1852
Epoch 128/200, Loss: 2776.1330
Epoch 129/200, Loss: 2776.2197
Epoch 130/200, Loss: 2776.2336
Epoch 131/200, Loss: 2776.1718
Epoch 132/200, Loss: 2776.2384
Epoch 133/200, Loss: 2776.2687
Epoch 134/200, Loss: 2776.2128
Epoch 135/200, Loss: 2776.1985
Epoch 136/200, Loss: 2776.1398
Epoch 137/200, Loss: 2776.2066
Epoch 138/200, Loss: 2776.2065
Epoch 139/200, Loss: 2776.3372
Epoch 140/200, Loss: 2776.3229
Epoch 141/200, Loss: 2776.1436
Epoch 142/200, Loss: 2776.2154
Epoch 143/200, Loss: 2776.1463
Epoch 144/200, Loss: 2776.2155
Epoch 145/200, Loss: 2776.2432
Epoch 146/200, Loss: 2776.1361
Epoch 147/200, Loss: 2776.1550
Epoch 148/200, Loss: 2776.2232
Epoch 149/200, Loss: 2776.1013
Epoch 150/200, Loss: 2776.2665
Epoch 151/200, Loss: 2776.1862
Epoch 152/200, Loss: 2776.1426
Epoch 153/200, Loss: 2776.2908
Epoch 154/200, Loss: 2776.1493
Epoch 155/200, Loss: 2776.1168
Epoch 156/200, Loss: 2776.1554
Epoch 157/200, Loss: 2776.2518
Epoch 158/200, Loss: 2776.1467
Epoch 159/200, Loss: 2776.2923
Epoch 160/200, Loss: 2776.2571
Epoch 161/200, Loss: 2776.1689
Epoch 162/200, Loss: 2776.1844
Epoch 163/200, Loss: 2776.2922
Epoch 164/200, Loss: 2776.2911
Epoch 165/200, Loss: 2776.1525
Epoch 166/200, Loss: 2776.1746
Epoch 167/200, Loss: 2776.1676
Epoch 168/200, Loss: 2776.1570
Epoch 169/200, Loss: 2776.2199
Epoch 170/200, Loss: 2776.2022
Epoch 171/200, Loss: 2776.2274
Epoch 172/200, Loss: 2776.1790
Epoch 173/200, Loss: 2776.1453
Epoch 174/200, Loss: 2776.2437
Epoch 175/200, Loss: 2776.1403
Epoch 176/200, Loss: 2776.1675
Epoch 177/200, Loss: 2776.2368
Epoch 178/200, Loss: 2776.1529
Epoch 179/200, Loss: 2776.2217
Epoch 180/200, Loss: 2776.2292
Epoch 181/200, Loss: 2776.2125
Epoch 182/200, Loss: 2776.2061
Epoch 183/200, Loss: 2776.1242
Epoch 184/200, Loss: 2776.1978
Epoch 185/200, Loss: 2776.1341
Epoch 186/200, Loss: 2776.1923
Epoch 187/200, Loss: 2776.3689
Epoch 188/200, Loss: 2776.1969
Epoch 189/200, Loss: 2776.1998
Epoch 190/200, Loss: 2776.2427
Epoch 191/200, Loss: 2776.1717
Epoch 192/200, Loss: 2776.2627
Epoch 193/200, Loss: 2776.2504
Epoch 194/200, Loss: 2776.2178
Epoch 195/200, Loss: 2776.1843
Epoch 196/200, Loss: 2776.2118
Epoch 197/200, Loss: 2776.2890
Epoch 198/200, Loss: 2776.2342
Epoch 199/200, Loss: 2776.2054
Epoch 200/200, Loss: 2776.1403
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 21.91% | Test Accuracy: 24.21%
Precision: 0.1781 | Recall: 0.2421 | F1-Score: 0.1753

Processing Subject 8...
Top 32 discriminative features: [26 46 44 41 29 38 14 12 45 42 58 60 18 16 22 25 37 40  9 33 21 54 56 52
  1 30 55 34 59 28 13 48]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69490
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1729
		Class 12: 1651
		Class 13: 1795
		Class 14: 1800
		Class 15: 1793
		Class 16: 1708
		Class 17: 1654
		Class 18: 1753
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1800
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1660
	# of Testing Samples: 7597
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 155
		Class 12: 195
		Class 13: 194
		Class 14: 200
		Class 15: 200
		Class 16: 150
		Class 17: 198
		Class 18: 190
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 157
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 158
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 69.66%
Precision: 0.7632 | Recall: 0.6966 | F1-Score: 0.7025


Running Deep Learning Classifier...
DataLoader: Training set - 1086 batches, Testing set - 119 batches
Epoch 1/200, Loss: 2220.6558
Epoch 2/200, Loss: 1153.1695
Epoch 3/200, Loss: 908.0890
Epoch 4/200, Loss: 770.6825
Epoch 5/200, Loss: 682.1706
Epoch 6/200, Loss: 626.4239
Epoch 7/200, Loss: 618.6032
Epoch 8/200, Loss: 612.3475
Epoch 9/200, Loss: 606.5879
Epoch 10/200, Loss: 601.1057
Epoch 11/200, Loss: 595.8313
Epoch 12/200, Loss: 594.9446
Epoch 13/200, Loss: 594.3678
Epoch 14/200, Loss: 593.8003
Epoch 15/200, Loss: 593.2207
Epoch 16/200, Loss: 592.6692
Epoch 17/200, Loss: 592.5906
Epoch 18/200, Loss: 592.5031
Epoch 19/200, Loss: 592.5331
Epoch 20/200, Loss: 592.4334
Epoch 21/200, Loss: 592.3299
Epoch 22/200, Loss: 592.3624
Epoch 23/200, Loss: 592.3697
Epoch 24/200, Loss: 592.3075
Epoch 25/200, Loss: 592.3386
Epoch 26/200, Loss: 592.3035
Epoch 27/200, Loss: 592.3064
Epoch 28/200, Loss: 592.3426
Epoch 29/200, Loss: 592.3099
Epoch 30/200, Loss: 592.3195
Epoch 31/200, Loss: 592.3145
Epoch 32/200, Loss: 592.3145
Epoch 33/200, Loss: 592.3404
Epoch 34/200, Loss: 592.2983
Epoch 35/200, Loss: 592.3723
Epoch 36/200, Loss: 592.3558
Epoch 37/200, Loss: 592.3184
Epoch 38/200, Loss: 592.3456
Epoch 39/200, Loss: 592.2914
Epoch 40/200, Loss: 592.3639
Epoch 41/200, Loss: 592.3279
Epoch 42/200, Loss: 592.4016
Epoch 43/200, Loss: 592.3405
Epoch 44/200, Loss: 592.3601
Epoch 45/200, Loss: 592.3227
Epoch 46/200, Loss: 592.3419
Epoch 47/200, Loss: 592.3426
Epoch 48/200, Loss: 592.3329
Epoch 49/200, Loss: 592.3463
Epoch 50/200, Loss: 592.3220
Epoch 51/200, Loss: 592.3200
Epoch 52/200, Loss: 592.3563
Epoch 53/200, Loss: 592.3344
Epoch 54/200, Loss: 592.3031
Epoch 55/200, Loss: 592.3362
Epoch 56/200, Loss: 592.3019
Epoch 57/200, Loss: 592.3457
Epoch 58/200, Loss: 592.3098
Epoch 59/200, Loss: 592.3644
Epoch 60/200, Loss: 592.3246
Epoch 61/200, Loss: 592.3213
Epoch 62/200, Loss: 592.3028
Epoch 63/200, Loss: 592.2832
Epoch 64/200, Loss: 592.3090
Epoch 65/200, Loss: 592.3295
Epoch 66/200, Loss: 592.3387
Epoch 67/200, Loss: 592.2932
Epoch 68/200, Loss: 592.3088
Epoch 69/200, Loss: 592.3826
Epoch 70/200, Loss: 592.3230
Epoch 71/200, Loss: 592.2745
Epoch 72/200, Loss: 592.3248
Epoch 73/200, Loss: 592.3051
Epoch 74/200, Loss: 592.2960
Epoch 75/200, Loss: 592.2766
Epoch 76/200, Loss: 592.3336
Epoch 77/200, Loss: 592.3492
Epoch 78/200, Loss: 592.2966
Epoch 79/200, Loss: 592.3242
Epoch 80/200, Loss: 592.3677
Epoch 81/200, Loss: 592.3273
Epoch 82/200, Loss: 592.3746
Epoch 83/200, Loss: 592.3331
Epoch 84/200, Loss: 592.3789
Epoch 85/200, Loss: 592.3336
Epoch 86/200, Loss: 592.2988
Epoch 87/200, Loss: 592.2930
Epoch 88/200, Loss: 592.3270
Epoch 89/200, Loss: 592.3087
Epoch 90/200, Loss: 592.3290
Epoch 91/200, Loss: 592.3586
Epoch 92/200, Loss: 592.3361
Epoch 93/200, Loss: 592.2886
Epoch 94/200, Loss: 592.3454
Epoch 95/200, Loss: 592.3216
Epoch 96/200, Loss: 592.3214
Epoch 97/200, Loss: 592.3032
Epoch 98/200, Loss: 592.3086
Epoch 99/200, Loss: 592.3249
Epoch 100/200, Loss: 592.3392
Epoch 101/200, Loss: 592.2960
Epoch 102/200, Loss: 592.3451
Epoch 103/200, Loss: 592.3029
Epoch 104/200, Loss: 592.2813
Epoch 105/200, Loss: 592.4014
Epoch 106/200, Loss: 592.3461
Epoch 107/200, Loss: 592.3249
Epoch 108/200, Loss: 592.3875
Epoch 109/200, Loss: 592.2968
Epoch 110/200, Loss: 592.3300
Epoch 111/200, Loss: 592.3386
Epoch 112/200, Loss: 592.3482
Epoch 113/200, Loss: 592.3782
Epoch 114/200, Loss: 592.3322
Epoch 115/200, Loss: 592.3141
Epoch 116/200, Loss: 592.3776
Epoch 117/200, Loss: 592.3332
Epoch 118/200, Loss: 592.3302
Epoch 119/200, Loss: 592.3373
Epoch 120/200, Loss: 592.3372
Epoch 121/200, Loss: 592.3206
Epoch 122/200, Loss: 592.2968
Epoch 123/200, Loss: 592.3550
Epoch 124/200, Loss: 592.3318
Epoch 125/200, Loss: 592.3284
Epoch 126/200, Loss: 592.3492
Epoch 127/200, Loss: 592.2904
Epoch 128/200, Loss: 592.3242
Epoch 129/200, Loss: 592.3927
Epoch 130/200, Loss: 592.3149
Epoch 131/200, Loss: 592.3327
Epoch 132/200, Loss: 592.3628
Epoch 133/200, Loss: 592.3797
Epoch 134/200, Loss: 592.3086
Epoch 135/200, Loss: 592.3229
Epoch 136/200, Loss: 592.3361
Epoch 137/200, Loss: 592.3251
Epoch 138/200, Loss: 592.3170
Epoch 139/200, Loss: 592.3237
Epoch 140/200, Loss: 592.3139
Epoch 141/200, Loss: 592.4029
Epoch 142/200, Loss: 592.3626
Epoch 143/200, Loss: 592.3569
Epoch 144/200, Loss: 592.3524
Epoch 145/200, Loss: 592.3769
Epoch 146/200, Loss: 592.3470
Epoch 147/200, Loss: 592.3317
Epoch 148/200, Loss: 592.3129
Epoch 149/200, Loss: 592.3112
Epoch 150/200, Loss: 592.3338
Epoch 151/200, Loss: 592.3289
Epoch 152/200, Loss: 592.3188
Epoch 153/200, Loss: 592.3275
Epoch 154/200, Loss: 592.3073
Epoch 155/200, Loss: 592.3355
Epoch 156/200, Loss: 592.3033
Epoch 157/200, Loss: 592.3523
Epoch 158/200, Loss: 592.3183
Epoch 159/200, Loss: 592.3632
Epoch 160/200, Loss: 592.3708
Epoch 161/200, Loss: 592.3639
Epoch 162/200, Loss: 592.3691
Epoch 163/200, Loss: 592.3109
Epoch 164/200, Loss: 592.3205
Epoch 165/200, Loss: 592.3436
Epoch 166/200, Loss: 592.3166
Epoch 167/200, Loss: 592.3271
Epoch 168/200, Loss: 592.3685
Epoch 169/200, Loss: 592.3171
Epoch 170/200, Loss: 592.3235
Epoch 171/200, Loss: 592.3575
Epoch 172/200, Loss: 592.3139
Epoch 173/200, Loss: 592.3486
Epoch 174/200, Loss: 592.3555
Epoch 175/200, Loss: 592.3418
Epoch 176/200, Loss: 592.2939
Epoch 177/200, Loss: 592.3357
Epoch 178/200, Loss: 592.3613
Epoch 179/200, Loss: 592.3276
Epoch 180/200, Loss: 592.3087
Epoch 181/200, Loss: 592.3360
Epoch 182/200, Loss: 592.3280
Epoch 183/200, Loss: 592.3622
Epoch 184/200, Loss: 592.3726
Epoch 185/200, Loss: 592.3094
Epoch 186/200, Loss: 592.2988
Epoch 187/200, Loss: 592.3271
Epoch 188/200, Loss: 592.3173
Epoch 189/200, Loss: 592.3471
Epoch 190/200, Loss: 592.3339
Epoch 191/200, Loss: 592.2859
Epoch 192/200, Loss: 592.3058
Epoch 193/200, Loss: 592.3482
Epoch 194/200, Loss: 592.3382
Epoch 195/200, Loss: 592.3039
Epoch 196/200, Loss: 592.3541
Epoch 197/200, Loss: 592.3847
Epoch 198/200, Loss: 592.3026
Epoch 199/200, Loss: 592.3145
Epoch 200/200, Loss: 592.3080
Train Accuracy: 83.31% | Test Accuracy: 66.76%
Precision: 0.6841 | Recall: 0.6676 | F1-Score: 0.6590
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 25.01%
Precision: 0.2500 | Recall: 0.2501 | F1-Score: 0.2395


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1086 batches, Testing set - 119 batches
Epoch 1/200, Loss: 3418.5579
Epoch 2/200, Loss: 2930.8623
Epoch 3/200, Loss: 2800.7921
Epoch 4/200, Loss: 2679.7744
Epoch 5/200, Loss: 2571.6446
Epoch 6/200, Loss: 2491.0327
Epoch 7/200, Loss: 2481.3047
Epoch 8/200, Loss: 2473.3718
Epoch 9/200, Loss: 2465.4913
Epoch 10/200, Loss: 2458.2674
Epoch 11/200, Loss: 2451.1729
Epoch 12/200, Loss: 2450.1245
Epoch 13/200, Loss: 2449.3223
Epoch 14/200, Loss: 2448.5928
Epoch 15/200, Loss: 2447.8946
Epoch 16/200, Loss: 2447.1616
Epoch 17/200, Loss: 2447.0036
Epoch 18/200, Loss: 2446.9332
Epoch 19/200, Loss: 2446.9110
Epoch 20/200, Loss: 2446.7823
Epoch 21/200, Loss: 2446.6970
Epoch 22/200, Loss: 2446.7611
Epoch 23/200, Loss: 2446.7149
Epoch 24/200, Loss: 2446.7535
Epoch 25/200, Loss: 2446.6476
Epoch 26/200, Loss: 2446.6656
Epoch 27/200, Loss: 2446.6842
Epoch 28/200, Loss: 2446.7288
Epoch 29/200, Loss: 2446.6755
Epoch 30/200, Loss: 2446.6597
Epoch 31/200, Loss: 2446.6170
Epoch 32/200, Loss: 2446.7241
Epoch 33/200, Loss: 2446.6741
Epoch 34/200, Loss: 2446.7188
Epoch 35/200, Loss: 2446.6596
Epoch 36/200, Loss: 2446.6530
Epoch 37/200, Loss: 2446.6675
Epoch 38/200, Loss: 2446.6383
Epoch 39/200, Loss: 2446.6652
Epoch 40/200, Loss: 2446.6848
Epoch 41/200, Loss: 2446.6856
Epoch 42/200, Loss: 2446.6977
Epoch 43/200, Loss: 2446.6988
Epoch 44/200, Loss: 2446.6921
Epoch 45/200, Loss: 2446.6827
Epoch 46/200, Loss: 2446.6561
Epoch 47/200, Loss: 2446.6739
Epoch 48/200, Loss: 2446.7117
Epoch 49/200, Loss: 2446.7045
Epoch 50/200, Loss: 2446.6784
Epoch 51/200, Loss: 2446.6985
Epoch 52/200, Loss: 2446.6593
Epoch 53/200, Loss: 2446.7348
Epoch 54/200, Loss: 2446.6881
Epoch 55/200, Loss: 2446.7041
Epoch 56/200, Loss: 2446.6869
Epoch 57/200, Loss: 2446.6815
Epoch 58/200, Loss: 2446.7236
Epoch 59/200, Loss: 2446.7178
Epoch 60/200, Loss: 2446.7186
Epoch 61/200, Loss: 2446.6819
Epoch 62/200, Loss: 2446.6868
Epoch 63/200, Loss: 2446.7310
Epoch 64/200, Loss: 2446.7255
Epoch 65/200, Loss: 2446.7412
Epoch 66/200, Loss: 2446.6645
Epoch 67/200, Loss: 2446.6932
Epoch 68/200, Loss: 2446.6384
Epoch 69/200, Loss: 2446.7126
Epoch 70/200, Loss: 2446.6869
Epoch 71/200, Loss: 2446.6693
Epoch 72/200, Loss: 2446.6308
Epoch 73/200, Loss: 2446.6358
Epoch 74/200, Loss: 2446.6952
Epoch 75/200, Loss: 2446.7800
Epoch 76/200, Loss: 2446.6761
Epoch 77/200, Loss: 2446.6961
Epoch 78/200, Loss: 2446.7317
Epoch 79/200, Loss: 2446.6841
Epoch 80/200, Loss: 2446.6651
Epoch 81/200, Loss: 2446.6932
Epoch 82/200, Loss: 2446.6739
Epoch 83/200, Loss: 2446.6599
Epoch 84/200, Loss: 2446.6953
Epoch 85/200, Loss: 2446.6156
Epoch 86/200, Loss: 2446.6727
Epoch 87/200, Loss: 2446.6291
Epoch 88/200, Loss: 2446.6665
Epoch 89/200, Loss: 2446.6523
Epoch 90/200, Loss: 2446.6960
Epoch 91/200, Loss: 2446.6597
Epoch 92/200, Loss: 2446.6912
Epoch 93/200, Loss: 2446.6765
Epoch 94/200, Loss: 2446.6494
Epoch 95/200, Loss: 2446.6141
Epoch 96/200, Loss: 2446.6857
Epoch 97/200, Loss: 2446.6721
Epoch 98/200, Loss: 2446.6819
Epoch 99/200, Loss: 2446.7237
Epoch 100/200, Loss: 2446.7217
Epoch 101/200, Loss: 2446.6458
Epoch 102/200, Loss: 2446.6898
Epoch 103/200, Loss: 2446.6772
Epoch 104/200, Loss: 2446.6726
Epoch 105/200, Loss: 2446.6755
Epoch 106/200, Loss: 2446.7786
Epoch 107/200, Loss: 2446.7205
Epoch 108/200, Loss: 2446.7034
Epoch 109/200, Loss: 2446.6676
Epoch 110/200, Loss: 2446.6735
Epoch 111/200, Loss: 2446.6484
Epoch 112/200, Loss: 2446.6971
Epoch 113/200, Loss: 2446.7156
Epoch 114/200, Loss: 2446.6709
Epoch 115/200, Loss: 2446.6455
Epoch 116/200, Loss: 2446.6780
Epoch 117/200, Loss: 2446.6893
Epoch 118/200, Loss: 2446.7432
Epoch 119/200, Loss: 2446.6603
Epoch 120/200, Loss: 2446.6937
Epoch 121/200, Loss: 2446.6867
Epoch 122/200, Loss: 2446.6806
Epoch 123/200, Loss: 2446.6765
Epoch 124/200, Loss: 2446.6504
Epoch 125/200, Loss: 2446.6887
Epoch 126/200, Loss: 2446.7138
Epoch 127/200, Loss: 2446.7455
Epoch 128/200, Loss: 2446.6997
Epoch 129/200, Loss: 2446.7716
Epoch 130/200, Loss: 2446.6871
Epoch 131/200, Loss: 2446.6017
Epoch 132/200, Loss: 2446.6741
Epoch 133/200, Loss: 2446.6958
Epoch 134/200, Loss: 2446.7593
Epoch 135/200, Loss: 2446.6911
Epoch 136/200, Loss: 2446.7086
Epoch 137/200, Loss: 2446.6700
Epoch 138/200, Loss: 2446.6874
Epoch 139/200, Loss: 2446.6758
Epoch 140/200, Loss: 2446.6756
Epoch 141/200, Loss: 2446.7208
Epoch 142/200, Loss: 2446.6767
Epoch 143/200, Loss: 2446.6003
Epoch 144/200, Loss: 2446.6895
Epoch 145/200, Loss: 2446.6660
Epoch 146/200, Loss: 2446.6766
Epoch 147/200, Loss: 2446.7031
Epoch 148/200, Loss: 2446.7134
Epoch 149/200, Loss: 2446.6551
Epoch 150/200, Loss: 2446.6528
Epoch 151/200, Loss: 2446.7949
Epoch 152/200, Loss: 2446.7029
Epoch 153/200, Loss: 2446.6973
Epoch 154/200, Loss: 2446.6878
Epoch 155/200, Loss: 2446.6761
Epoch 156/200, Loss: 2446.6619
Epoch 157/200, Loss: 2446.7087
Epoch 158/200, Loss: 2446.7054
Epoch 159/200, Loss: 2446.7186
Epoch 160/200, Loss: 2446.7940
Epoch 161/200, Loss: 2446.7262
Epoch 162/200, Loss: 2446.6192
Epoch 163/200, Loss: 2446.6308
Epoch 164/200, Loss: 2446.7091
Epoch 165/200, Loss: 2446.6755
Epoch 166/200, Loss: 2446.6165
Epoch 167/200, Loss: 2446.6843
Epoch 168/200, Loss: 2446.6848
Epoch 169/200, Loss: 2446.6357
Epoch 170/200, Loss: 2446.7981
Epoch 171/200, Loss: 2446.6650
Epoch 172/200, Loss: 2446.6889
Epoch 173/200, Loss: 2446.6966
Epoch 174/200, Loss: 2446.7041
Epoch 175/200, Loss: 2446.6692
Epoch 176/200, Loss: 2446.6448
Epoch 177/200, Loss: 2446.6439
Epoch 178/200, Loss: 2446.7057
Epoch 179/200, Loss: 2446.6792
Epoch 180/200, Loss: 2446.7248
Epoch 181/200, Loss: 2446.6550
Epoch 182/200, Loss: 2446.6340
Epoch 183/200, Loss: 2446.7397
Epoch 184/200, Loss: 2446.6802
Epoch 185/200, Loss: 2446.6576
Epoch 186/200, Loss: 2446.6220
Epoch 187/200, Loss: 2446.6822
Epoch 188/200, Loss: 2446.6699
Epoch 189/200, Loss: 2446.6753
Epoch 190/200, Loss: 2446.7346
Epoch 191/200, Loss: 2446.7151
Epoch 192/200, Loss: 2446.6448
Epoch 193/200, Loss: 2446.6796
Epoch 194/200, Loss: 2446.6634
Epoch 195/200, Loss: 2446.6321
Epoch 196/200, Loss: 2446.6358
Epoch 197/200, Loss: 2446.6426
Epoch 198/200, Loss: 2446.7180
Epoch 199/200, Loss: 2446.6177
Epoch 200/200, Loss: 2446.7305
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 33.32% | Test Accuracy: 27.88%
Precision: 0.2732 | Recall: 0.2788 | F1-Score: 0.2297

Processing Subject 9...
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:112: UserWarning: Features [ 3 15 39 43 47 57 61] are constant.
  warnings.warn("Features %s are constant." % constant_features_idx, UserWarning)
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide
  f = msb / msw
Top 32 discriminative features: [61 39 57  3 43 15 47 26 46 44 41 38 29 14 12 45 58 60 22 42 18 16  1 55
 30 40 25 52 37  9 21 54]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69561
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1800
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1716
		Class 12: 1715
		Class 13: 1794
		Class 14: 1800
		Class 15: 1793
		Class 16: 1680
		Class 17: 1720
		Class 18: 1768
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1779
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1650
	# of Testing Samples: 7526
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 184
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 168
		Class 12: 131
		Class 13: 195
		Class 14: 200
		Class 15: 200
		Class 16: 178
		Class 17: 132
		Class 18: 175
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 195
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 168
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 69.21%
Precision: 0.7604 | Recall: 0.6921 | F1-Score: 0.7017


Running Deep Learning Classifier...
DataLoader: Training set - 1087 batches, Testing set - 118 batches
Epoch 1/200, Loss: 2210.5646
Epoch 2/200, Loss: 1150.3258
Epoch 3/200, Loss: 921.3369
Epoch 4/200, Loss: 791.1038
Epoch 5/200, Loss: 704.4281
Epoch 6/200, Loss: 647.6214
Epoch 7/200, Loss: 639.4148
Epoch 8/200, Loss: 633.5828
Epoch 9/200, Loss: 627.5783
Epoch 10/200, Loss: 621.9564
Epoch 11/200, Loss: 616.8287
Epoch 12/200, Loss: 615.8058
Epoch 13/200, Loss: 615.2379
Epoch 14/200, Loss: 614.6351
Epoch 15/200, Loss: 614.1301
Epoch 16/200, Loss: 613.5493
Epoch 17/200, Loss: 613.4713
Epoch 18/200, Loss: 613.3721
Epoch 19/200, Loss: 613.3255
Epoch 20/200, Loss: 613.2675
Epoch 21/200, Loss: 613.1819
Epoch 22/200, Loss: 613.2051
Epoch 23/200, Loss: 613.1824
Epoch 24/200, Loss: 613.1782
Epoch 25/200, Loss: 613.1787
Epoch 26/200, Loss: 613.1826
Epoch 27/200, Loss: 613.1712
Epoch 28/200, Loss: 613.1700
Epoch 29/200, Loss: 613.1650
Epoch 30/200, Loss: 613.1757
Epoch 31/200, Loss: 613.1860
Epoch 32/200, Loss: 613.1614
Epoch 33/200, Loss: 613.1795
Epoch 34/200, Loss: 613.1668
Epoch 35/200, Loss: 613.1886
Epoch 36/200, Loss: 613.1929
Epoch 37/200, Loss: 613.1706
Epoch 38/200, Loss: 613.1641
Epoch 39/200, Loss: 613.1878
Epoch 40/200, Loss: 613.1677
Epoch 41/200, Loss: 613.1728
Epoch 42/200, Loss: 613.1835
Epoch 43/200, Loss: 613.1706
Epoch 44/200, Loss: 613.1738
Epoch 45/200, Loss: 613.1727
Epoch 46/200, Loss: 613.1686
Epoch 47/200, Loss: 613.1883
Epoch 48/200, Loss: 613.1671
Epoch 49/200, Loss: 613.1807
Epoch 50/200, Loss: 613.1773
Epoch 51/200, Loss: 613.1605
Epoch 52/200, Loss: 613.1926
Epoch 53/200, Loss: 613.1555
Epoch 54/200, Loss: 613.1666
Epoch 55/200, Loss: 613.1587
Epoch 56/200, Loss: 613.1668
Epoch 57/200, Loss: 613.1689
Epoch 58/200, Loss: 613.2061
Epoch 59/200, Loss: 613.1498
Epoch 60/200, Loss: 613.1842
Epoch 61/200, Loss: 613.1623
Epoch 62/200, Loss: 613.1927
Epoch 63/200, Loss: 613.1616
Epoch 64/200, Loss: 613.1905
Epoch 65/200, Loss: 613.1777
Epoch 66/200, Loss: 613.1797
Epoch 67/200, Loss: 613.1442
Epoch 68/200, Loss: 613.1693
Epoch 69/200, Loss: 613.1668
Epoch 70/200, Loss: 613.1780
Epoch 71/200, Loss: 613.1585
Epoch 72/200, Loss: 613.1642
Epoch 73/200, Loss: 613.1560
Epoch 74/200, Loss: 613.1716
Epoch 75/200, Loss: 613.1598
Epoch 76/200, Loss: 613.1828
Epoch 77/200, Loss: 613.1840
Epoch 78/200, Loss: 613.1930
Epoch 79/200, Loss: 613.1643
Epoch 80/200, Loss: 613.1651
Epoch 81/200, Loss: 613.1689
Epoch 82/200, Loss: 613.1852
Epoch 83/200, Loss: 613.1635
Epoch 84/200, Loss: 613.1960
Epoch 85/200, Loss: 613.1724
Epoch 86/200, Loss: 613.2258
Epoch 87/200, Loss: 613.1834
Epoch 88/200, Loss: 613.1820
Epoch 89/200, Loss: 613.1799
Epoch 90/200, Loss: 613.1671
Epoch 91/200, Loss: 613.1652
Epoch 92/200, Loss: 613.1748
Epoch 93/200, Loss: 613.1433
Epoch 94/200, Loss: 613.1685
Epoch 95/200, Loss: 613.1664
Epoch 96/200, Loss: 613.1733
Epoch 97/200, Loss: 613.1556
Epoch 98/200, Loss: 613.1809
Epoch 99/200, Loss: 613.1739
Epoch 100/200, Loss: 613.1692
Epoch 101/200, Loss: 613.1601
Epoch 102/200, Loss: 613.1701
Epoch 103/200, Loss: 613.1574
Epoch 104/200, Loss: 613.1691
Epoch 105/200, Loss: 613.1637
Epoch 106/200, Loss: 613.1872
Epoch 107/200, Loss: 613.1961
Epoch 108/200, Loss: 613.1910
Epoch 109/200, Loss: 613.1582
Epoch 110/200, Loss: 613.1897
Epoch 111/200, Loss: 613.1655
Epoch 112/200, Loss: 613.1769
Epoch 113/200, Loss: 613.1713
Epoch 114/200, Loss: 613.1757
Epoch 115/200, Loss: 613.1564
Epoch 116/200, Loss: 613.1631
Epoch 117/200, Loss: 613.1732
Epoch 118/200, Loss: 613.1582
Epoch 119/200, Loss: 613.2004
Epoch 120/200, Loss: 613.1707
Epoch 121/200, Loss: 613.1713
Epoch 122/200, Loss: 613.1866
Epoch 123/200, Loss: 613.1624
Epoch 124/200, Loss: 613.1849
Epoch 125/200, Loss: 613.1664
Epoch 126/200, Loss: 613.1649
Epoch 127/200, Loss: 613.1647
Epoch 128/200, Loss: 613.1810
Epoch 129/200, Loss: 613.1615
Epoch 130/200, Loss: 613.1838
Epoch 131/200, Loss: 613.1672
Epoch 132/200, Loss: 613.1521
Epoch 133/200, Loss: 613.1828
Epoch 134/200, Loss: 613.1713
Epoch 135/200, Loss: 613.1681
Epoch 136/200, Loss: 613.1493
Epoch 137/200, Loss: 613.1787
Epoch 138/200, Loss: 613.1881
Epoch 139/200, Loss: 613.1828
Epoch 140/200, Loss: 613.1742
Epoch 141/200, Loss: 613.1655
Epoch 142/200, Loss: 613.1536
Epoch 143/200, Loss: 613.1690
Epoch 144/200, Loss: 613.1952
Epoch 145/200, Loss: 613.1794
Epoch 146/200, Loss: 613.1518
Epoch 147/200, Loss: 613.1590
Epoch 148/200, Loss: 613.1743
Epoch 149/200, Loss: 613.1429
Epoch 150/200, Loss: 613.1761
Epoch 151/200, Loss: 613.1655
Epoch 152/200, Loss: 613.1573
Epoch 153/200, Loss: 613.1593
Epoch 154/200, Loss: 613.1665
Epoch 155/200, Loss: 613.1787
Epoch 156/200, Loss: 613.1824
Epoch 157/200, Loss: 613.1672
Epoch 158/200, Loss: 613.1566
Epoch 159/200, Loss: 613.1613
Epoch 160/200, Loss: 613.1668
Epoch 161/200, Loss: 613.1857
Epoch 162/200, Loss: 613.1681
Epoch 163/200, Loss: 613.1622
Epoch 164/200, Loss: 613.1766
Epoch 165/200, Loss: 613.1952
Epoch 166/200, Loss: 613.1724
Epoch 167/200, Loss: 613.1643
Epoch 168/200, Loss: 613.1767
Epoch 169/200, Loss: 613.1919
Epoch 170/200, Loss: 613.1742
Epoch 171/200, Loss: 613.1683
Epoch 172/200, Loss: 613.1677
Epoch 173/200, Loss: 613.1878
Epoch 174/200, Loss: 613.2069
Epoch 175/200, Loss: 613.1803
Epoch 176/200, Loss: 613.1812
Epoch 177/200, Loss: 613.1664
Epoch 178/200, Loss: 613.1993
Epoch 179/200, Loss: 613.1669
Epoch 180/200, Loss: 613.1871
Epoch 181/200, Loss: 613.1947
Epoch 182/200, Loss: 613.1642
Epoch 183/200, Loss: 613.1610
Epoch 184/200, Loss: 613.1740
Epoch 185/200, Loss: 613.1610
Epoch 186/200, Loss: 613.1800
Epoch 187/200, Loss: 613.1584
Epoch 188/200, Loss: 613.1668
Epoch 189/200, Loss: 613.1797
Epoch 190/200, Loss: 613.1904
Epoch 191/200, Loss: 613.1801
Epoch 192/200, Loss: 613.1765
Epoch 193/200, Loss: 613.1714
Epoch 194/200, Loss: 613.1711
Epoch 195/200, Loss: 613.1669
Epoch 196/200, Loss: 613.1803
Epoch 197/200, Loss: 613.1708
Epoch 198/200, Loss: 613.1897
Epoch 199/200, Loss: 613.1555
Epoch 200/200, Loss: 613.1657
Train Accuracy: 83.01% | Test Accuracy: 69.86%
Precision: 0.7176 | Recall: 0.6986 | F1-Score: 0.6874
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 22.08%
Precision: 0.2117 | Recall: 0.2208 | F1-Score: 0.2086


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1087 batches, Testing set - 118 batches
Epoch 1/200, Loss: 3635.6937
Epoch 2/200, Loss: 3007.4287
Epoch 3/200, Loss: 2902.5150
Epoch 4/200, Loss: 2837.7539
Epoch 5/200, Loss: 2795.1135
Epoch 6/200, Loss: 2755.6545
Epoch 7/200, Loss: 2750.8276
Epoch 8/200, Loss: 2747.1976
Epoch 9/200, Loss: 2743.9772
Epoch 10/200, Loss: 2740.8761
Epoch 11/200, Loss: 2737.0540
Epoch 12/200, Loss: 2736.3466
Epoch 13/200, Loss: 2735.9676
Epoch 14/200, Loss: 2735.6522
Epoch 15/200, Loss: 2735.3129
Epoch 16/200, Loss: 2734.9212
Epoch 17/200, Loss: 2734.8631
Epoch 18/200, Loss: 2734.7939
Epoch 19/200, Loss: 2734.7552
Epoch 20/200, Loss: 2734.7500
Epoch 21/200, Loss: 2734.6670
Epoch 22/200, Loss: 2734.6691
Epoch 23/200, Loss: 2734.6746
Epoch 24/200, Loss: 2734.6883
Epoch 25/200, Loss: 2734.6671
Epoch 26/200, Loss: 2734.6633
Epoch 27/200, Loss: 2734.6698
Epoch 28/200, Loss: 2734.6637
Epoch 29/200, Loss: 2734.6973
Epoch 30/200, Loss: 2734.6475
Epoch 31/200, Loss: 2734.6739
Epoch 32/200, Loss: 2734.6525
Epoch 33/200, Loss: 2734.6749
Epoch 34/200, Loss: 2734.6715
Epoch 35/200, Loss: 2734.6728
Epoch 36/200, Loss: 2734.6564
Epoch 37/200, Loss: 2734.6515
Epoch 38/200, Loss: 2734.6734
Epoch 39/200, Loss: 2734.6676
Epoch 40/200, Loss: 2734.6683
Epoch 41/200, Loss: 2734.6735
Epoch 42/200, Loss: 2734.6657
Epoch 43/200, Loss: 2734.6514
Epoch 44/200, Loss: 2734.6714
Epoch 45/200, Loss: 2734.6347
Epoch 46/200, Loss: 2734.6729
Epoch 47/200, Loss: 2734.6507
Epoch 48/200, Loss: 2734.6748
Epoch 49/200, Loss: 2734.6601
Epoch 50/200, Loss: 2734.6489
Epoch 51/200, Loss: 2734.6479
Epoch 52/200, Loss: 2734.6520
Epoch 53/200, Loss: 2734.6641
Epoch 54/200, Loss: 2734.6586
Epoch 55/200, Loss: 2734.6597
Epoch 56/200, Loss: 2734.6558
Epoch 57/200, Loss: 2734.6606
Epoch 58/200, Loss: 2734.6373
Epoch 59/200, Loss: 2734.6491
Epoch 60/200, Loss: 2734.6329
Epoch 61/200, Loss: 2734.6483
Epoch 62/200, Loss: 2734.6782
Epoch 63/200, Loss: 2734.6769
Epoch 64/200, Loss: 2734.6554
Epoch 65/200, Loss: 2734.6655
Epoch 66/200, Loss: 2734.7110
Epoch 67/200, Loss: 2734.6744
Epoch 68/200, Loss: 2734.6665
Epoch 69/200, Loss: 2734.6675
Epoch 70/200, Loss: 2734.6950
Epoch 71/200, Loss: 2734.6787
Epoch 72/200, Loss: 2734.6427
Epoch 73/200, Loss: 2734.6844
Epoch 74/200, Loss: 2734.6635
Epoch 75/200, Loss: 2734.6476
Epoch 76/200, Loss: 2734.6766
Epoch 77/200, Loss: 2734.6510
Epoch 78/200, Loss: 2734.6753
Epoch 79/200, Loss: 2734.6819
Epoch 80/200, Loss: 2734.6565
Epoch 81/200, Loss: 2734.6673
Epoch 82/200, Loss: 2734.6788
Epoch 83/200, Loss: 2734.6560
Epoch 84/200, Loss: 2734.6471
Epoch 85/200, Loss: 2734.6620
Epoch 86/200, Loss: 2734.6559
Epoch 87/200, Loss: 2734.6557
Epoch 88/200, Loss: 2734.7036
Epoch 89/200, Loss: 2734.6660
Epoch 90/200, Loss: 2734.6573
Epoch 91/200, Loss: 2734.6586
Epoch 92/200, Loss: 2734.6499
Epoch 93/200, Loss: 2734.6387
Epoch 94/200, Loss: 2734.6741
Epoch 95/200, Loss: 2734.6834
Epoch 96/200, Loss: 2734.6545
Epoch 97/200, Loss: 2734.6442
Epoch 98/200, Loss: 2734.6698
Epoch 99/200, Loss: 2734.6401
Epoch 100/200, Loss: 2734.6553
Epoch 101/200, Loss: 2734.6320
Epoch 102/200, Loss: 2734.6677
Epoch 103/200, Loss: 2734.6607
Epoch 104/200, Loss: 2734.6471
Epoch 105/200, Loss: 2734.6736
Epoch 106/200, Loss: 2734.6442
Epoch 107/200, Loss: 2734.6445
Epoch 108/200, Loss: 2734.6813
Epoch 109/200, Loss: 2734.6768
Epoch 110/200, Loss: 2734.6555
Epoch 111/200, Loss: 2734.6797
Epoch 112/200, Loss: 2734.6689
Epoch 113/200, Loss: 2734.6515
Epoch 114/200, Loss: 2734.6810
Epoch 115/200, Loss: 2734.6489
Epoch 116/200, Loss: 2734.6597
Epoch 117/200, Loss: 2734.6589
Epoch 118/200, Loss: 2734.6457
Epoch 119/200, Loss: 2734.6805
Epoch 120/200, Loss: 2734.6826
Epoch 121/200, Loss: 2734.6653
Epoch 122/200, Loss: 2734.6537
Epoch 123/200, Loss: 2734.6770
Epoch 124/200, Loss: 2734.6665
Epoch 125/200, Loss: 2734.6501
Epoch 126/200, Loss: 2734.6654
Epoch 127/200, Loss: 2734.6511
Epoch 128/200, Loss: 2734.6651
Epoch 129/200, Loss: 2734.6588
Epoch 130/200, Loss: 2734.6587
Epoch 131/200, Loss: 2734.6544
Epoch 132/200, Loss: 2734.6584
Epoch 133/200, Loss: 2734.6799
Epoch 134/200, Loss: 2734.6397
Epoch 135/200, Loss: 2734.6845
Epoch 136/200, Loss: 2734.6482
Epoch 137/200, Loss: 2734.6644
Epoch 138/200, Loss: 2734.6719
Epoch 139/200, Loss: 2734.6632
Epoch 140/200, Loss: 2734.6586
Epoch 141/200, Loss: 2734.6627
Epoch 142/200, Loss: 2734.6578
Epoch 143/200, Loss: 2734.6577
Epoch 144/200, Loss: 2734.6766
Epoch 145/200, Loss: 2734.6627
Epoch 146/200, Loss: 2734.6974
Epoch 147/200, Loss: 2734.6614
Epoch 148/200, Loss: 2734.6756
Epoch 149/200, Loss: 2734.6302
Epoch 150/200, Loss: 2734.6795
Epoch 151/200, Loss: 2734.6580
Epoch 152/200, Loss: 2734.6404
Epoch 153/200, Loss: 2734.6600
Epoch 154/200, Loss: 2734.6569
Epoch 155/200, Loss: 2734.6904
Epoch 156/200, Loss: 2734.6545
Epoch 157/200, Loss: 2734.6624
Epoch 158/200, Loss: 2734.6509
Epoch 159/200, Loss: 2734.6892
Epoch 160/200, Loss: 2734.6461
Epoch 161/200, Loss: 2734.6670
Epoch 162/200, Loss: 2734.6860
Epoch 163/200, Loss: 2734.6794
Epoch 164/200, Loss: 2734.6625
Epoch 165/200, Loss: 2734.6699
Epoch 166/200, Loss: 2734.6840
Epoch 167/200, Loss: 2734.6646
Epoch 168/200, Loss: 2734.6427
Epoch 169/200, Loss: 2734.6572
Epoch 170/200, Loss: 2734.6733
Epoch 171/200, Loss: 2734.6816
Epoch 172/200, Loss: 2734.6737
Epoch 173/200, Loss: 2734.6469
Epoch 174/200, Loss: 2734.6521
Epoch 175/200, Loss: 2734.6389
Epoch 176/200, Loss: 2734.6480
Epoch 177/200, Loss: 2734.6459
Epoch 178/200, Loss: 2734.6564
Epoch 179/200, Loss: 2734.6587
Epoch 180/200, Loss: 2734.6500
Epoch 181/200, Loss: 2734.6494
Epoch 182/200, Loss: 2734.6589
Epoch 183/200, Loss: 2734.6577
Epoch 184/200, Loss: 2734.6564
Epoch 185/200, Loss: 2734.6867
Epoch 186/200, Loss: 2734.6603
Epoch 187/200, Loss: 2734.6754
Epoch 188/200, Loss: 2734.6679
Epoch 189/200, Loss: 2734.6791
Epoch 190/200, Loss: 2734.6476
Epoch 191/200, Loss: 2734.6566
Epoch 192/200, Loss: 2734.6775
Epoch 193/200, Loss: 2734.6640
Epoch 194/200, Loss: 2734.6715
Epoch 195/200, Loss: 2734.6657
Epoch 196/200, Loss: 2734.6705
Epoch 197/200, Loss: 2734.6778
Epoch 198/200, Loss: 2734.6751
Epoch 199/200, Loss: 2734.6417
Epoch 200/200, Loss: 2734.6497
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 21.01% | Test Accuracy: 14.56%
Precision: 0.0901 | Recall: 0.1456 | F1-Score: 0.0928

Processing Subject 10...
Top 32 discriminative features: [26 46 44 41 29 38 14 12 45 42 22 58 60 25 18 37 16 34  9 33 40 30 21 28
 54 56  1 52 55 32 59 48]

Dataset Loaded: Taiji_dataset_100.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 69294
	# per Class in Train Dataset:
		Class 0: 1800
		Class 1: 1784
		Class 2: 1800
		Class 3: 1800
		Class 4: 1800
		Class 5: 1800
		Class 6: 1800
		Class 7: 1800
		Class 8: 1800
		Class 9: 1800
		Class 10: 1800
		Class 11: 1684
		Class 12: 1646
		Class 13: 1789
		Class 14: 1800
		Class 15: 1793
		Class 16: 1658
		Class 17: 1652
		Class 18: 1743
		Class 19: 1800
		Class 20: 1800
		Class 21: 1800
		Class 22: 1800
		Class 23: 1800
		Class 24: 1800
		Class 25: 1800
		Class 26: 1757
		Class 27: 1800
		Class 28: 1789
		Class 29: 1800
		Class 30: 1800
		Class 31: 1800
		Class 32: 1800
		Class 33: 1774
		Class 34: 1800
		Class 35: 1800
		Class 36: 1800
		Class 37: 1800
		Class 38: 1625
	# of Testing Samples: 7793
	# per Class in Test Dataset:
		Class 0: 200
		Class 1: 200
		Class 2: 200
		Class 3: 200
		Class 4: 200
		Class 5: 200
		Class 6: 200
		Class 7: 200
		Class 8: 200
		Class 9: 200
		Class 10: 200
		Class 11: 200
		Class 12: 200
		Class 13: 200
		Class 14: 200
		Class 15: 200
		Class 16: 200
		Class 17: 200
		Class 18: 200
		Class 19: 200
		Class 20: 200
		Class 21: 200
		Class 22: 200
		Class 23: 200
		Class 24: 200
		Class 25: 200
		Class 26: 200
		Class 27: 200
		Class 28: 200
		Class 29: 200
		Class 30: 200
		Class 31: 200
		Class 32: 200
		Class 33: 200
		Class 34: 200
		Class 35: 200
		Class 36: 200
		Class 37: 200
		Class 38: 193
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 63.56%
Precision: 0.7304 | Recall: 0.6356 | F1-Score: 0.6454


Running Deep Learning Classifier...
DataLoader: Training set - 1083 batches, Testing set - 122 batches
Epoch 1/200, Loss: 2208.1561
Epoch 2/200, Loss: 1137.7162
Epoch 3/200, Loss: 890.1035
Epoch 4/200, Loss: 752.9674
Epoch 5/200, Loss: 665.9773
Epoch 6/200, Loss: 608.5868
Epoch 7/200, Loss: 600.4890
Epoch 8/200, Loss: 594.1910
Epoch 9/200, Loss: 588.3043
Epoch 10/200, Loss: 582.5127
Epoch 11/200, Loss: 577.3866
Epoch 12/200, Loss: 576.5134
Epoch 13/200, Loss: 575.8545
Epoch 14/200, Loss: 575.3182
Epoch 15/200, Loss: 574.7512
Epoch 16/200, Loss: 574.2382
Epoch 17/200, Loss: 574.1062
Epoch 18/200, Loss: 574.0483
Epoch 19/200, Loss: 573.9775
Epoch 20/200, Loss: 573.8993
Epoch 21/200, Loss: 573.7862
Epoch 22/200, Loss: 573.8730
Epoch 23/200, Loss: 573.7960
Epoch 24/200, Loss: 573.8561
Epoch 25/200, Loss: 573.8387
Epoch 26/200, Loss: 573.8020
Epoch 27/200, Loss: 573.7929
Epoch 28/200, Loss: 573.8796
Epoch 29/200, Loss: 573.7939
Epoch 30/200, Loss: 573.8448
Epoch 31/200, Loss: 573.8321
Epoch 32/200, Loss: 573.8760
Epoch 33/200, Loss: 573.8266
Epoch 34/200, Loss: 573.8434
Epoch 35/200, Loss: 573.7888
Epoch 36/200, Loss: 573.7930
Epoch 37/200, Loss: 573.7830
Epoch 38/200, Loss: 573.8332
Epoch 39/200, Loss: 573.8217
Epoch 40/200, Loss: 573.7866
Epoch 41/200, Loss: 573.8160
Epoch 42/200, Loss: 573.8429
Epoch 43/200, Loss: 573.7959
Epoch 44/200, Loss: 573.8530
Epoch 45/200, Loss: 573.8396
Epoch 46/200, Loss: 573.8406
Epoch 47/200, Loss: 573.7992
Epoch 48/200, Loss: 573.8430
Epoch 49/200, Loss: 573.8767
Epoch 50/200, Loss: 573.8912
Epoch 51/200, Loss: 573.7840
Epoch 52/200, Loss: 573.8388
Epoch 53/200, Loss: 573.7961
Epoch 54/200, Loss: 573.8015
Epoch 55/200, Loss: 573.8264
Epoch 56/200, Loss: 573.8135
Epoch 57/200, Loss: 573.7716
Epoch 58/200, Loss: 573.8344
Epoch 59/200, Loss: 573.8789
Epoch 60/200, Loss: 573.8321
Epoch 61/200, Loss: 573.7694
Epoch 62/200, Loss: 573.9103
Epoch 63/200, Loss: 573.8330
Epoch 64/200, Loss: 573.9103
Epoch 65/200, Loss: 573.7782
Epoch 66/200, Loss: 573.8269
Epoch 67/200, Loss: 573.8490
Epoch 68/200, Loss: 573.9307
Epoch 69/200, Loss: 573.7718
Epoch 70/200, Loss: 573.8412
Epoch 71/200, Loss: 573.8176
Epoch 72/200, Loss: 573.8743
Epoch 73/200, Loss: 573.8450
Epoch 74/200, Loss: 573.8519
Epoch 75/200, Loss: 573.8569
Epoch 76/200, Loss: 573.7917
Epoch 77/200, Loss: 573.8575
Epoch 78/200, Loss: 573.8236
Epoch 79/200, Loss: 573.8574
Epoch 80/200, Loss: 573.8228
Epoch 81/200, Loss: 573.8166
Epoch 82/200, Loss: 573.9091
Epoch 83/200, Loss: 573.8674
Epoch 84/200, Loss: 573.8116
Epoch 85/200, Loss: 573.8401
Epoch 86/200, Loss: 573.8922
Epoch 87/200, Loss: 573.8168
Epoch 88/200, Loss: 573.8160
Epoch 89/200, Loss: 573.8621
Epoch 90/200, Loss: 573.8334
Epoch 91/200, Loss: 573.7876
Epoch 92/200, Loss: 573.9374
Epoch 93/200, Loss: 573.8072
Epoch 94/200, Loss: 573.8869
Epoch 95/200, Loss: 573.8388
Epoch 96/200, Loss: 573.8362
Epoch 97/200, Loss: 573.8495
Epoch 98/200, Loss: 573.8807
Epoch 99/200, Loss: 573.7846
Epoch 100/200, Loss: 573.8089
Epoch 101/200, Loss: 573.7777
Epoch 102/200, Loss: 573.8299
Epoch 103/200, Loss: 573.8670
Epoch 104/200, Loss: 573.8411
Epoch 105/200, Loss: 573.8593
Epoch 106/200, Loss: 573.7853
Epoch 107/200, Loss: 573.7844
Epoch 108/200, Loss: 573.8425
Epoch 109/200, Loss: 573.8271
Epoch 110/200, Loss: 573.8053
Epoch 111/200, Loss: 573.8247
Epoch 112/200, Loss: 573.8339
Epoch 113/200, Loss: 573.8498
Epoch 114/200, Loss: 573.7807
Epoch 115/200, Loss: 573.7914
Epoch 116/200, Loss: 573.8373
Epoch 117/200, Loss: 573.8173
Epoch 118/200, Loss: 573.8748
Epoch 119/200, Loss: 573.8049
Epoch 120/200, Loss: 573.8033
Epoch 121/200, Loss: 573.8525
Epoch 122/200, Loss: 573.7826
Epoch 123/200, Loss: 573.8346
Epoch 124/200, Loss: 573.8404
Epoch 125/200, Loss: 573.8119
Epoch 126/200, Loss: 573.7774
Epoch 127/200, Loss: 573.8362
Epoch 128/200, Loss: 573.9266
Epoch 129/200, Loss: 573.7889
Epoch 130/200, Loss: 573.8518
Epoch 131/200, Loss: 573.7993
Epoch 132/200, Loss: 573.7986
Epoch 133/200, Loss: 573.8138
Epoch 134/200, Loss: 573.8133
Epoch 135/200, Loss: 573.8222
Epoch 136/200, Loss: 573.8570
Epoch 137/200, Loss: 573.8383
Epoch 138/200, Loss: 573.8448
Epoch 139/200, Loss: 573.7805
Epoch 140/200, Loss: 573.7802
Epoch 141/200, Loss: 573.7947
Epoch 142/200, Loss: 573.7906
Epoch 143/200, Loss: 573.8441
Epoch 144/200, Loss: 573.8285
Epoch 145/200, Loss: 573.7709
Epoch 146/200, Loss: 573.8251
Epoch 147/200, Loss: 573.8157
Epoch 148/200, Loss: 573.8136
Epoch 149/200, Loss: 573.7846
Epoch 150/200, Loss: 573.7949
Epoch 151/200, Loss: 573.8439
Epoch 152/200, Loss: 573.8650
Epoch 153/200, Loss: 573.8523
Epoch 154/200, Loss: 573.8034
Epoch 155/200, Loss: 573.7951
Epoch 156/200, Loss: 573.7908
Epoch 157/200, Loss: 573.8583
Epoch 158/200, Loss: 573.8367
Epoch 159/200, Loss: 573.8090
Epoch 160/200, Loss: 573.7832
Epoch 161/200, Loss: 573.8000
Epoch 162/200, Loss: 573.8040
Epoch 163/200, Loss: 573.7976
Epoch 164/200, Loss: 573.8055
Epoch 165/200, Loss: 573.8970
Epoch 166/200, Loss: 573.8590
Epoch 167/200, Loss: 573.8039
Epoch 168/200, Loss: 573.8090
Epoch 169/200, Loss: 573.8177
Epoch 170/200, Loss: 573.8473
Epoch 171/200, Loss: 573.8262
Epoch 172/200, Loss: 573.8085
Epoch 173/200, Loss: 573.8947
Epoch 174/200, Loss: 573.8805
Epoch 175/200, Loss: 573.7937
Epoch 176/200, Loss: 573.8637
Epoch 177/200, Loss: 573.8650
Epoch 178/200, Loss: 573.8465
Epoch 179/200, Loss: 573.8059
Epoch 180/200, Loss: 573.8395
Epoch 181/200, Loss: 573.8349
Epoch 182/200, Loss: 573.8174
Epoch 183/200, Loss: 573.8467
Epoch 184/200, Loss: 573.7785
Epoch 185/200, Loss: 573.8691
Epoch 186/200, Loss: 573.8822
Epoch 187/200, Loss: 573.8365
Epoch 188/200, Loss: 573.8138
Epoch 189/200, Loss: 573.8502
Epoch 190/200, Loss: 573.7974
Epoch 191/200, Loss: 573.8175
Epoch 192/200, Loss: 573.7932
Epoch 193/200, Loss: 573.8590
Epoch 194/200, Loss: 573.9344
Epoch 195/200, Loss: 573.8270
Epoch 196/200, Loss: 573.8587
Epoch 197/200, Loss: 573.8537
Epoch 198/200, Loss: 573.8041
Epoch 199/200, Loss: 573.7977
Epoch 200/200, Loss: 573.8131
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 83.17% | Test Accuracy: 71.73%
Precision: 0.7325 | Recall: 0.7173 | F1-Score: 0.6918
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 31.66%
Precision: 0.3193 | Recall: 0.3166 | F1-Score: 0.3034


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1083 batches, Testing set - 122 batches
Epoch 1/200, Loss: 3231.5977
Epoch 2/200, Loss: 2586.0639
Epoch 3/200, Loss: 2524.2000
Epoch 4/200, Loss: 2488.2504
Epoch 5/200, Loss: 2464.0518
Epoch 6/200, Loss: 2428.9886
Epoch 7/200, Loss: 2425.5861
Epoch 8/200, Loss: 2423.7120
Epoch 9/200, Loss: 2421.8068
Epoch 10/200, Loss: 2419.4520
Epoch 11/200, Loss: 2416.5495
Epoch 12/200, Loss: 2415.7173
Epoch 13/200, Loss: 2415.3941
Epoch 14/200, Loss: 2415.1240
Epoch 15/200, Loss: 2415.0336
Epoch 16/200, Loss: 2414.4696
Epoch 17/200, Loss: 2414.4862
Epoch 18/200, Loss: 2414.4738
Epoch 19/200, Loss: 2414.4299
Epoch 20/200, Loss: 2414.3602
Epoch 21/200, Loss: 2414.2946
Epoch 22/200, Loss: 2414.2956
Epoch 23/200, Loss: 2414.2971
Epoch 24/200, Loss: 2414.2667
Epoch 25/200, Loss: 2414.3188
Epoch 26/200, Loss: 2414.3326
Epoch 27/200, Loss: 2414.3578
Epoch 28/200, Loss: 2414.3870
Epoch 29/200, Loss: 2414.3436
Epoch 30/200, Loss: 2414.2469
Epoch 31/200, Loss: 2414.3432
Epoch 32/200, Loss: 2414.3221
Epoch 33/200, Loss: 2414.2531
Epoch 34/200, Loss: 2414.3275
Epoch 35/200, Loss: 2414.3205
Epoch 36/200, Loss: 2414.3994
Epoch 37/200, Loss: 2414.3611
Epoch 38/200, Loss: 2414.3290
Epoch 39/200, Loss: 2414.3392
Epoch 40/200, Loss: 2414.3185
Epoch 41/200, Loss: 2414.2719
Epoch 42/200, Loss: 2414.3319
Epoch 43/200, Loss: 2414.3297
Epoch 44/200, Loss: 2414.3138
Epoch 45/200, Loss: 2414.3581
Epoch 46/200, Loss: 2414.4335
Epoch 47/200, Loss: 2414.3277
Epoch 48/200, Loss: 2414.2395
Epoch 49/200, Loss: 2414.3271
Epoch 50/200, Loss: 2414.3196
Epoch 51/200, Loss: 2414.3327
Epoch 52/200, Loss: 2414.2573
Epoch 53/200, Loss: 2414.3176
Epoch 54/200, Loss: 2414.3719
Epoch 55/200, Loss: 2414.2906
Epoch 56/200, Loss: 2414.3442
Epoch 57/200, Loss: 2414.3245
Epoch 58/200, Loss: 2414.2731
Epoch 59/200, Loss: 2414.3428
Epoch 60/200, Loss: 2414.2146
Epoch 61/200, Loss: 2414.3831
Epoch 62/200, Loss: 2414.3070
Epoch 63/200, Loss: 2414.3221
Epoch 64/200, Loss: 2414.2782
Epoch 65/200, Loss: 2414.2833
Epoch 66/200, Loss: 2414.3456
Epoch 67/200, Loss: 2414.2960
Epoch 68/200, Loss: 2414.3985
Epoch 69/200, Loss: 2414.2979
Epoch 70/200, Loss: 2414.2823
Epoch 71/200, Loss: 2414.3119
Epoch 72/200, Loss: 2414.3240
Epoch 73/200, Loss: 2414.3414
Epoch 74/200, Loss: 2414.2942
Epoch 75/200, Loss: 2414.2835
Epoch 76/200, Loss: 2414.3848
Epoch 77/200, Loss: 2414.3609
Epoch 78/200, Loss: 2414.3238
Epoch 79/200, Loss: 2414.2811
Epoch 80/200, Loss: 2414.2414
Epoch 81/200, Loss: 2414.3176
Epoch 82/200, Loss: 2414.3491
Epoch 83/200, Loss: 2414.2745
Epoch 84/200, Loss: 2414.3041
Epoch 85/200, Loss: 2414.3435
Epoch 86/200, Loss: 2414.2936
Epoch 87/200, Loss: 2414.2987
Epoch 88/200, Loss: 2414.2500
Epoch 89/200, Loss: 2414.3058
Epoch 90/200, Loss: 2414.2857
Epoch 91/200, Loss: 2414.2837
Epoch 92/200, Loss: 2414.3234
Epoch 93/200, Loss: 2414.3006
Epoch 94/200, Loss: 2414.3131
Epoch 95/200, Loss: 2414.3330
Epoch 96/200, Loss: 2414.3496
Epoch 97/200, Loss: 2414.2749
Epoch 98/200, Loss: 2414.3182
Epoch 99/200, Loss: 2414.3220
Epoch 100/200, Loss: 2414.3802
Epoch 101/200, Loss: 2414.2393
Epoch 102/200, Loss: 2414.3441
Epoch 103/200, Loss: 2414.3534
Epoch 104/200, Loss: 2414.2539
Epoch 105/200, Loss: 2414.2308
Epoch 106/200, Loss: 2414.3414
Epoch 107/200, Loss: 2414.3194
Epoch 108/200, Loss: 2414.3452
Epoch 109/200, Loss: 2414.2433
Epoch 110/200, Loss: 2414.3197
Epoch 111/200, Loss: 2414.2622
Epoch 112/200, Loss: 2414.3113
Epoch 113/200, Loss: 2414.2543
Epoch 114/200, Loss: 2414.2630
Epoch 115/200, Loss: 2414.3161
Epoch 116/200, Loss: 2414.3605
Epoch 117/200, Loss: 2414.3498
Epoch 118/200, Loss: 2414.3169
Epoch 119/200, Loss: 2414.4453
Epoch 120/200, Loss: 2414.3423
Epoch 121/200, Loss: 2414.3289
Epoch 122/200, Loss: 2414.2514
Epoch 123/200, Loss: 2414.2881
Epoch 124/200, Loss: 2414.3588
Epoch 125/200, Loss: 2414.2702
Epoch 126/200, Loss: 2414.3245
Epoch 127/200, Loss: 2414.3043
Epoch 128/200, Loss: 2414.2787
Epoch 129/200, Loss: 2414.3252
Epoch 130/200, Loss: 2414.3545
Epoch 131/200, Loss: 2414.3500
Epoch 132/200, Loss: 2414.3084
Epoch 133/200, Loss: 2414.2936
Epoch 134/200, Loss: 2414.2886
Epoch 135/200, Loss: 2414.3107
Epoch 136/200, Loss: 2414.3555
Epoch 137/200, Loss: 2414.3181
Epoch 138/200, Loss: 2414.3584
Epoch 139/200, Loss: 2414.3125
Epoch 140/200, Loss: 2414.2909
Epoch 141/200, Loss: 2414.3826
Epoch 142/200, Loss: 2414.2586
Epoch 143/200, Loss: 2414.2601
Epoch 144/200, Loss: 2414.3241
Epoch 145/200, Loss: 2414.4354
Epoch 146/200, Loss: 2414.3068
Epoch 147/200, Loss: 2414.2868
Epoch 148/200, Loss: 2414.3887
Epoch 149/200, Loss: 2414.3454
Epoch 150/200, Loss: 2414.4132
Epoch 151/200, Loss: 2414.3158
Epoch 152/200, Loss: 2414.3238
Epoch 153/200, Loss: 2414.2346
Epoch 154/200, Loss: 2414.3774
Epoch 155/200, Loss: 2414.2924
Epoch 156/200, Loss: 2414.3093
Epoch 157/200, Loss: 2414.2993
Epoch 158/200, Loss: 2414.2667
Epoch 159/200, Loss: 2414.3172
Epoch 160/200, Loss: 2414.3008
Epoch 161/200, Loss: 2414.3830
Epoch 162/200, Loss: 2414.3043
Epoch 163/200, Loss: 2414.3028
Epoch 164/200, Loss: 2414.2193
Epoch 165/200, Loss: 2414.2752
Epoch 166/200, Loss: 2414.2288
Epoch 167/200, Loss: 2414.3638
Epoch 168/200, Loss: 2414.2613
Epoch 169/200, Loss: 2414.3549
Epoch 170/200, Loss: 2414.3381
Epoch 171/200, Loss: 2414.2560
Epoch 172/200, Loss: 2414.2346
Epoch 173/200, Loss: 2414.2749
Epoch 174/200, Loss: 2414.3218
Epoch 175/200, Loss: 2414.2970
Epoch 176/200, Loss: 2414.2671
Epoch 177/200, Loss: 2414.3308
Epoch 178/200, Loss: 2414.3854
Epoch 179/200, Loss: 2414.3107
Epoch 180/200, Loss: 2414.2550
Epoch 181/200, Loss: 2414.3458
Epoch 182/200, Loss: 2414.2730
Epoch 183/200, Loss: 2414.2787
Epoch 184/200, Loss: 2414.3534
Epoch 185/200, Loss: 2414.3139
Epoch 186/200, Loss: 2414.3208
Epoch 187/200, Loss: 2414.3737
Epoch 188/200, Loss: 2414.2727
Epoch 189/200, Loss: 2414.2533
Epoch 190/200, Loss: 2414.3199
Epoch 191/200, Loss: 2414.3302
Epoch 192/200, Loss: 2414.2509
Epoch 193/200, Loss: 2414.2754
Epoch 194/200, Loss: 2414.3325
Epoch 195/200, Loss: 2414.3409
Epoch 196/200, Loss: 2414.3489
Epoch 197/200, Loss: 2414.3009
Epoch 198/200, Loss: 2414.3006
Epoch 199/200, Loss: 2414.2582
Epoch 200/200, Loss: 2414.3410
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 27.41% | Test Accuracy: 25.64%
Precision: 0.1820 | Recall: 0.2564 | F1-Score: 0.1907

========== Running Classification on Taiji_dataset_200.csv ==========

Processing Subject 1...
Top 32 discriminative features: [26 38 46 44 14 12 41 29 18 42 16 40 25 37 30 22 45 34 33 63 61 32 28 21
  9 59 57 49 51 20  1 10]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 115797
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3075
		Class 2: 3326
		Class 3: 3600
		Class 4: 3529
		Class 5: 3488
		Class 6: 3600
		Class 7: 3471
		Class 8: 3139
		Class 9: 3600
		Class 10: 3529
		Class 11: 1751
		Class 12: 1758
		Class 13: 2298
		Class 14: 2894
		Class 15: 2559
		Class 16: 1808
		Class 17: 1737
		Class 18: 2107
		Class 19: 2798
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2598
		Class 25: 3600
		Class 26: 2309
		Class 27: 3118
		Class 28: 2706
		Class 29: 3044
		Class 30: 3298
		Class 31: 3122
		Class 32: 2547
		Class 33: 2130
		Class 34: 3600
		Class 35: 3005
		Class 36: 3023
		Class 37: 3342
		Class 38: 2508
	# of Testing Samples: 13778
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 400
		Class 2: 332
		Class 3: 400
		Class 4: 400
		Class 5: 400
		Class 6: 400
		Class 7: 400
		Class 8: 400
		Class 9: 400
		Class 10: 400
		Class 11: 180
		Class 12: 224
		Class 13: 273
		Class 14: 400
		Class 15: 289
		Class 16: 163
		Class 17: 200
		Class 18: 282
		Class 19: 395
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 338
		Class 25: 400
		Class 26: 281
		Class 27: 400
		Class 28: 367
		Class 29: 347
		Class 30: 400
		Class 31: 400
		Class 32: 343
		Class 33: 297
		Class 34: 400
		Class 35: 400
		Class 36: 400
		Class 37: 400
		Class 38: 267
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 72.53%
Precision: 0.7549 | Recall: 0.7253 | F1-Score: 0.7136


Running Deep Learning Classifier...
DataLoader: Training set - 1810 batches, Testing set - 216 batches
Epoch 1/200, Loss: 3911.8782
Epoch 2/200, Loss: 2293.5275
Epoch 3/200, Loss: 1820.9776
Epoch 4/200, Loss: 1550.2214
Epoch 5/200, Loss: 1373.3258
Epoch 6/200, Loss: 1256.1400
Epoch 7/200, Loss: 1238.8067
Epoch 8/200, Loss: 1225.7899
Epoch 9/200, Loss: 1213.3169
Epoch 10/200, Loss: 1201.0538
Epoch 11/200, Loss: 1189.8967
Epoch 12/200, Loss: 1188.2419
Epoch 13/200, Loss: 1186.6942
Epoch 14/200, Loss: 1185.5901
Epoch 15/200, Loss: 1184.5779
Epoch 16/200, Loss: 1183.1066
Epoch 17/200, Loss: 1182.7696
Epoch 18/200, Loss: 1182.7858
Epoch 19/200, Loss: 1182.6192
Epoch 20/200, Loss: 1182.4633
Epoch 21/200, Loss: 1182.3976
Epoch 22/200, Loss: 1182.2389
Epoch 23/200, Loss: 1182.5581
Epoch 24/200, Loss: 1182.1842
Epoch 25/200, Loss: 1182.2730
Epoch 26/200, Loss: 1182.4430
Epoch 27/200, Loss: 1182.4261
Epoch 28/200, Loss: 1182.2164
Epoch 29/200, Loss: 1182.3320
Epoch 30/200, Loss: 1182.3718
Epoch 31/200, Loss: 1182.4318
Epoch 32/200, Loss: 1182.3404
Epoch 33/200, Loss: 1182.4009
Epoch 34/200, Loss: 1182.3604
Epoch 35/200, Loss: 1182.2624
Epoch 36/200, Loss: 1182.2193
Epoch 37/200, Loss: 1182.4519
Epoch 38/200, Loss: 1182.4326
Epoch 39/200, Loss: 1182.3199
Epoch 40/200, Loss: 1182.3448
Epoch 41/200, Loss: 1182.3200
Epoch 42/200, Loss: 1182.5274
Epoch 43/200, Loss: 1182.1935
Epoch 44/200, Loss: 1182.3680
Epoch 45/200, Loss: 1182.7363
Epoch 46/200, Loss: 1182.4847
Epoch 47/200, Loss: 1182.1419
Epoch 48/200, Loss: 1182.5452
Epoch 49/200, Loss: 1182.4654
Epoch 50/200, Loss: 1182.3825
Epoch 51/200, Loss: 1182.4184
Epoch 52/200, Loss: 1182.1793
Epoch 53/200, Loss: 1182.5086
Epoch 54/200, Loss: 1182.3152
Epoch 55/200, Loss: 1182.2652
Epoch 56/200, Loss: 1182.4896
Epoch 57/200, Loss: 1182.4083
Epoch 58/200, Loss: 1182.4719
Epoch 59/200, Loss: 1182.4894
Epoch 60/200, Loss: 1182.5629
Epoch 61/200, Loss: 1182.2008
Epoch 62/200, Loss: 1182.3735
Epoch 63/200, Loss: 1182.4388
Epoch 64/200, Loss: 1182.2784
Epoch 65/200, Loss: 1182.3506
Epoch 66/200, Loss: 1182.4293
Epoch 67/200, Loss: 1182.2624
Epoch 68/200, Loss: 1182.2301
Epoch 69/200, Loss: 1182.2223
Epoch 70/200, Loss: 1182.4647
Epoch 71/200, Loss: 1182.7068
Epoch 72/200, Loss: 1182.3157
Epoch 73/200, Loss: 1182.2340
Epoch 74/200, Loss: 1182.2079
Epoch 75/200, Loss: 1182.2565
Epoch 76/200, Loss: 1182.2282
Epoch 77/200, Loss: 1182.1495
Epoch 78/200, Loss: 1182.4666
Epoch 79/200, Loss: 1182.2297
Epoch 80/200, Loss: 1182.2743
Epoch 81/200, Loss: 1182.4461
Epoch 82/200, Loss: 1182.4509
Epoch 83/200, Loss: 1182.2665
Epoch 84/200, Loss: 1182.1078
Epoch 85/200, Loss: 1182.2254
Epoch 86/200, Loss: 1182.3154
Epoch 87/200, Loss: 1182.4160
Epoch 88/200, Loss: 1182.1884
Epoch 89/200, Loss: 1182.4250
Epoch 90/200, Loss: 1182.5884
Epoch 91/200, Loss: 1182.5662
Epoch 92/200, Loss: 1182.3685
Epoch 93/200, Loss: 1182.1936
Epoch 94/200, Loss: 1182.3009
Epoch 95/200, Loss: 1182.4284
Epoch 96/200, Loss: 1182.3883
Epoch 97/200, Loss: 1182.4901
Epoch 98/200, Loss: 1182.1434
Epoch 99/200, Loss: 1182.3727
Epoch 100/200, Loss: 1182.4332
Epoch 101/200, Loss: 1182.2823
Epoch 102/200, Loss: 1182.5425
Epoch 103/200, Loss: 1182.2443
Epoch 104/200, Loss: 1182.1874
Epoch 105/200, Loss: 1182.4177
Epoch 106/200, Loss: 1182.2311
Epoch 107/200, Loss: 1182.2925
Epoch 108/200, Loss: 1182.3074
Epoch 109/200, Loss: 1182.3106
Epoch 110/200, Loss: 1182.3275
Epoch 111/200, Loss: 1182.3855
Epoch 112/200, Loss: 1182.4877
Epoch 113/200, Loss: 1182.3333
Epoch 114/200, Loss: 1182.2904
Epoch 115/200, Loss: 1182.2124
Epoch 116/200, Loss: 1182.2546
Epoch 117/200, Loss: 1182.4640
Epoch 118/200, Loss: 1182.3989
Epoch 119/200, Loss: 1182.3351
Epoch 120/200, Loss: 1182.2993
Epoch 121/200, Loss: 1182.4627
Epoch 122/200, Loss: 1182.3060
Epoch 123/200, Loss: 1182.4016
Epoch 124/200, Loss: 1182.5962
Epoch 125/200, Loss: 1182.3452
Epoch 126/200, Loss: 1182.3334
Epoch 127/200, Loss: 1182.2934
Epoch 128/200, Loss: 1182.2480
Epoch 129/200, Loss: 1182.4408
Epoch 130/200, Loss: 1182.1875
Epoch 131/200, Loss: 1182.3975
Epoch 132/200, Loss: 1182.2332
Epoch 133/200, Loss: 1182.5069
Epoch 134/200, Loss: 1182.3144
Epoch 135/200, Loss: 1182.3198
Epoch 136/200, Loss: 1182.2258
Epoch 137/200, Loss: 1182.3022
Epoch 138/200, Loss: 1182.1810
Epoch 139/200, Loss: 1182.1288
Epoch 140/200, Loss: 1182.5149
Epoch 141/200, Loss: 1182.3271
Epoch 142/200, Loss: 1182.4035
Epoch 143/200, Loss: 1182.4095
Epoch 144/200, Loss: 1182.2954
Epoch 145/200, Loss: 1182.4899
Epoch 146/200, Loss: 1182.3625
Epoch 147/200, Loss: 1182.3471
Epoch 148/200, Loss: 1182.3808
Epoch 149/200, Loss: 1182.4314
Epoch 150/200, Loss: 1182.4526
Epoch 151/200, Loss: 1182.3415
Epoch 152/200, Loss: 1182.2573
Epoch 153/200, Loss: 1182.7055
Epoch 154/200, Loss: 1182.3416
Epoch 155/200, Loss: 1182.2394
Epoch 156/200, Loss: 1182.4947
Epoch 157/200, Loss: 1182.3178
Epoch 158/200, Loss: 1182.3703
Epoch 159/200, Loss: 1182.1952
Epoch 160/200, Loss: 1182.3535
Epoch 161/200, Loss: 1182.3034
Epoch 162/200, Loss: 1182.3301
Epoch 163/200, Loss: 1182.2895
Epoch 164/200, Loss: 1182.5835
Epoch 165/200, Loss: 1182.2698
Epoch 166/200, Loss: 1182.4974
Epoch 167/200, Loss: 1182.2204
Epoch 168/200, Loss: 1182.3317
Epoch 169/200, Loss: 1182.3008
Epoch 170/200, Loss: 1182.4448
Epoch 171/200, Loss: 1182.2899
Epoch 172/200, Loss: 1182.4727
Epoch 173/200, Loss: 1182.3278
Epoch 174/200, Loss: 1182.2787
Epoch 175/200, Loss: 1182.7718
Epoch 176/200, Loss: 1182.4432
Epoch 177/200, Loss: 1182.3456
Epoch 178/200, Loss: 1182.2004
Epoch 179/200, Loss: 1182.2950
Epoch 180/200, Loss: 1182.2835
Epoch 181/200, Loss: 1182.3988
Epoch 182/200, Loss: 1182.2181
Epoch 183/200, Loss: 1182.1987
Epoch 184/200, Loss: 1182.2052
Epoch 185/200, Loss: 1182.3508
Epoch 186/200, Loss: 1182.5495
Epoch 187/200, Loss: 1182.3897
Epoch 188/200, Loss: 1182.4334
Epoch 189/200, Loss: 1182.3046
Epoch 190/200, Loss: 1182.2626
Epoch 191/200, Loss: 1182.4860
Epoch 192/200, Loss: 1182.3651
Epoch 193/200, Loss: 1182.3932
Epoch 194/200, Loss: 1182.3743
Epoch 195/200, Loss: 1182.3982
Epoch 196/200, Loss: 1182.2391
Epoch 197/200, Loss: 1182.3950
Epoch 198/200, Loss: 1182.4986
Epoch 199/200, Loss: 1182.3600
Epoch 200/200, Loss: 1182.3498
Train Accuracy: 79.48% | Test Accuracy: 66.44%
Precision: 0.6853 | Recall: 0.6644 | F1-Score: 0.6572
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 17.77%
Precision: 0.1715 | Recall: 0.1777 | F1-Score: 0.1719


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1810 batches, Testing set - 216 batches
Epoch 1/200, Loss: 6158.0505
Epoch 2/200, Loss: 5551.3042
Epoch 3/200, Loss: 5289.6691
Epoch 4/200, Loss: 5102.8898
Epoch 5/200, Loss: 4941.9478
Epoch 6/200, Loss: 4820.9967
Epoch 7/200, Loss: 4806.8578
Epoch 8/200, Loss: 4793.5197
Epoch 9/200, Loss: 4781.2328
Epoch 10/200, Loss: 4766.9774
Epoch 11/200, Loss: 4754.7244
Epoch 12/200, Loss: 4753.0819
Epoch 13/200, Loss: 4751.7166
Epoch 14/200, Loss: 4750.6076
Epoch 15/200, Loss: 4749.3112
Epoch 16/200, Loss: 4748.2848
Epoch 17/200, Loss: 4747.9586
Epoch 18/200, Loss: 4747.6497
Epoch 19/200, Loss: 4747.8786
Epoch 20/200, Loss: 4747.4175
Epoch 21/200, Loss: 4747.3820
Epoch 22/200, Loss: 4747.2192
Epoch 23/200, Loss: 4747.1951
Epoch 24/200, Loss: 4747.2812
Epoch 25/200, Loss: 4747.3298
Epoch 26/200, Loss: 4747.0982
Epoch 27/200, Loss: 4747.0340
Epoch 28/200, Loss: 4747.0787
Epoch 29/200, Loss: 4747.1535
Epoch 30/200, Loss: 4747.3121
Epoch 31/200, Loss: 4747.2498
Epoch 32/200, Loss: 4747.1535
Epoch 33/200, Loss: 4747.1968
Epoch 34/200, Loss: 4747.4142
Epoch 35/200, Loss: 4747.2814
Epoch 36/200, Loss: 4747.4177
Epoch 37/200, Loss: 4747.3610
Epoch 38/200, Loss: 4747.3985
Epoch 39/200, Loss: 4747.5631
Epoch 40/200, Loss: 4747.4424
Epoch 41/200, Loss: 4747.2513
Epoch 42/200, Loss: 4747.3054
Epoch 43/200, Loss: 4747.2857
Epoch 44/200, Loss: 4747.2147
Epoch 45/200, Loss: 4747.2683
Epoch 46/200, Loss: 4747.3473
Epoch 47/200, Loss: 4747.1224
Epoch 48/200, Loss: 4747.3067
Epoch 49/200, Loss: 4747.1978
Epoch 50/200, Loss: 4747.2128
Epoch 51/200, Loss: 4747.1183
Epoch 52/200, Loss: 4747.1494
Epoch 53/200, Loss: 4747.2000
Epoch 54/200, Loss: 4747.0721
Epoch 55/200, Loss: 4747.3334
Epoch 56/200, Loss: 4747.3419
Epoch 57/200, Loss: 4747.3722
Epoch 58/200, Loss: 4747.1368
Epoch 59/200, Loss: 4747.2576
Epoch 60/200, Loss: 4747.1909
Epoch 61/200, Loss: 4747.2623
Epoch 62/200, Loss: 4746.9852
Epoch 63/200, Loss: 4746.9814
Epoch 64/200, Loss: 4747.1777
Epoch 65/200, Loss: 4747.5360
Epoch 66/200, Loss: 4747.1534
Epoch 67/200, Loss: 4747.2262
Epoch 68/200, Loss: 4747.1970
Epoch 69/200, Loss: 4747.1152
Epoch 70/200, Loss: 4747.2506
Epoch 71/200, Loss: 4747.4992
Epoch 72/200, Loss: 4747.1681
Epoch 73/200, Loss: 4747.2061
Epoch 74/200, Loss: 4747.2043
Epoch 75/200, Loss: 4747.3895
Epoch 76/200, Loss: 4747.2362
Epoch 77/200, Loss: 4747.2612
Epoch 78/200, Loss: 4747.3231
Epoch 79/200, Loss: 4747.3646
Epoch 80/200, Loss: 4747.2299
Epoch 81/200, Loss: 4747.2189
Epoch 82/200, Loss: 4747.4415
Epoch 83/200, Loss: 4747.6118
Epoch 84/200, Loss: 4747.2819
Epoch 85/200, Loss: 4747.4237
Epoch 86/200, Loss: 4747.0706
Epoch 87/200, Loss: 4747.1954
Epoch 88/200, Loss: 4747.3872
Epoch 89/200, Loss: 4746.9533
Epoch 90/200, Loss: 4747.2251
Epoch 91/200, Loss: 4747.2076
Epoch 92/200, Loss: 4747.4756
Epoch 93/200, Loss: 4747.3623
Epoch 94/200, Loss: 4747.1901
Epoch 95/200, Loss: 4747.0488
Epoch 96/200, Loss: 4747.3212
Epoch 97/200, Loss: 4747.3383
Epoch 98/200, Loss: 4747.4884
Epoch 99/200, Loss: 4747.3460
Epoch 100/200, Loss: 4747.2132
Epoch 101/200, Loss: 4747.0272
Epoch 102/200, Loss: 4747.2369
Epoch 103/200, Loss: 4747.2998
Epoch 104/200, Loss: 4747.1583
Epoch 105/200, Loss: 4747.5770
Epoch 106/200, Loss: 4747.2358
Epoch 107/200, Loss: 4747.2692
Epoch 108/200, Loss: 4747.6052
Epoch 109/200, Loss: 4747.2576
Epoch 110/200, Loss: 4747.3192
Epoch 111/200, Loss: 4747.1798
Epoch 112/200, Loss: 4747.2935
Epoch 113/200, Loss: 4747.2774
Epoch 114/200, Loss: 4747.2763
Epoch 115/200, Loss: 4747.4471
Epoch 116/200, Loss: 4747.2411
Epoch 117/200, Loss: 4747.0709
Epoch 118/200, Loss: 4747.2046
Epoch 119/200, Loss: 4747.4820
Epoch 120/200, Loss: 4747.3550
Epoch 121/200, Loss: 4747.4264
Epoch 122/200, Loss: 4747.4656
Epoch 123/200, Loss: 4747.4389
Epoch 124/200, Loss: 4747.0626
Epoch 125/200, Loss: 4747.3460
Epoch 126/200, Loss: 4747.1529
Epoch 127/200, Loss: 4747.2997
Epoch 128/200, Loss: 4747.0556
Epoch 129/200, Loss: 4747.3711
Epoch 130/200, Loss: 4747.3253
Epoch 131/200, Loss: 4747.2732
Epoch 132/200, Loss: 4747.4000
Epoch 133/200, Loss: 4747.3552
Epoch 134/200, Loss: 4747.1162
Epoch 135/200, Loss: 4747.1942
Epoch 136/200, Loss: 4747.4641
Epoch 137/200, Loss: 4747.3707
Epoch 138/200, Loss: 4747.2911
Epoch 139/200, Loss: 4747.3253
Epoch 140/200, Loss: 4747.3066
Epoch 141/200, Loss: 4747.4196
Epoch 142/200, Loss: 4747.3342
Epoch 143/200, Loss: 4747.4161
Epoch 144/200, Loss: 4747.2197
Epoch 145/200, Loss: 4746.9996
Epoch 146/200, Loss: 4747.2958
Epoch 147/200, Loss: 4747.3786
Epoch 148/200, Loss: 4747.2624
Epoch 149/200, Loss: 4747.2988
Epoch 150/200, Loss: 4747.1950
Epoch 151/200, Loss: 4746.9757
Epoch 152/200, Loss: 4747.4223
Epoch 153/200, Loss: 4747.2282
Epoch 154/200, Loss: 4747.3093
Epoch 155/200, Loss: 4747.4873
Epoch 156/200, Loss: 4747.2284
Epoch 157/200, Loss: 4747.2195
Epoch 158/200, Loss: 4747.4240
Epoch 159/200, Loss: 4747.4847
Epoch 160/200, Loss: 4747.6656
Epoch 161/200, Loss: 4747.2176
Epoch 162/200, Loss: 4747.2720
Epoch 163/200, Loss: 4747.1463
Epoch 164/200, Loss: 4747.3523
Epoch 165/200, Loss: 4747.2035
Epoch 166/200, Loss: 4747.2065
Epoch 167/200, Loss: 4747.1353
Epoch 168/200, Loss: 4747.3502
Epoch 169/200, Loss: 4747.4347
Epoch 170/200, Loss: 4747.3189
Epoch 171/200, Loss: 4747.2170
Epoch 172/200, Loss: 4747.1737
Epoch 173/200, Loss: 4747.3586
Epoch 174/200, Loss: 4747.3613
Epoch 175/200, Loss: 4747.1649
Epoch 176/200, Loss: 4747.4074
Epoch 177/200, Loss: 4747.1788
Epoch 178/200, Loss: 4747.2337
Epoch 179/200, Loss: 4747.3189
Epoch 180/200, Loss: 4747.3871
Epoch 181/200, Loss: 4747.2394
Epoch 182/200, Loss: 4747.3416
Epoch 183/200, Loss: 4747.7618
Epoch 184/200, Loss: 4747.2739
Epoch 185/200, Loss: 4747.1969
Epoch 186/200, Loss: 4747.2935
Epoch 187/200, Loss: 4747.0338
Epoch 188/200, Loss: 4747.1163
Epoch 189/200, Loss: 4747.4203
Epoch 190/200, Loss: 4747.1822
Epoch 191/200, Loss: 4747.1079
Epoch 192/200, Loss: 4747.5609
Epoch 193/200, Loss: 4747.3016
Epoch 194/200, Loss: 4747.1258
Epoch 195/200, Loss: 4747.1867
Epoch 196/200, Loss: 4747.5768
Epoch 197/200, Loss: 4747.1879
Epoch 198/200, Loss: 4747.4367
Epoch 199/200, Loss: 4747.4553
Epoch 200/200, Loss: 4747.2318
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 20.73% | Test Accuracy: 22.33%
Precision: 0.1470 | Recall: 0.2233 | F1-Score: 0.1607

Processing Subject 2...
Top 32 discriminative features: [26 38 46 44 41 14 12 29 18 16 42 30 22 40 25 37 45 63 61 21 33 28  9 49
 34 51 59 57  2  0 32 20]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 117088
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3136
		Class 2: 3258
		Class 3: 3600
		Class 4: 3529
		Class 5: 3498
		Class 6: 3600
		Class 7: 3471
		Class 8: 3191
		Class 9: 3600
		Class 10: 3529
		Class 11: 1723
		Class 12: 1794
		Class 13: 2363
		Class 14: 3024
		Class 15: 2572
		Class 16: 1772
		Class 17: 1751
		Class 18: 2180
		Class 19: 2904
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2686
		Class 25: 3600
		Class 26: 2305
		Class 27: 3223
		Class 28: 2840
		Class 29: 3074
		Class 30: 3403
		Class 31: 3187
		Class 32: 2619
		Class 33: 2165
		Class 34: 3600
		Class 35: 3115
		Class 36: 3098
		Class 37: 3432
		Class 38: 2466
	# of Testing Samples: 12487
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 339
		Class 2: 400
		Class 3: 400
		Class 4: 400
		Class 5: 390
		Class 6: 400
		Class 7: 400
		Class 8: 348
		Class 9: 400
		Class 10: 400
		Class 11: 208
		Class 12: 188
		Class 13: 208
		Class 14: 270
		Class 15: 276
		Class 16: 199
		Class 17: 186
		Class 18: 209
		Class 19: 289
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 250
		Class 25: 400
		Class 26: 285
		Class 27: 295
		Class 28: 233
		Class 29: 317
		Class 30: 295
		Class 31: 335
		Class 32: 271
		Class 33: 262
		Class 34: 400
		Class 35: 290
		Class 36: 325
		Class 37: 310
		Class 38: 309
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 63.35%
Precision: 0.6466 | Recall: 0.6335 | F1-Score: 0.6202


Running Deep Learning Classifier...
DataLoader: Training set - 1830 batches, Testing set - 196 batches
Epoch 1/200, Loss: 3941.8545
Epoch 2/200, Loss: 2372.5694
Epoch 3/200, Loss: 1899.1226
Epoch 4/200, Loss: 1616.0088
Epoch 5/200, Loss: 1422.4569
Epoch 6/200, Loss: 1295.4778
Epoch 7/200, Loss: 1276.6281
Epoch 8/200, Loss: 1262.0745
Epoch 9/200, Loss: 1248.7813
Epoch 10/200, Loss: 1235.4058
Epoch 11/200, Loss: 1223.4691
Epoch 12/200, Loss: 1221.4671
Epoch 13/200, Loss: 1220.2827
Epoch 14/200, Loss: 1218.8428
Epoch 15/200, Loss: 1217.4797
Epoch 16/200, Loss: 1216.0882
Epoch 17/200, Loss: 1215.8794
Epoch 18/200, Loss: 1215.9243
Epoch 19/200, Loss: 1215.7307
Epoch 20/200, Loss: 1215.6932
Epoch 21/200, Loss: 1215.6527
Epoch 22/200, Loss: 1215.3107
Epoch 23/200, Loss: 1215.3349
Epoch 24/200, Loss: 1215.3103
Epoch 25/200, Loss: 1215.3214
Epoch 26/200, Loss: 1215.3667
Epoch 27/200, Loss: 1215.3518
Epoch 28/200, Loss: 1215.4214
Epoch 29/200, Loss: 1215.3428
Epoch 30/200, Loss: 1215.3075
Epoch 31/200, Loss: 1215.3377
Epoch 32/200, Loss: 1215.4881
Epoch 33/200, Loss: 1215.2902
Epoch 34/200, Loss: 1215.2763
Epoch 35/200, Loss: 1215.3142
Epoch 36/200, Loss: 1215.3405
Epoch 37/200, Loss: 1215.2522
Epoch 38/200, Loss: 1215.3514
Epoch 39/200, Loss: 1215.4179
Epoch 40/200, Loss: 1215.4672
Epoch 41/200, Loss: 1215.3047
Epoch 42/200, Loss: 1215.3949
Epoch 43/200, Loss: 1215.4379
Epoch 44/200, Loss: 1215.3583
Epoch 45/200, Loss: 1215.4226
Epoch 46/200, Loss: 1215.4366
Epoch 47/200, Loss: 1215.2735
Epoch 48/200, Loss: 1215.2467
Epoch 49/200, Loss: 1215.2579
Epoch 50/200, Loss: 1215.2767
Epoch 51/200, Loss: 1215.2599
Epoch 52/200, Loss: 1215.2899
Epoch 53/200, Loss: 1215.3721
Epoch 54/200, Loss: 1215.4859
Epoch 55/200, Loss: 1215.3170
Epoch 56/200, Loss: 1215.3716
Epoch 57/200, Loss: 1215.4762
Epoch 58/200, Loss: 1215.3442
Epoch 59/200, Loss: 1215.3306
Epoch 60/200, Loss: 1215.4240
Epoch 61/200, Loss: 1215.4348
Epoch 62/200, Loss: 1215.2953
Epoch 63/200, Loss: 1215.4440
Epoch 64/200, Loss: 1215.2290
Epoch 65/200, Loss: 1215.2922
Epoch 66/200, Loss: 1215.2461
Epoch 67/200, Loss: 1215.3437
Epoch 68/200, Loss: 1215.4797
Epoch 69/200, Loss: 1215.3573
Epoch 70/200, Loss: 1215.3909
Epoch 71/200, Loss: 1215.4617
Epoch 72/200, Loss: 1215.3741
Epoch 73/200, Loss: 1215.4022
Epoch 74/200, Loss: 1215.3353
Epoch 75/200, Loss: 1215.3967
Epoch 76/200, Loss: 1215.3766
Epoch 77/200, Loss: 1215.3919
Epoch 78/200, Loss: 1215.3372
Epoch 79/200, Loss: 1215.3662
Epoch 80/200, Loss: 1215.3425
Epoch 81/200, Loss: 1215.3930
Epoch 82/200, Loss: 1215.3487
Epoch 83/200, Loss: 1215.4813
Epoch 84/200, Loss: 1215.2531
Epoch 85/200, Loss: 1215.4812
Epoch 86/200, Loss: 1215.2818
Epoch 87/200, Loss: 1215.3213
Epoch 88/200, Loss: 1215.2624
Epoch 89/200, Loss: 1215.2330
Epoch 90/200, Loss: 1215.1856
Epoch 91/200, Loss: 1215.2546
Epoch 92/200, Loss: 1215.3582
Epoch 93/200, Loss: 1215.3388
Epoch 94/200, Loss: 1215.3030
Epoch 95/200, Loss: 1215.2718
Epoch 96/200, Loss: 1215.4491
Epoch 97/200, Loss: 1215.5081
Epoch 98/200, Loss: 1215.3640
Epoch 99/200, Loss: 1215.4707
Epoch 100/200, Loss: 1215.3497
Epoch 101/200, Loss: 1215.2519
Epoch 102/200, Loss: 1215.4412
Epoch 103/200, Loss: 1215.3678
Epoch 104/200, Loss: 1215.4160
Epoch 105/200, Loss: 1215.3790
Epoch 106/200, Loss: 1215.4028
Epoch 107/200, Loss: 1215.2154
Epoch 108/200, Loss: 1215.5190
Epoch 109/200, Loss: 1215.5872
Epoch 110/200, Loss: 1215.2979
Epoch 111/200, Loss: 1215.3883
Epoch 112/200, Loss: 1215.2750
Epoch 113/200, Loss: 1215.4055
Epoch 114/200, Loss: 1215.3931
Epoch 115/200, Loss: 1215.3131
Epoch 116/200, Loss: 1215.3339
Epoch 117/200, Loss: 1215.4512
Epoch 118/200, Loss: 1215.3233
Epoch 119/200, Loss: 1215.2655
Epoch 120/200, Loss: 1215.3809
Epoch 121/200, Loss: 1215.3117
Epoch 122/200, Loss: 1215.2360
Epoch 123/200, Loss: 1215.4293
Epoch 124/200, Loss: 1215.5220
Epoch 125/200, Loss: 1215.2932
Epoch 126/200, Loss: 1215.4742
Epoch 127/200, Loss: 1215.3210
Epoch 128/200, Loss: 1215.3064
Epoch 129/200, Loss: 1215.2827
Epoch 130/200, Loss: 1215.4447
Epoch 131/200, Loss: 1215.2817
Epoch 132/200, Loss: 1215.2297
Epoch 133/200, Loss: 1215.4397
Epoch 134/200, Loss: 1215.4406
Epoch 135/200, Loss: 1215.2948
Epoch 136/200, Loss: 1215.4066
Epoch 137/200, Loss: 1215.3894
Epoch 138/200, Loss: 1215.4760
Epoch 139/200, Loss: 1215.2680
Epoch 140/200, Loss: 1215.4052
Epoch 141/200, Loss: 1215.3503
Epoch 142/200, Loss: 1215.3073
Epoch 143/200, Loss: 1215.3224
Epoch 144/200, Loss: 1215.3781
Epoch 145/200, Loss: 1215.2385
Epoch 146/200, Loss: 1215.3874
Epoch 147/200, Loss: 1215.3424
Epoch 148/200, Loss: 1215.3276
Epoch 149/200, Loss: 1215.4752
Epoch 150/200, Loss: 1215.5029
Epoch 151/200, Loss: 1215.5022
Epoch 152/200, Loss: 1215.3744
Epoch 153/200, Loss: 1215.4755
Epoch 154/200, Loss: 1215.1786
Epoch 155/200, Loss: 1215.2651
Epoch 156/200, Loss: 1215.3645
Epoch 157/200, Loss: 1215.3912
Epoch 158/200, Loss: 1215.2649
Epoch 159/200, Loss: 1215.2781
Epoch 160/200, Loss: 1215.4798
Epoch 161/200, Loss: 1215.3148
Epoch 162/200, Loss: 1215.3049
Epoch 163/200, Loss: 1215.3709
Epoch 164/200, Loss: 1215.3103
Epoch 165/200, Loss: 1215.3192
Epoch 166/200, Loss: 1215.4183
Epoch 167/200, Loss: 1215.2553
Epoch 168/200, Loss: 1215.2183
Epoch 169/200, Loss: 1215.4329
Epoch 170/200, Loss: 1215.3635
Epoch 171/200, Loss: 1215.2697
Epoch 172/200, Loss: 1215.2703
Epoch 173/200, Loss: 1215.3409
Epoch 174/200, Loss: 1215.3499
Epoch 175/200, Loss: 1215.4129
Epoch 176/200, Loss: 1215.2418
Epoch 177/200, Loss: 1215.3299
Epoch 178/200, Loss: 1215.4675
Epoch 179/200, Loss: 1215.3328
Epoch 180/200, Loss: 1215.3244
Epoch 181/200, Loss: 1215.4530
Epoch 182/200, Loss: 1215.3880
Epoch 183/200, Loss: 1215.3205
Epoch 184/200, Loss: 1215.5772
Epoch 185/200, Loss: 1215.2964
Epoch 186/200, Loss: 1215.3285
Epoch 187/200, Loss: 1215.2041
Epoch 188/200, Loss: 1215.3374
Epoch 189/200, Loss: 1215.4684
Epoch 190/200, Loss: 1215.3962
Epoch 191/200, Loss: 1215.3813
Epoch 192/200, Loss: 1215.4365
Epoch 193/200, Loss: 1215.3094
Epoch 194/200, Loss: 1215.2991
Epoch 195/200, Loss: 1215.3075
Epoch 196/200, Loss: 1215.3234
Epoch 197/200, Loss: 1215.3590
Epoch 198/200, Loss: 1215.3133
Epoch 199/200, Loss: 1215.5420
Epoch 200/200, Loss: 1215.2937
Train Accuracy: 79.27% | Test Accuracy: 53.26%
Precision: 0.5442 | Recall: 0.5326 | F1-Score: 0.5159
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 14.70%
Precision: 0.1456 | Recall: 0.1470 | F1-Score: 0.1448


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1830 batches, Testing set - 196 batches
Epoch 1/200, Loss: 6352.4918
Epoch 2/200, Loss: 5700.5088
Epoch 3/200, Loss: 5476.6498
Epoch 4/200, Loss: 5291.9661
Epoch 5/200, Loss: 5144.9148
Epoch 6/200, Loss: 5010.9895
Epoch 7/200, Loss: 4992.2287
Epoch 8/200, Loss: 4977.7869
Epoch 9/200, Loss: 4963.3095
Epoch 10/200, Loss: 4947.4594
Epoch 11/200, Loss: 4934.0562
Epoch 12/200, Loss: 4932.0444
Epoch 13/200, Loss: 4930.6884
Epoch 14/200, Loss: 4929.3545
Epoch 15/200, Loss: 4928.0721
Epoch 16/200, Loss: 4926.5727
Epoch 17/200, Loss: 4926.1920
Epoch 18/200, Loss: 4925.9240
Epoch 19/200, Loss: 4925.9323
Epoch 20/200, Loss: 4925.8160
Epoch 21/200, Loss: 4925.5923
Epoch 22/200, Loss: 4925.5405
Epoch 23/200, Loss: 4925.5261
Epoch 24/200, Loss: 4925.5498
Epoch 25/200, Loss: 4925.4311
Epoch 26/200, Loss: 4925.6455
Epoch 27/200, Loss: 4925.5770
Epoch 28/200, Loss: 4925.5090
Epoch 29/200, Loss: 4925.5786
Epoch 30/200, Loss: 4925.5684
Epoch 31/200, Loss: 4925.5708
Epoch 32/200, Loss: 4925.5965
Epoch 33/200, Loss: 4925.4190
Epoch 34/200, Loss: 4925.4825
Epoch 35/200, Loss: 4925.7180
Epoch 36/200, Loss: 4925.6807
Epoch 37/200, Loss: 4925.5615
Epoch 38/200, Loss: 4925.4902
Epoch 39/200, Loss: 4925.5752
Epoch 40/200, Loss: 4925.6344
Epoch 41/200, Loss: 4925.6904
Epoch 42/200, Loss: 4925.4945
Epoch 43/200, Loss: 4925.4119
Epoch 44/200, Loss: 4925.6684
Epoch 45/200, Loss: 4925.5893
Epoch 46/200, Loss: 4925.6242
Epoch 47/200, Loss: 4925.5715
Epoch 48/200, Loss: 4925.5300
Epoch 49/200, Loss: 4925.5603
Epoch 50/200, Loss: 4925.4945
Epoch 51/200, Loss: 4925.5056
Epoch 52/200, Loss: 4925.5662
Epoch 53/200, Loss: 4925.5013
Epoch 54/200, Loss: 4925.5503
Epoch 55/200, Loss: 4925.5855
Epoch 56/200, Loss: 4925.5070
Epoch 57/200, Loss: 4925.6396
Epoch 58/200, Loss: 4925.5385
Epoch 59/200, Loss: 4925.5500
Epoch 60/200, Loss: 4925.5019
Epoch 61/200, Loss: 4925.5018
Epoch 62/200, Loss: 4925.5898
Epoch 63/200, Loss: 4925.5260
Epoch 64/200, Loss: 4925.6556
Epoch 65/200, Loss: 4925.4656
Epoch 66/200, Loss: 4925.6351
Epoch 67/200, Loss: 4925.5371
Epoch 68/200, Loss: 4925.3987
Epoch 69/200, Loss: 4925.5431
Epoch 70/200, Loss: 4925.6742
Epoch 71/200, Loss: 4925.6232
Epoch 72/200, Loss: 4925.6843
Epoch 73/200, Loss: 4925.6365
Epoch 74/200, Loss: 4925.4924
Epoch 75/200, Loss: 4925.6696
Epoch 76/200, Loss: 4925.6114
Epoch 77/200, Loss: 4925.5995
Epoch 78/200, Loss: 4925.4945
Epoch 79/200, Loss: 4925.6717
Epoch 80/200, Loss: 4925.5754
Epoch 81/200, Loss: 4925.4849
Epoch 82/200, Loss: 4925.6611
Epoch 83/200, Loss: 4925.6798
Epoch 84/200, Loss: 4925.3703
Epoch 85/200, Loss: 4925.5360
Epoch 86/200, Loss: 4925.5456
Epoch 87/200, Loss: 4925.5719
Epoch 88/200, Loss: 4925.5647
Epoch 89/200, Loss: 4925.5815
Epoch 90/200, Loss: 4925.4880
Epoch 91/200, Loss: 4925.5087
Epoch 92/200, Loss: 4925.6040
Epoch 93/200, Loss: 4925.5142
Epoch 94/200, Loss: 4925.5246
Epoch 95/200, Loss: 4925.5903
Epoch 96/200, Loss: 4925.5063
Epoch 97/200, Loss: 4925.6091
Epoch 98/200, Loss: 4925.4791
Epoch 99/200, Loss: 4925.4539
Epoch 100/200, Loss: 4925.5100
Epoch 101/200, Loss: 4925.6732
Epoch 102/200, Loss: 4925.5341
Epoch 103/200, Loss: 4925.4664
Epoch 104/200, Loss: 4925.4804
Epoch 105/200, Loss: 4925.5718
Epoch 106/200, Loss: 4925.3913
Epoch 107/200, Loss: 4925.5108
Epoch 108/200, Loss: 4925.4804
Epoch 109/200, Loss: 4925.4382
Epoch 110/200, Loss: 4925.6865
Epoch 111/200, Loss: 4925.4723
Epoch 112/200, Loss: 4925.6295
Epoch 113/200, Loss: 4925.6544
Epoch 114/200, Loss: 4925.5414
Epoch 115/200, Loss: 4925.4758
Epoch 116/200, Loss: 4925.5759
Epoch 117/200, Loss: 4925.6282
Epoch 118/200, Loss: 4925.5548
Epoch 119/200, Loss: 4925.4852
Epoch 120/200, Loss: 4925.5802
Epoch 121/200, Loss: 4925.5662
Epoch 122/200, Loss: 4925.5003
Epoch 123/200, Loss: 4925.5977
Epoch 124/200, Loss: 4925.5209
Epoch 125/200, Loss: 4925.5735
Epoch 126/200, Loss: 4925.4494
Epoch 127/200, Loss: 4925.5769
Epoch 128/200, Loss: 4925.4882
Epoch 129/200, Loss: 4925.6458
Epoch 130/200, Loss: 4925.6106
Epoch 131/200, Loss: 4925.5413
Epoch 132/200, Loss: 4925.5300
Epoch 133/200, Loss: 4925.6168
Epoch 134/200, Loss: 4925.5362
Epoch 135/200, Loss: 4925.7642
Epoch 136/200, Loss: 4925.6308
Epoch 137/200, Loss: 4925.5205
Epoch 138/200, Loss: 4925.6302
Epoch 139/200, Loss: 4925.5228
Epoch 140/200, Loss: 4925.5567
Epoch 141/200, Loss: 4925.5552
Epoch 142/200, Loss: 4925.6036
Epoch 143/200, Loss: 4925.6568
Epoch 144/200, Loss: 4925.6325
Epoch 145/200, Loss: 4925.4750
Epoch 146/200, Loss: 4925.5939
Epoch 147/200, Loss: 4925.5982
Epoch 148/200, Loss: 4925.5137
Epoch 149/200, Loss: 4925.4963
Epoch 150/200, Loss: 4925.6761
Epoch 151/200, Loss: 4925.6030
Epoch 152/200, Loss: 4925.5324
Epoch 153/200, Loss: 4925.7872
Epoch 154/200, Loss: 4925.5842
Epoch 155/200, Loss: 4925.7443
Epoch 156/200, Loss: 4925.6018
Epoch 157/200, Loss: 4925.6019
Epoch 158/200, Loss: 4925.5803
Epoch 159/200, Loss: 4925.6082
Epoch 160/200, Loss: 4925.5928
Epoch 161/200, Loss: 4925.5607
Epoch 162/200, Loss: 4925.6296
Epoch 163/200, Loss: 4925.5688
Epoch 164/200, Loss: 4925.5788
Epoch 165/200, Loss: 4925.5391
Epoch 166/200, Loss: 4925.6409
Epoch 167/200, Loss: 4925.6349
Epoch 168/200, Loss: 4925.4914
Epoch 169/200, Loss: 4925.4862
Epoch 170/200, Loss: 4925.6943
Epoch 171/200, Loss: 4925.4605
Epoch 172/200, Loss: 4925.6843
Epoch 173/200, Loss: 4925.6487
Epoch 174/200, Loss: 4925.5285
Epoch 175/200, Loss: 4925.5188
Epoch 176/200, Loss: 4925.5013
Epoch 177/200, Loss: 4925.5045
Epoch 178/200, Loss: 4925.5175
Epoch 179/200, Loss: 4925.5994
Epoch 180/200, Loss: 4925.5697
Epoch 181/200, Loss: 4925.6543
Epoch 182/200, Loss: 4925.4870
Epoch 183/200, Loss: 4925.5891
Epoch 184/200, Loss: 4925.5225
Epoch 185/200, Loss: 4925.5652
Epoch 186/200, Loss: 4925.6250
Epoch 187/200, Loss: 4925.6020
Epoch 188/200, Loss: 4925.5587
Epoch 189/200, Loss: 4925.5665
Epoch 190/200, Loss: 4925.4357
Epoch 191/200, Loss: 4925.5728
Epoch 192/200, Loss: 4925.5251
Epoch 193/200, Loss: 4925.4404
Epoch 194/200, Loss: 4925.6257
Epoch 195/200, Loss: 4925.5414
Epoch 196/200, Loss: 4925.5567
Epoch 197/200, Loss: 4925.5615
Epoch 198/200, Loss: 4925.4884
Epoch 199/200, Loss: 4925.5951
Epoch 200/200, Loss: 4925.6009
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 19.43% | Test Accuracy: 17.94%
Precision: 0.1029 | Recall: 0.1794 | F1-Score: 0.1163

Processing Subject 3...
Top 32 discriminative features: [26 38 46 44 14 41 12 29 18 16 42 40 30 22 45 25 37 63 61 34 33 21 28  9
 32 59 57 49 51 20  2  0]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 117783
	# per Class in Train Dataset:
		Class 0: 3512
		Class 1: 3144
		Class 2: 3277
		Class 3: 3600
		Class 4: 3595
		Class 5: 3496
		Class 6: 3600
		Class 7: 3488
		Class 8: 3288
		Class 9: 3600
		Class 10: 3561
		Class 11: 1753
		Class 12: 1765
		Class 13: 2355
		Class 14: 3074
		Class 15: 2650
		Class 16: 1751
		Class 17: 1718
		Class 18: 2212
		Class 19: 2980
		Class 20: 3600
		Class 21: 3600
		Class 22: 3600
		Class 23: 3579
		Class 24: 2636
		Class 25: 3600
		Class 26: 2360
		Class 27: 3118
		Class 28: 2871
		Class 29: 3109
		Class 30: 3343
		Class 31: 3180
		Class 32: 2684
		Class 33: 2259
		Class 34: 3600
		Class 35: 3143
		Class 36: 3187
		Class 37: 3399
		Class 38: 2496
	# of Testing Samples: 11792
	# per Class in Test Dataset:
		Class 0: 336
		Class 1: 331
		Class 2: 381
		Class 3: 400
		Class 4: 334
		Class 5: 392
		Class 6: 400
		Class 7: 383
		Class 8: 251
		Class 9: 400
		Class 10: 368
		Class 11: 178
		Class 12: 217
		Class 13: 216
		Class 14: 220
		Class 15: 198
		Class 16: 220
		Class 17: 219
		Class 18: 177
		Class 19: 213
		Class 20: 391
		Class 21: 400
		Class 22: 379
		Class 23: 383
		Class 24: 300
		Class 25: 400
		Class 26: 230
		Class 27: 400
		Class 28: 202
		Class 29: 282
		Class 30: 355
		Class 31: 342
		Class 32: 206
		Class 33: 168
		Class 34: 400
		Class 35: 262
		Class 36: 236
		Class 37: 343
		Class 38: 279
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 60.13%
Precision: 0.6506 | Recall: 0.6013 | F1-Score: 0.5897


Running Deep Learning Classifier...
DataLoader: Training set - 1841 batches, Testing set - 185 batches
Epoch 1/200, Loss: 3962.6918
Epoch 2/200, Loss: 2374.9022
Epoch 3/200, Loss: 1914.5663
Epoch 4/200, Loss: 1631.9332
Epoch 5/200, Loss: 1442.8128
Epoch 6/200, Loss: 1314.7439
Epoch 7/200, Loss: 1296.4675
Epoch 8/200, Loss: 1282.0154
Epoch 9/200, Loss: 1268.4763
Epoch 10/200, Loss: 1255.9465
Epoch 11/200, Loss: 1243.9982
Epoch 12/200, Loss: 1241.7107
Epoch 13/200, Loss: 1240.2627
Epoch 14/200, Loss: 1239.1146
Epoch 15/200, Loss: 1237.9227
Epoch 16/200, Loss: 1236.3505
Epoch 17/200, Loss: 1236.2859
Epoch 18/200, Loss: 1235.9373
Epoch 19/200, Loss: 1236.0632
Epoch 20/200, Loss: 1236.0114
Epoch 21/200, Loss: 1235.5717
Epoch 22/200, Loss: 1235.6066
Epoch 23/200, Loss: 1235.4705
Epoch 24/200, Loss: 1235.7165
Epoch 25/200, Loss: 1235.9016
Epoch 26/200, Loss: 1235.4111
Epoch 27/200, Loss: 1235.4317
Epoch 28/200, Loss: 1235.5491
Epoch 29/200, Loss: 1235.8029
Epoch 30/200, Loss: 1235.6407
Epoch 31/200, Loss: 1235.6380
Epoch 32/200, Loss: 1235.6063
Epoch 33/200, Loss: 1235.5662
Epoch 34/200, Loss: 1235.7106
Epoch 35/200, Loss: 1235.5617
Epoch 36/200, Loss: 1235.6731
Epoch 37/200, Loss: 1235.5570
Epoch 38/200, Loss: 1235.7138
Epoch 39/200, Loss: 1235.4678
Epoch 40/200, Loss: 1235.5897
Epoch 41/200, Loss: 1235.4814
Epoch 42/200, Loss: 1235.5858
Epoch 43/200, Loss: 1235.5973
Epoch 44/200, Loss: 1235.7378
Epoch 45/200, Loss: 1235.6419
Epoch 46/200, Loss: 1235.6927
Epoch 47/200, Loss: 1235.4638
Epoch 48/200, Loss: 1235.7533
Epoch 49/200, Loss: 1235.5357
Epoch 50/200, Loss: 1235.7746
Epoch 51/200, Loss: 1235.6519
Epoch 52/200, Loss: 1235.6919
Epoch 53/200, Loss: 1235.4872
Epoch 54/200, Loss: 1235.7478
Epoch 55/200, Loss: 1235.5503
Epoch 56/200, Loss: 1235.5685
Epoch 57/200, Loss: 1235.5260
Epoch 58/200, Loss: 1235.5912
Epoch 59/200, Loss: 1235.8178
Epoch 60/200, Loss: 1235.6040
Epoch 61/200, Loss: 1235.7421
Epoch 62/200, Loss: 1235.9168
Epoch 63/200, Loss: 1235.8938
Epoch 64/200, Loss: 1235.6690
Epoch 65/200, Loss: 1235.6482
Epoch 66/200, Loss: 1235.5778
Epoch 67/200, Loss: 1235.6239
Epoch 68/200, Loss: 1235.3935
Epoch 69/200, Loss: 1235.4534
Epoch 70/200, Loss: 1235.6807
Epoch 71/200, Loss: 1235.8616
Epoch 72/200, Loss: 1235.4608
Epoch 73/200, Loss: 1235.5901
Epoch 74/200, Loss: 1235.5552
Epoch 75/200, Loss: 1235.7680
Epoch 76/200, Loss: 1235.5115
Epoch 77/200, Loss: 1235.4905
Epoch 78/200, Loss: 1235.7742
Epoch 79/200, Loss: 1235.6769
Epoch 80/200, Loss: 1235.6456
Epoch 81/200, Loss: 1235.4939
Epoch 82/200, Loss: 1235.7009
Epoch 83/200, Loss: 1235.5475
Epoch 84/200, Loss: 1235.5771
Epoch 85/200, Loss: 1235.7221
Epoch 86/200, Loss: 1235.6042
Epoch 87/200, Loss: 1235.4373
Epoch 88/200, Loss: 1235.6081
Epoch 89/200, Loss: 1235.5866
Epoch 90/200, Loss: 1235.8229
Epoch 91/200, Loss: 1235.4597
Epoch 92/200, Loss: 1235.5458
Epoch 93/200, Loss: 1235.6482
Epoch 94/200, Loss: 1235.8148
Epoch 95/200, Loss: 1235.5461
Epoch 96/200, Loss: 1235.8123
Epoch 97/200, Loss: 1235.7308
Epoch 98/200, Loss: 1235.4631
Epoch 99/200, Loss: 1235.6532
Epoch 100/200, Loss: 1235.6635
Epoch 101/200, Loss: 1235.7367
Epoch 102/200, Loss: 1235.3916
Epoch 103/200, Loss: 1235.5230
Epoch 104/200, Loss: 1235.8009
Epoch 105/200, Loss: 1235.4914
Epoch 106/200, Loss: 1235.4969
Epoch 107/200, Loss: 1235.6051
Epoch 108/200, Loss: 1235.4639
Epoch 109/200, Loss: 1235.4021
Epoch 110/200, Loss: 1235.6403
Epoch 111/200, Loss: 1235.7118
Epoch 112/200, Loss: 1235.6092
Epoch 113/200, Loss: 1235.5044
Epoch 114/200, Loss: 1235.7408
Epoch 115/200, Loss: 1235.6821
Epoch 116/200, Loss: 1235.5672
Epoch 117/200, Loss: 1235.8187
Epoch 118/200, Loss: 1235.6756
Epoch 119/200, Loss: 1235.5513
Epoch 120/200, Loss: 1235.5291
Epoch 121/200, Loss: 1235.6943
Epoch 122/200, Loss: 1235.6209
Epoch 123/200, Loss: 1235.7209
Epoch 124/200, Loss: 1235.5736
Epoch 125/200, Loss: 1235.5354
Epoch 126/200, Loss: 1235.6177
Epoch 127/200, Loss: 1235.6219
Epoch 128/200, Loss: 1235.6247
Epoch 129/200, Loss: 1235.8182
Epoch 130/200, Loss: 1235.5645
Epoch 131/200, Loss: 1235.6931
Epoch 132/200, Loss: 1235.6671
Epoch 133/200, Loss: 1235.5974
Epoch 134/200, Loss: 1235.4536
Epoch 135/200, Loss: 1235.9261
Epoch 136/200, Loss: 1235.6233
Epoch 137/200, Loss: 1235.5921
Epoch 138/200, Loss: 1235.5910
Epoch 139/200, Loss: 1235.5320
Epoch 140/200, Loss: 1235.6868
Epoch 141/200, Loss: 1235.8889
Epoch 142/200, Loss: 1235.7924
Epoch 143/200, Loss: 1235.6247
Epoch 144/200, Loss: 1235.6684
Epoch 145/200, Loss: 1235.6994
Epoch 146/200, Loss: 1235.8212
Epoch 147/200, Loss: 1235.4863
Epoch 148/200, Loss: 1235.6351
Epoch 149/200, Loss: 1235.5146
Epoch 150/200, Loss: 1235.5518
Epoch 151/200, Loss: 1235.3771
Epoch 152/200, Loss: 1235.6757
Epoch 153/200, Loss: 1235.5876
Epoch 154/200, Loss: 1235.6041
Epoch 155/200, Loss: 1235.5602
Epoch 156/200, Loss: 1235.8361
Epoch 157/200, Loss: 1235.4776
Epoch 158/200, Loss: 1235.8356
Epoch 159/200, Loss: 1235.5938
Epoch 160/200, Loss: 1235.7291
Epoch 161/200, Loss: 1235.6780
Epoch 162/200, Loss: 1235.5007
Epoch 163/200, Loss: 1235.5553
Epoch 164/200, Loss: 1235.4444
Epoch 165/200, Loss: 1235.4864
Epoch 166/200, Loss: 1235.5210
Epoch 167/200, Loss: 1235.6528
Epoch 168/200, Loss: 1235.8820
Epoch 169/200, Loss: 1235.6064
Epoch 170/200, Loss: 1235.5880
Epoch 171/200, Loss: 1235.5275
Epoch 172/200, Loss: 1235.7809
Epoch 173/200, Loss: 1235.4980
Epoch 174/200, Loss: 1235.4716
Epoch 175/200, Loss: 1235.5857
Epoch 176/200, Loss: 1235.4500
Epoch 177/200, Loss: 1235.6880
Epoch 178/200, Loss: 1235.5627
Epoch 179/200, Loss: 1235.6723
Epoch 180/200, Loss: 1235.5043
Epoch 181/200, Loss: 1235.6291
Epoch 182/200, Loss: 1235.4382
Epoch 183/200, Loss: 1235.5667
Epoch 184/200, Loss: 1235.5678
Epoch 185/200, Loss: 1235.7486
Epoch 186/200, Loss: 1235.6401
Epoch 187/200, Loss: 1235.7469
Epoch 188/200, Loss: 1235.5812
Epoch 189/200, Loss: 1235.5984
Epoch 190/200, Loss: 1235.7682
Epoch 191/200, Loss: 1235.6103
Epoch 192/200, Loss: 1235.6540
Epoch 193/200, Loss: 1235.6915
Epoch 194/200, Loss: 1235.5750
Epoch 195/200, Loss: 1235.4983
Epoch 196/200, Loss: 1235.6660
Epoch 197/200, Loss: 1235.6352
Epoch 198/200, Loss: 1235.5677
Epoch 199/200, Loss: 1235.7166
Epoch 200/200, Loss: 1235.6026
Train Accuracy: 78.08% | Test Accuracy: 50.72%
Precision: 0.5295 | Recall: 0.5072 | F1-Score: 0.4936
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 13.94%
Precision: 0.1401 | Recall: 0.1394 | F1-Score: 0.1372


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1841 batches, Testing set - 185 batches
Epoch 1/200, Loss: 6098.2893
Epoch 2/200, Loss: 5638.2420
Epoch 3/200, Loss: 5406.2361
Epoch 4/200, Loss: 5227.9221
Epoch 5/200, Loss: 5098.0257
Epoch 6/200, Loss: 4977.7720
Epoch 7/200, Loss: 4963.9404
Epoch 8/200, Loss: 4952.8612
Epoch 9/200, Loss: 4943.2818
Epoch 10/200, Loss: 4934.5681
Epoch 11/200, Loss: 4921.8335
Epoch 12/200, Loss: 4920.4895
Epoch 13/200, Loss: 4919.3240
Epoch 14/200, Loss: 4918.5394
Epoch 15/200, Loss: 4917.4373
Epoch 16/200, Loss: 4916.1313
Epoch 17/200, Loss: 4916.0307
Epoch 18/200, Loss: 4915.8748
Epoch 19/200, Loss: 4915.8665
Epoch 20/200, Loss: 4915.7053
Epoch 21/200, Loss: 4915.5290
Epoch 22/200, Loss: 4915.4568
Epoch 23/200, Loss: 4915.4211
Epoch 24/200, Loss: 4915.2709
Epoch 25/200, Loss: 4915.5013
Epoch 26/200, Loss: 4915.4630
Epoch 27/200, Loss: 4915.4016
Epoch 28/200, Loss: 4915.2818
Epoch 29/200, Loss: 4915.5436
Epoch 30/200, Loss: 4915.4266
Epoch 31/200, Loss: 4915.7026
Epoch 32/200, Loss: 4915.3762
Epoch 33/200, Loss: 4915.3831
Epoch 34/200, Loss: 4915.4163
Epoch 35/200, Loss: 4915.3689
Epoch 36/200, Loss: 4915.4651
Epoch 37/200, Loss: 4915.6112
Epoch 38/200, Loss: 4915.3333
Epoch 39/200, Loss: 4915.4173
Epoch 40/200, Loss: 4915.3997
Epoch 41/200, Loss: 4915.3701
Epoch 42/200, Loss: 4915.4999
Epoch 43/200, Loss: 4915.2798
Epoch 44/200, Loss: 4915.4671
Epoch 45/200, Loss: 4915.2270
Epoch 46/200, Loss: 4915.5322
Epoch 47/200, Loss: 4915.5771
Epoch 48/200, Loss: 4915.1913
Epoch 49/200, Loss: 4915.2607
Epoch 50/200, Loss: 4915.3953
Epoch 51/200, Loss: 4915.3495
Epoch 52/200, Loss: 4915.4318
Epoch 53/200, Loss: 4915.2802
Epoch 54/200, Loss: 4915.5352
Epoch 55/200, Loss: 4915.4597
Epoch 56/200, Loss: 4915.3288
Epoch 57/200, Loss: 4915.2236
Epoch 58/200, Loss: 4915.4560
Epoch 59/200, Loss: 4915.4891
Epoch 60/200, Loss: 4915.3172
Epoch 61/200, Loss: 4915.6089
Epoch 62/200, Loss: 4915.2561
Epoch 63/200, Loss: 4915.3010
Epoch 64/200, Loss: 4915.4461
Epoch 65/200, Loss: 4915.4728
Epoch 66/200, Loss: 4915.3622
Epoch 67/200, Loss: 4915.3286
Epoch 68/200, Loss: 4915.3888
Epoch 69/200, Loss: 4915.3157
Epoch 70/200, Loss: 4915.3242
Epoch 71/200, Loss: 4915.4253
Epoch 72/200, Loss: 4915.3537
Epoch 73/200, Loss: 4915.1614
Epoch 74/200, Loss: 4915.4405
Epoch 75/200, Loss: 4915.2642
Epoch 76/200, Loss: 4915.4020
Epoch 77/200, Loss: 4915.3075
Epoch 78/200, Loss: 4915.5388
Epoch 79/200, Loss: 4915.3397
Epoch 80/200, Loss: 4915.4529
Epoch 81/200, Loss: 4915.3721
Epoch 82/200, Loss: 4915.4792
Epoch 83/200, Loss: 4915.5846
Epoch 84/200, Loss: 4915.3707
Epoch 85/200, Loss: 4915.5061
Epoch 86/200, Loss: 4915.2898
Epoch 87/200, Loss: 4915.4770
Epoch 88/200, Loss: 4915.3097
Epoch 89/200, Loss: 4915.3854
Epoch 90/200, Loss: 4915.6603
Epoch 91/200, Loss: 4915.5097
Epoch 92/200, Loss: 4915.4779
Epoch 93/200, Loss: 4915.3988
Epoch 94/200, Loss: 4915.6664
Epoch 95/200, Loss: 4915.5233
Epoch 96/200, Loss: 4915.5330
Epoch 97/200, Loss: 4915.4340
Epoch 98/200, Loss: 4915.3075
Epoch 99/200, Loss: 4915.3508
Epoch 100/200, Loss: 4915.3808
Epoch 101/200, Loss: 4915.3510
Epoch 102/200, Loss: 4915.1276
Epoch 103/200, Loss: 4915.5035
Epoch 104/200, Loss: 4915.1753
Epoch 105/200, Loss: 4915.3293
Epoch 106/200, Loss: 4915.2537
Epoch 107/200, Loss: 4915.3770
Epoch 108/200, Loss: 4915.4496
Epoch 109/200, Loss: 4915.5509
Epoch 110/200, Loss: 4915.2598
Epoch 111/200, Loss: 4915.4253
Epoch 112/200, Loss: 4915.4199
Epoch 113/200, Loss: 4915.5877
Epoch 114/200, Loss: 4915.3576
Epoch 115/200, Loss: 4915.2667
Epoch 116/200, Loss: 4915.3267
Epoch 117/200, Loss: 4915.4803
Epoch 118/200, Loss: 4915.3851
Epoch 119/200, Loss: 4915.2622
Epoch 120/200, Loss: 4915.4487
Epoch 121/200, Loss: 4915.4717
Epoch 122/200, Loss: 4915.3832
Epoch 123/200, Loss: 4915.4340
Epoch 124/200, Loss: 4915.4519
Epoch 125/200, Loss: 4915.5324
Epoch 126/200, Loss: 4915.5244
Epoch 127/200, Loss: 4915.4147
Epoch 128/200, Loss: 4915.3524
Epoch 129/200, Loss: 4915.2584
Epoch 130/200, Loss: 4915.3086
Epoch 131/200, Loss: 4915.4327
Epoch 132/200, Loss: 4915.5507
Epoch 133/200, Loss: 4915.4026
Epoch 134/200, Loss: 4915.5219
Epoch 135/200, Loss: 4915.4361
Epoch 136/200, Loss: 4915.4041
Epoch 137/200, Loss: 4915.4078
Epoch 138/200, Loss: 4915.3649
Epoch 139/200, Loss: 4915.4032
Epoch 140/200, Loss: 4915.5087
Epoch 141/200, Loss: 4915.3794
Epoch 142/200, Loss: 4915.2938
Epoch 143/200, Loss: 4915.6266
Epoch 144/200, Loss: 4915.3558
Epoch 145/200, Loss: 4915.4140
Epoch 146/200, Loss: 4915.6095
Epoch 147/200, Loss: 4915.4364
Epoch 148/200, Loss: 4915.3457
Epoch 149/200, Loss: 4915.5850
Epoch 150/200, Loss: 4915.4984
Epoch 151/200, Loss: 4915.1870
Epoch 152/200, Loss: 4915.3877
Epoch 153/200, Loss: 4915.3719
Epoch 154/200, Loss: 4915.3111
Epoch 155/200, Loss: 4915.2091
Epoch 156/200, Loss: 4915.4076
Epoch 157/200, Loss: 4915.4874
Epoch 158/200, Loss: 4915.3597
Epoch 159/200, Loss: 4915.3695
Epoch 160/200, Loss: 4915.3148
Epoch 161/200, Loss: 4915.4858
Epoch 162/200, Loss: 4915.5871
Epoch 163/200, Loss: 4915.2903
Epoch 164/200, Loss: 4915.2750
Epoch 165/200, Loss: 4915.3843
Epoch 166/200, Loss: 4915.5094
Epoch 167/200, Loss: 4915.4277
Epoch 168/200, Loss: 4915.3169
Epoch 169/200, Loss: 4915.4280
Epoch 170/200, Loss: 4915.3343
Epoch 171/200, Loss: 4915.3043
Epoch 172/200, Loss: 4915.4304
Epoch 173/200, Loss: 4915.5231
Epoch 174/200, Loss: 4915.3288
Epoch 175/200, Loss: 4915.5356
Epoch 176/200, Loss: 4915.3902
Epoch 177/200, Loss: 4915.4262
Epoch 178/200, Loss: 4915.5969
Epoch 179/200, Loss: 4915.3771
Epoch 180/200, Loss: 4915.3765
Epoch 181/200, Loss: 4915.5094
Epoch 182/200, Loss: 4915.5199
Epoch 183/200, Loss: 4915.4128
Epoch 184/200, Loss: 4915.2430
Epoch 185/200, Loss: 4915.4037
Epoch 186/200, Loss: 4915.4214
Epoch 187/200, Loss: 4915.2955
Epoch 188/200, Loss: 4915.3752
Epoch 189/200, Loss: 4915.3899
Epoch 190/200, Loss: 4915.5474
Epoch 191/200, Loss: 4915.3746
Epoch 192/200, Loss: 4915.5110
Epoch 193/200, Loss: 4915.3982
Epoch 194/200, Loss: 4915.3307
Epoch 195/200, Loss: 4915.4439
Epoch 196/200, Loss: 4915.6888
Epoch 197/200, Loss: 4915.3013
Epoch 198/200, Loss: 4915.4844
Epoch 199/200, Loss: 4915.2679
Epoch 200/200, Loss: 4915.3600
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 18.58% | Test Accuracy: 12.64%
Precision: 0.0824 | Recall: 0.1264 | F1-Score: 0.0888

Processing Subject 4...
Top 32 discriminative features: [26 38 46 44 29 41 14 12 18 16 42 40 45 22 30 37 25 33 63 61 34 21  9 28
 32 49 59 51 57 20  2  0]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 116815
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3135
		Class 2: 3368
		Class 3: 3600
		Class 4: 3529
		Class 5: 3488
		Class 6: 3600
		Class 7: 3491
		Class 8: 3189
		Class 9: 3600
		Class 10: 3530
		Class 11: 1766
		Class 12: 1830
		Class 13: 2319
		Class 14: 2953
		Class 15: 2579
		Class 16: 1795
		Class 17: 1783
		Class 18: 2168
		Class 19: 2854
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2641
		Class 25: 3600
		Class 26: 2317
		Class 27: 3139
		Class 28: 2800
		Class 29: 2991
		Class 30: 3326
		Class 31: 3216
		Class 32: 2655
		Class 33: 2202
		Class 34: 3600
		Class 35: 3104
		Class 36: 3106
		Class 37: 3342
		Class 38: 2419
	# of Testing Samples: 12760
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 340
		Class 2: 290
		Class 3: 400
		Class 4: 400
		Class 5: 400
		Class 6: 400
		Class 7: 380
		Class 8: 350
		Class 9: 400
		Class 10: 399
		Class 11: 165
		Class 12: 152
		Class 13: 252
		Class 14: 341
		Class 15: 269
		Class 16: 176
		Class 17: 154
		Class 18: 221
		Class 19: 339
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 295
		Class 25: 400
		Class 26: 273
		Class 27: 379
		Class 28: 273
		Class 29: 400
		Class 30: 372
		Class 31: 306
		Class 32: 235
		Class 33: 225
		Class 34: 400
		Class 35: 301
		Class 36: 317
		Class 37: 400
		Class 38: 356
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 68.69%
Precision: 0.7033 | Recall: 0.6869 | F1-Score: 0.6710


Running Deep Learning Classifier...
DataLoader: Training set - 1826 batches, Testing set - 200 batches
Epoch 1/200, Loss: 3858.8296
Epoch 2/200, Loss: 2379.2794
Epoch 3/200, Loss: 1962.2830
Epoch 4/200, Loss: 1703.9685
Epoch 5/200, Loss: 1518.7837
Epoch 6/200, Loss: 1388.6997
Epoch 7/200, Loss: 1370.8546
Epoch 8/200, Loss: 1354.4862
Epoch 9/200, Loss: 1340.9041
Epoch 10/200, Loss: 1326.4498
Epoch 11/200, Loss: 1315.0950
Epoch 12/200, Loss: 1313.2403
Epoch 13/200, Loss: 1311.4058
Epoch 14/200, Loss: 1309.9350
Epoch 15/200, Loss: 1308.9603
Epoch 16/200, Loss: 1307.2552
Epoch 17/200, Loss: 1307.1030
Epoch 18/200, Loss: 1306.9880
Epoch 19/200, Loss: 1306.6854
Epoch 20/200, Loss: 1306.6867
Epoch 21/200, Loss: 1306.7541
Epoch 22/200, Loss: 1306.2765
Epoch 23/200, Loss: 1306.5214
Epoch 24/200, Loss: 1306.9761
Epoch 25/200, Loss: 1306.6076
Epoch 26/200, Loss: 1306.5419
Epoch 27/200, Loss: 1306.5718
Epoch 28/200, Loss: 1306.7368
Epoch 29/200, Loss: 1306.6077
Epoch 30/200, Loss: 1306.7449
Epoch 31/200, Loss: 1306.6108
Epoch 32/200, Loss: 1306.4145
Epoch 33/200, Loss: 1306.7253
Epoch 34/200, Loss: 1306.3803
Epoch 35/200, Loss: 1306.4865
Epoch 36/200, Loss: 1306.9460
Epoch 37/200, Loss: 1306.3981
Epoch 38/200, Loss: 1306.2900
Epoch 39/200, Loss: 1306.7913
Epoch 40/200, Loss: 1306.3064
Epoch 41/200, Loss: 1306.4816
Epoch 42/200, Loss: 1306.4670
Epoch 43/200, Loss: 1306.3819
Epoch 44/200, Loss: 1306.4347
Epoch 45/200, Loss: 1306.7060
Epoch 46/200, Loss: 1306.2436
Epoch 47/200, Loss: 1306.3496
Epoch 48/200, Loss: 1306.4061
Epoch 49/200, Loss: 1306.4934
Epoch 50/200, Loss: 1306.3419
Epoch 51/200, Loss: 1306.7243
Epoch 52/200, Loss: 1306.6745
Epoch 53/200, Loss: 1306.7285
Epoch 54/200, Loss: 1306.3347
Epoch 55/200, Loss: 1306.6323
Epoch 56/200, Loss: 1306.3151
Epoch 57/200, Loss: 1306.7816
Epoch 58/200, Loss: 1306.6511
Epoch 59/200, Loss: 1306.5565
Epoch 60/200, Loss: 1306.4273
Epoch 61/200, Loss: 1307.0922
Epoch 62/200, Loss: 1306.4163
Epoch 63/200, Loss: 1306.4097
Epoch 64/200, Loss: 1306.6499
Epoch 65/200, Loss: 1306.6619
Epoch 66/200, Loss: 1306.5539
Epoch 67/200, Loss: 1306.6208
Epoch 68/200, Loss: 1307.0373
Epoch 69/200, Loss: 1306.4619
Epoch 70/200, Loss: 1306.9727
Epoch 71/200, Loss: 1306.6878
Epoch 72/200, Loss: 1306.6093
Epoch 73/200, Loss: 1306.3794
Epoch 74/200, Loss: 1306.7578
Epoch 75/200, Loss: 1306.5520
Epoch 76/200, Loss: 1306.7311
Epoch 77/200, Loss: 1306.6045
Epoch 78/200, Loss: 1306.6240
Epoch 79/200, Loss: 1306.5469
Epoch 80/200, Loss: 1306.5151
Epoch 81/200, Loss: 1306.7829
Epoch 82/200, Loss: 1306.6511
Epoch 83/200, Loss: 1306.3965
Epoch 84/200, Loss: 1306.5137
Epoch 85/200, Loss: 1306.3550
Epoch 86/200, Loss: 1306.1922
Epoch 87/200, Loss: 1306.5844
Epoch 88/200, Loss: 1306.9018
Epoch 89/200, Loss: 1306.6493
Epoch 90/200, Loss: 1306.4946
Epoch 91/200, Loss: 1306.4640
Epoch 92/200, Loss: 1306.4853
Epoch 93/200, Loss: 1306.4864
Epoch 94/200, Loss: 1306.6456
Epoch 95/200, Loss: 1306.6693
Epoch 96/200, Loss: 1306.6197
Epoch 97/200, Loss: 1306.8855
Epoch 98/200, Loss: 1306.7529
Epoch 99/200, Loss: 1306.6285
Epoch 100/200, Loss: 1306.4135
Epoch 101/200, Loss: 1307.0046
Epoch 102/200, Loss: 1306.4577
Epoch 103/200, Loss: 1306.6400
Epoch 104/200, Loss: 1306.4588
Epoch 105/200, Loss: 1306.4849
Epoch 106/200, Loss: 1306.5152
Epoch 107/200, Loss: 1306.5273
Epoch 108/200, Loss: 1306.6545
Epoch 109/200, Loss: 1306.3605
Epoch 110/200, Loss: 1306.9063
Epoch 111/200, Loss: 1306.8831
Epoch 112/200, Loss: 1306.3431
Epoch 113/200, Loss: 1306.9666
Epoch 114/200, Loss: 1306.5555
Epoch 115/200, Loss: 1306.5298
Epoch 116/200, Loss: 1306.5932
Epoch 117/200, Loss: 1306.4292
Epoch 118/200, Loss: 1306.5408
Epoch 119/200, Loss: 1306.5809
Epoch 120/200, Loss: 1306.5611
Epoch 121/200, Loss: 1306.4008
Epoch 122/200, Loss: 1306.7769
Epoch 123/200, Loss: 1306.5250
Epoch 124/200, Loss: 1306.7793
Epoch 125/200, Loss: 1306.4520
Epoch 126/200, Loss: 1306.7702
Epoch 127/200, Loss: 1306.4642
Epoch 128/200, Loss: 1306.8733
Epoch 129/200, Loss: 1306.5759
Epoch 130/200, Loss: 1306.6294
Epoch 131/200, Loss: 1306.6045
Epoch 132/200, Loss: 1306.6371
Epoch 133/200, Loss: 1306.4013
Epoch 134/200, Loss: 1306.5550
Epoch 135/200, Loss: 1306.5560
Epoch 136/200, Loss: 1306.5855
Epoch 137/200, Loss: 1306.9758
Epoch 138/200, Loss: 1306.6121
Epoch 139/200, Loss: 1306.7296
Epoch 140/200, Loss: 1306.4524
Epoch 141/200, Loss: 1306.6505
Epoch 142/200, Loss: 1306.2430
Epoch 143/200, Loss: 1306.9439
Epoch 144/200, Loss: 1306.2956
Epoch 145/200, Loss: 1306.7423
Epoch 146/200, Loss: 1306.3683
Epoch 147/200, Loss: 1306.7259
Epoch 148/200, Loss: 1306.4158
Epoch 149/200, Loss: 1306.5610
Epoch 150/200, Loss: 1306.3808
Epoch 151/200, Loss: 1306.7783
Epoch 152/200, Loss: 1306.3418
Epoch 153/200, Loss: 1306.4163
Epoch 154/200, Loss: 1306.5492
Epoch 155/200, Loss: 1306.4492
Epoch 156/200, Loss: 1306.2801
Epoch 157/200, Loss: 1306.8246
Epoch 158/200, Loss: 1306.5871
Epoch 159/200, Loss: 1306.9747
Epoch 160/200, Loss: 1306.3531
Epoch 161/200, Loss: 1306.4121
Epoch 162/200, Loss: 1306.6677
Epoch 163/200, Loss: 1306.7537
Epoch 164/200, Loss: 1306.6034
Epoch 165/200, Loss: 1306.5997
Epoch 166/200, Loss: 1306.4437
Epoch 167/200, Loss: 1306.4128
Epoch 168/200, Loss: 1306.9667
Epoch 169/200, Loss: 1306.3134
Epoch 170/200, Loss: 1306.6269
Epoch 171/200, Loss: 1307.1083
Epoch 172/200, Loss: 1306.8988
Epoch 173/200, Loss: 1306.5529
Epoch 174/200, Loss: 1307.0979
Epoch 175/200, Loss: 1306.2685
Epoch 176/200, Loss: 1306.4373
Epoch 177/200, Loss: 1307.0570
Epoch 178/200, Loss: 1306.3288
Epoch 179/200, Loss: 1306.6346
Epoch 180/200, Loss: 1307.2456
Epoch 181/200, Loss: 1306.7926
Epoch 182/200, Loss: 1306.4007
Epoch 183/200, Loss: 1306.6682
Epoch 184/200, Loss: 1306.6332
Epoch 185/200, Loss: 1306.6724
Epoch 186/200, Loss: 1306.5117
Epoch 187/200, Loss: 1306.5278
Epoch 188/200, Loss: 1306.7233
Epoch 189/200, Loss: 1306.3318
Epoch 190/200, Loss: 1306.4267
Epoch 191/200, Loss: 1306.7330
Epoch 192/200, Loss: 1306.4469
Epoch 193/200, Loss: 1306.6416
Epoch 194/200, Loss: 1306.9175
Epoch 195/200, Loss: 1306.4625
Epoch 196/200, Loss: 1306.6168
Epoch 197/200, Loss: 1306.8134
Epoch 198/200, Loss: 1306.5482
Epoch 199/200, Loss: 1306.5143
Epoch 200/200, Loss: 1306.4056
Train Accuracy: 77.43% | Test Accuracy: 59.76%
Precision: 0.6239 | Recall: 0.5976 | F1-Score: 0.5784
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 17.10%
Precision: 0.1709 | Recall: 0.1710 | F1-Score: 0.1683


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1826 batches, Testing set - 200 batches
Epoch 1/200, Loss: 6288.0211
Epoch 2/200, Loss: 5794.2480
Epoch 3/200, Loss: 5663.8848
Epoch 4/200, Loss: 5531.4145
Epoch 5/200, Loss: 5431.5473
Epoch 6/200, Loss: 5330.7884
Epoch 7/200, Loss: 5316.3555
Epoch 8/200, Loss: 5302.9801
Epoch 9/200, Loss: 5291.2519
Epoch 10/200, Loss: 5280.1373
Epoch 11/200, Loss: 5268.6732
Epoch 12/200, Loss: 5266.9002
Epoch 13/200, Loss: 5265.5874
Epoch 14/200, Loss: 5264.7605
Epoch 15/200, Loss: 5263.4248
Epoch 16/200, Loss: 5262.0075
Epoch 17/200, Loss: 5261.8379
Epoch 18/200, Loss: 5261.9979
Epoch 19/200, Loss: 5261.6195
Epoch 20/200, Loss: 5261.4000
Epoch 21/200, Loss: 5261.5084
Epoch 22/200, Loss: 5261.3763
Epoch 23/200, Loss: 5261.2125
Epoch 24/200, Loss: 5261.3224
Epoch 25/200, Loss: 5261.1351
Epoch 26/200, Loss: 5261.1228
Epoch 27/200, Loss: 5261.2347
Epoch 28/200, Loss: 5261.0683
Epoch 29/200, Loss: 5261.6006
Epoch 30/200, Loss: 5260.9855
Epoch 31/200, Loss: 5261.0863
Epoch 32/200, Loss: 5261.2629
Epoch 33/200, Loss: 5261.3146
Epoch 34/200, Loss: 5260.9247
Epoch 35/200, Loss: 5260.9030
Epoch 36/200, Loss: 5261.0617
Epoch 37/200, Loss: 5261.1589
Epoch 38/200, Loss: 5261.1464
Epoch 39/200, Loss: 5261.2326
Epoch 40/200, Loss: 5261.2269
Epoch 41/200, Loss: 5260.9537
Epoch 42/200, Loss: 5261.4793
Epoch 43/200, Loss: 5261.1106
Epoch 44/200, Loss: 5260.8820
Epoch 45/200, Loss: 5261.1154
Epoch 46/200, Loss: 5261.1934
Epoch 47/200, Loss: 5261.3094
Epoch 48/200, Loss: 5261.0501
Epoch 49/200, Loss: 5261.0950
Epoch 50/200, Loss: 5261.3439
Epoch 51/200, Loss: 5261.4397
Epoch 52/200, Loss: 5261.1812
Epoch 53/200, Loss: 5261.0698
Epoch 54/200, Loss: 5261.2441
Epoch 55/200, Loss: 5261.2202
Epoch 56/200, Loss: 5261.3054
Epoch 57/200, Loss: 5261.1297
Epoch 58/200, Loss: 5261.0674
Epoch 59/200, Loss: 5261.5035
Epoch 60/200, Loss: 5261.3928
Epoch 61/200, Loss: 5261.3066
Epoch 62/200, Loss: 5261.0942
Epoch 63/200, Loss: 5261.0882
Epoch 64/200, Loss: 5261.1688
Epoch 65/200, Loss: 5261.1403
Epoch 66/200, Loss: 5261.3219
Epoch 67/200, Loss: 5261.1701
Epoch 68/200, Loss: 5261.2571
Epoch 69/200, Loss: 5261.2009
Epoch 70/200, Loss: 5260.9281
Epoch 71/200, Loss: 5261.1376
Epoch 72/200, Loss: 5261.2833
Epoch 73/200, Loss: 5261.1467
Epoch 74/200, Loss: 5261.0943
Epoch 75/200, Loss: 5260.9939
Epoch 76/200, Loss: 5261.3390
Epoch 77/200, Loss: 5260.9280
Epoch 78/200, Loss: 5261.4657
Epoch 79/200, Loss: 5261.2156
Epoch 80/200, Loss: 5261.1907
Epoch 81/200, Loss: 5260.9328
Epoch 82/200, Loss: 5261.3675
Epoch 83/200, Loss: 5261.1428
Epoch 84/200, Loss: 5261.0950
Epoch 85/200, Loss: 5261.0955
Epoch 86/200, Loss: 5261.1167
Epoch 87/200, Loss: 5261.2707
Epoch 88/200, Loss: 5261.2363
Epoch 89/200, Loss: 5261.1207
Epoch 90/200, Loss: 5261.2542
Epoch 91/200, Loss: 5261.0129
Epoch 92/200, Loss: 5261.1170
Epoch 93/200, Loss: 5261.2486
Epoch 94/200, Loss: 5261.4445
Epoch 95/200, Loss: 5261.2294
Epoch 96/200, Loss: 5261.2207
Epoch 97/200, Loss: 5261.3051
Epoch 98/200, Loss: 5260.9990
Epoch 99/200, Loss: 5261.1261
Epoch 100/200, Loss: 5261.1529
Epoch 101/200, Loss: 5261.1254
Epoch 102/200, Loss: 5261.0387
Epoch 103/200, Loss: 5261.3616
Epoch 104/200, Loss: 5260.9000
Epoch 105/200, Loss: 5261.2001
Epoch 106/200, Loss: 5260.8845
Epoch 107/200, Loss: 5261.4991
Epoch 108/200, Loss: 5261.2386
Epoch 109/200, Loss: 5261.2368
Epoch 110/200, Loss: 5261.0433
Epoch 111/200, Loss: 5261.3937
Epoch 112/200, Loss: 5261.5003
Epoch 113/200, Loss: 5261.1199
Epoch 114/200, Loss: 5261.1491
Epoch 115/200, Loss: 5261.3557
Epoch 116/200, Loss: 5261.0446
Epoch 117/200, Loss: 5260.9873
Epoch 118/200, Loss: 5261.5379
Epoch 119/200, Loss: 5261.3654
Epoch 120/200, Loss: 5261.5656
Epoch 121/200, Loss: 5261.2229
Epoch 122/200, Loss: 5261.4595
Epoch 123/200, Loss: 5261.2886
Epoch 124/200, Loss: 5261.2560
Epoch 125/200, Loss: 5261.2062
Epoch 126/200, Loss: 5261.0417
Epoch 127/200, Loss: 5261.1422
Epoch 128/200, Loss: 5261.1367
Epoch 129/200, Loss: 5261.0187
Epoch 130/200, Loss: 5260.9826
Epoch 131/200, Loss: 5261.1056
Epoch 132/200, Loss: 5261.0971
Epoch 133/200, Loss: 5261.2585
Epoch 134/200, Loss: 5261.0399
Epoch 135/200, Loss: 5261.5126
Epoch 136/200, Loss: 5261.4656
Epoch 137/200, Loss: 5261.3155
Epoch 138/200, Loss: 5261.0942
Epoch 139/200, Loss: 5261.0508
Epoch 140/200, Loss: 5261.2229
Epoch 141/200, Loss: 5261.1862
Epoch 142/200, Loss: 5261.2205
Epoch 143/200, Loss: 5261.1974
Epoch 144/200, Loss: 5261.3177
Epoch 145/200, Loss: 5261.0490
Epoch 146/200, Loss: 5261.1428
Epoch 147/200, Loss: 5260.8571
Epoch 148/200, Loss: 5261.3592
Epoch 149/200, Loss: 5261.2677
Epoch 150/200, Loss: 5260.8332
Epoch 151/200, Loss: 5260.8990
Epoch 152/200, Loss: 5261.0547
Epoch 153/200, Loss: 5261.2279
Epoch 154/200, Loss: 5261.2251
Epoch 155/200, Loss: 5261.2481
Epoch 156/200, Loss: 5260.9291
Epoch 157/200, Loss: 5261.6177
Epoch 158/200, Loss: 5261.0830
Epoch 159/200, Loss: 5260.8350
Epoch 160/200, Loss: 5260.9378
Epoch 161/200, Loss: 5261.3226
Epoch 162/200, Loss: 5260.9866
Epoch 163/200, Loss: 5261.2378
Epoch 164/200, Loss: 5260.9027
Epoch 165/200, Loss: 5261.4810
Epoch 166/200, Loss: 5261.3157
Epoch 167/200, Loss: 5260.9311
Epoch 168/200, Loss: 5261.2598
Epoch 169/200, Loss: 5261.1243
Epoch 170/200, Loss: 5261.3972
Epoch 171/200, Loss: 5261.4900
Epoch 172/200, Loss: 5261.1906
Epoch 173/200, Loss: 5260.9693
Epoch 174/200, Loss: 5261.2560
Epoch 175/200, Loss: 5261.1735
Epoch 176/200, Loss: 5261.4343
Epoch 177/200, Loss: 5261.3930
Epoch 178/200, Loss: 5261.1773
Epoch 179/200, Loss: 5261.1097
Epoch 180/200, Loss: 5261.0346
Epoch 181/200, Loss: 5261.1634
Epoch 182/200, Loss: 5261.1725
Epoch 183/200, Loss: 5261.0493
Epoch 184/200, Loss: 5261.0539
Epoch 185/200, Loss: 5261.2838
Epoch 186/200, Loss: 5261.4149
Epoch 187/200, Loss: 5261.3387
Epoch 188/200, Loss: 5260.9325
Epoch 189/200, Loss: 5260.9829
Epoch 190/200, Loss: 5261.4768
Epoch 191/200, Loss: 5261.0160
Epoch 192/200, Loss: 5261.2130
Epoch 193/200, Loss: 5260.9137
Epoch 194/200, Loss: 5261.0872
Epoch 195/200, Loss: 5261.1671
Epoch 196/200, Loss: 5261.4178
Epoch 197/200, Loss: 5260.9198
Epoch 198/200, Loss: 5261.0586
Epoch 199/200, Loss: 5261.0896
Epoch 200/200, Loss: 5261.1835
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 17.19% | Test Accuracy: 16.85%
Precision: 0.1351 | Recall: 0.1685 | F1-Score: 0.1109

Processing Subject 5...
Top 32 discriminative features: [26 38 46 44 14 12 41 29 18 16 42 40 22 30 25 37 63 61 45 33 21 59 57 34
 28 49 51  9 32  2  0 20]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 115718
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3110
		Class 2: 3258
		Class 3: 3600
		Class 4: 3529
		Class 5: 3488
		Class 6: 3600
		Class 7: 3471
		Class 8: 3139
		Class 9: 3600
		Class 10: 3529
		Class 11: 1712
		Class 12: 1788
		Class 13: 2332
		Class 14: 2914
		Class 15: 2532
		Class 16: 1719
		Class 17: 1713
		Class 18: 2148
		Class 19: 2823
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2636
		Class 25: 3600
		Class 26: 2347
		Class 27: 3182
		Class 28: 2673
		Class 29: 3025
		Class 30: 3298
		Class 31: 3122
		Class 32: 2490
		Class 33: 2096
		Class 34: 3600
		Class 35: 3005
		Class 36: 3023
		Class 37: 3342
		Class 38: 2494
	# of Testing Samples: 13857
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 365
		Class 2: 400
		Class 3: 400
		Class 4: 400
		Class 5: 400
		Class 6: 400
		Class 7: 400
		Class 8: 400
		Class 9: 400
		Class 10: 400
		Class 11: 219
		Class 12: 194
		Class 13: 239
		Class 14: 380
		Class 15: 316
		Class 16: 252
		Class 17: 224
		Class 18: 241
		Class 19: 370
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 300
		Class 25: 400
		Class 26: 243
		Class 27: 336
		Class 28: 400
		Class 29: 366
		Class 30: 400
		Class 31: 400
		Class 32: 400
		Class 33: 331
		Class 34: 400
		Class 35: 400
		Class 36: 400
		Class 37: 400
		Class 38: 281
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 62.43%
Precision: 0.6362 | Recall: 0.6243 | F1-Score: 0.5958


Running Deep Learning Classifier...
DataLoader: Training set - 1809 batches, Testing set - 217 batches
Epoch 1/200, Loss: 3782.7094
Epoch 2/200, Loss: 2366.9039
Epoch 3/200, Loss: 1928.7452
Epoch 4/200, Loss: 1668.7604
Epoch 5/200, Loss: 1489.3569
Epoch 6/200, Loss: 1365.8957
Epoch 7/200, Loss: 1347.5836
Epoch 8/200, Loss: 1334.0847
Epoch 9/200, Loss: 1320.5284
Epoch 10/200, Loss: 1307.0263
Epoch 11/200, Loss: 1295.6579
Epoch 12/200, Loss: 1294.1209
Epoch 13/200, Loss: 1292.2426
Epoch 14/200, Loss: 1291.7947
Epoch 15/200, Loss: 1289.9157
Epoch 16/200, Loss: 1288.5057
Epoch 17/200, Loss: 1287.9712
Epoch 18/200, Loss: 1288.0762
Epoch 19/200, Loss: 1287.5790
Epoch 20/200, Loss: 1289.0028
Epoch 21/200, Loss: 1287.8515
Epoch 22/200, Loss: 1287.4068
Epoch 23/200, Loss: 1287.4695
Epoch 24/200, Loss: 1287.4748
Epoch 25/200, Loss: 1288.5758
Epoch 26/200, Loss: 1287.6581
Epoch 27/200, Loss: 1287.3885
Epoch 28/200, Loss: 1287.5044
Epoch 29/200, Loss: 1287.1926
Epoch 30/200, Loss: 1287.4838
Epoch 31/200, Loss: 1287.1578
Epoch 32/200, Loss: 1287.1783
Epoch 33/200, Loss: 1287.9290
Epoch 34/200, Loss: 1288.0879
Epoch 35/200, Loss: 1287.6542
Epoch 36/200, Loss: 1288.6732
Epoch 37/200, Loss: 1287.6496
Epoch 38/200, Loss: 1287.6330
Epoch 39/200, Loss: 1287.7348
Epoch 40/200, Loss: 1287.5119
Epoch 41/200, Loss: 1287.6845
Epoch 42/200, Loss: 1287.6942
Epoch 43/200, Loss: 1287.3382
Epoch 44/200, Loss: 1287.7717
Epoch 45/200, Loss: 1287.8493
Epoch 46/200, Loss: 1287.7860
Epoch 47/200, Loss: 1287.2316
Epoch 48/200, Loss: 1287.5640
Epoch 49/200, Loss: 1287.3643
Epoch 50/200, Loss: 1288.0306
Epoch 51/200, Loss: 1287.4967
Epoch 52/200, Loss: 1287.4066
Epoch 53/200, Loss: 1287.2071
Epoch 54/200, Loss: 1287.6046
Epoch 55/200, Loss: 1288.0631
Epoch 56/200, Loss: 1288.1025
Epoch 57/200, Loss: 1288.0180
Epoch 58/200, Loss: 1287.3866
Epoch 59/200, Loss: 1287.2391
Epoch 60/200, Loss: 1287.3638
Epoch 61/200, Loss: 1287.1468
Epoch 62/200, Loss: 1287.6079
Epoch 63/200, Loss: 1287.3759
Epoch 64/200, Loss: 1287.2032
Epoch 65/200, Loss: 1287.4136
Epoch 66/200, Loss: 1287.2892
Epoch 67/200, Loss: 1287.3836
Epoch 68/200, Loss: 1287.8355
Epoch 69/200, Loss: 1287.6753
Epoch 70/200, Loss: 1287.2283
Epoch 71/200, Loss: 1287.8212
Epoch 72/200, Loss: 1287.2911
Epoch 73/200, Loss: 1287.8346
Epoch 74/200, Loss: 1287.8897
Epoch 75/200, Loss: 1287.8104
Epoch 76/200, Loss: 1288.0849
Epoch 77/200, Loss: 1287.5439
Epoch 78/200, Loss: 1287.2944
Epoch 79/200, Loss: 1287.3941
Epoch 80/200, Loss: 1287.8530
Epoch 81/200, Loss: 1287.3128
Epoch 82/200, Loss: 1287.5776
Epoch 83/200, Loss: 1287.4634
Epoch 84/200, Loss: 1287.2894
Epoch 85/200, Loss: 1287.4507
Epoch 86/200, Loss: 1287.3569
Epoch 87/200, Loss: 1288.2403
Epoch 88/200, Loss: 1287.3826
Epoch 89/200, Loss: 1287.4321
Epoch 90/200, Loss: 1287.3564
Epoch 91/200, Loss: 1287.5289
Epoch 92/200, Loss: 1287.2322
Epoch 93/200, Loss: 1288.4737
Epoch 94/200, Loss: 1287.2968
Epoch 95/200, Loss: 1287.6417
Epoch 96/200, Loss: 1288.2124
Epoch 97/200, Loss: 1287.0854
Epoch 98/200, Loss: 1287.6108
Epoch 99/200, Loss: 1287.6015
Epoch 100/200, Loss: 1287.9590
Epoch 101/200, Loss: 1287.4906
Epoch 102/200, Loss: 1287.1537
Epoch 103/200, Loss: 1287.7060
Epoch 104/200, Loss: 1287.4358
Epoch 105/200, Loss: 1287.3155
Epoch 106/200, Loss: 1287.2958
Epoch 107/200, Loss: 1287.5299
Epoch 108/200, Loss: 1287.4847
Epoch 109/200, Loss: 1287.8534
Epoch 110/200, Loss: 1287.7835
Epoch 111/200, Loss: 1288.0615
Epoch 112/200, Loss: 1287.9661
Epoch 113/200, Loss: 1287.5950
Epoch 114/200, Loss: 1287.5346
Epoch 115/200, Loss: 1287.3792
Epoch 116/200, Loss: 1287.6312
Epoch 117/200, Loss: 1288.0694
Epoch 118/200, Loss: 1287.3325
Epoch 119/200, Loss: 1288.7630
Epoch 120/200, Loss: 1287.2456
Epoch 121/200, Loss: 1287.7483
Epoch 122/200, Loss: 1287.3657
Epoch 123/200, Loss: 1287.8498
Epoch 124/200, Loss: 1287.4839
Epoch 125/200, Loss: 1287.6986
Epoch 126/200, Loss: 1287.6884
Epoch 127/200, Loss: 1287.6106
Epoch 128/200, Loss: 1287.4311
Epoch 129/200, Loss: 1287.3134
Epoch 130/200, Loss: 1287.3721
Epoch 131/200, Loss: 1288.3324
Epoch 132/200, Loss: 1288.0891
Epoch 133/200, Loss: 1287.3928
Epoch 134/200, Loss: 1287.4033
Epoch 135/200, Loss: 1288.1060
Epoch 136/200, Loss: 1287.5731
Epoch 137/200, Loss: 1287.3731
Epoch 138/200, Loss: 1287.3562
Epoch 139/200, Loss: 1287.4859
Epoch 140/200, Loss: 1288.6900
Epoch 141/200, Loss: 1287.5067
Epoch 142/200, Loss: 1287.1269
Epoch 143/200, Loss: 1287.0576
Epoch 144/200, Loss: 1287.9054
Epoch 145/200, Loss: 1287.3930
Epoch 146/200, Loss: 1287.5490
Epoch 147/200, Loss: 1287.1753
Epoch 148/200, Loss: 1288.3457
Epoch 149/200, Loss: 1287.7560
Epoch 150/200, Loss: 1287.2857
Epoch 151/200, Loss: 1287.1889
Epoch 152/200, Loss: 1287.6490
Epoch 153/200, Loss: 1287.3962
Epoch 154/200, Loss: 1287.3211
Epoch 155/200, Loss: 1287.6725
Epoch 156/200, Loss: 1287.4150
Epoch 157/200, Loss: 1287.5439
Epoch 158/200, Loss: 1287.3058
Epoch 159/200, Loss: 1287.2301
Epoch 160/200, Loss: 1287.2312
Epoch 161/200, Loss: 1287.5782
Epoch 162/200, Loss: 1287.4933
Epoch 163/200, Loss: 1287.3661
Epoch 164/200, Loss: 1287.7852
Epoch 165/200, Loss: 1287.6332
Epoch 166/200, Loss: 1287.5385
Epoch 167/200, Loss: 1287.9115
Epoch 168/200, Loss: 1287.5614
Epoch 169/200, Loss: 1288.6433
Epoch 170/200, Loss: 1287.6833
Epoch 171/200, Loss: 1287.5314
Epoch 172/200, Loss: 1287.9546
Epoch 173/200, Loss: 1287.8316
Epoch 174/200, Loss: 1287.4448
Epoch 175/200, Loss: 1287.5292
Epoch 176/200, Loss: 1288.7489
Epoch 177/200, Loss: 1287.7755
Epoch 178/200, Loss: 1287.4929
Epoch 179/200, Loss: 1287.3571
Epoch 180/200, Loss: 1287.2727
Epoch 181/200, Loss: 1287.6326
Epoch 182/200, Loss: 1288.1562
Epoch 183/200, Loss: 1287.2698
Epoch 184/200, Loss: 1287.1347
Epoch 185/200, Loss: 1287.7650
Epoch 186/200, Loss: 1288.1924
Epoch 187/200, Loss: 1287.8133
Epoch 188/200, Loss: 1288.0394
Epoch 189/200, Loss: 1287.3854
Epoch 190/200, Loss: 1287.8567
Epoch 191/200, Loss: 1287.5254
Epoch 192/200, Loss: 1288.0664
Epoch 193/200, Loss: 1288.4176
Epoch 194/200, Loss: 1287.6811
Epoch 195/200, Loss: 1287.9837
Epoch 196/200, Loss: 1287.5514
Epoch 197/200, Loss: 1288.3486
Epoch 198/200, Loss: 1288.1441
Epoch 199/200, Loss: 1287.7693
Epoch 200/200, Loss: 1287.9587
Train Accuracy: 77.52% | Test Accuracy: 53.93%
Precision: 0.5712 | Recall: 0.5393 | F1-Score: 0.5229
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 17.18%
Precision: 0.1721 | Recall: 0.1718 | F1-Score: 0.1699


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1809 batches, Testing set - 217 batches
Epoch 1/200, Loss: 6329.1634
Epoch 2/200, Loss: 5635.0895
Epoch 3/200, Loss: 5427.8601
Epoch 4/200, Loss: 5236.7698
Epoch 5/200, Loss: 5061.8681
Epoch 6/200, Loss: 4921.1832
Epoch 7/200, Loss: 4904.6038
Epoch 8/200, Loss: 4889.6044
Epoch 9/200, Loss: 4875.2118
Epoch 10/200, Loss: 4861.9802
Epoch 11/200, Loss: 4847.8188
Epoch 12/200, Loss: 4845.7529
Epoch 13/200, Loss: 4845.5183
Epoch 14/200, Loss: 4843.7690
Epoch 15/200, Loss: 4842.6061
Epoch 16/200, Loss: 4840.7170
Epoch 17/200, Loss: 4840.9397
Epoch 18/200, Loss: 4840.7484
Epoch 19/200, Loss: 4840.7958
Epoch 20/200, Loss: 4840.4215
Epoch 21/200, Loss: 4839.9133
Epoch 22/200, Loss: 4840.5629
Epoch 23/200, Loss: 4839.8370
Epoch 24/200, Loss: 4839.7947
Epoch 25/200, Loss: 4839.8873
Epoch 26/200, Loss: 4840.1733
Epoch 27/200, Loss: 4840.0739
Epoch 28/200, Loss: 4840.0343
Epoch 29/200, Loss: 4839.7145
Epoch 30/200, Loss: 4840.0447
Epoch 31/200, Loss: 4840.3151
Epoch 32/200, Loss: 4840.3476
Epoch 33/200, Loss: 4840.4506
Epoch 34/200, Loss: 4839.8427
Epoch 35/200, Loss: 4840.4239
Epoch 36/200, Loss: 4839.9473
Epoch 37/200, Loss: 4840.4238
Epoch 38/200, Loss: 4840.2336
Epoch 39/200, Loss: 4840.3133
Epoch 40/200, Loss: 4840.0166
Epoch 41/200, Loss: 4840.4359
Epoch 42/200, Loss: 4840.0093
Epoch 43/200, Loss: 4840.4047
Epoch 44/200, Loss: 4840.6466
Epoch 45/200, Loss: 4839.9011
Epoch 46/200, Loss: 4839.3644
Epoch 47/200, Loss: 4839.7919
Epoch 48/200, Loss: 4840.1173
Epoch 49/200, Loss: 4839.6545
Epoch 50/200, Loss: 4839.8214
Epoch 51/200, Loss: 4839.6820
Epoch 52/200, Loss: 4840.2997
Epoch 53/200, Loss: 4839.9519
Epoch 54/200, Loss: 4840.0602
Epoch 55/200, Loss: 4839.9967
Epoch 56/200, Loss: 4840.2652
Epoch 57/200, Loss: 4840.1492
Epoch 58/200, Loss: 4840.0931
Epoch 59/200, Loss: 4840.0461
Epoch 60/200, Loss: 4840.2260
Epoch 61/200, Loss: 4839.7086
Epoch 62/200, Loss: 4840.1611
Epoch 63/200, Loss: 4839.8976
Epoch 64/200, Loss: 4840.3015
Epoch 65/200, Loss: 4840.2680
Epoch 66/200, Loss: 4840.3494
Epoch 67/200, Loss: 4840.1184
Epoch 68/200, Loss: 4840.0051
Epoch 69/200, Loss: 4840.3820
Epoch 70/200, Loss: 4840.1500
Epoch 71/200, Loss: 4840.4238
Epoch 72/200, Loss: 4840.0717
Epoch 73/200, Loss: 4839.8025
Epoch 74/200, Loss: 4841.1961
Epoch 75/200, Loss: 4840.2847
Epoch 76/200, Loss: 4839.9224
Epoch 77/200, Loss: 4840.5252
Epoch 78/200, Loss: 4840.0807
Epoch 79/200, Loss: 4840.3122
Epoch 80/200, Loss: 4840.0767
Epoch 81/200, Loss: 4840.1417
Epoch 82/200, Loss: 4839.6197
Epoch 83/200, Loss: 4840.4456
Epoch 84/200, Loss: 4839.5555
Epoch 85/200, Loss: 4840.4683
Epoch 86/200, Loss: 4840.7247
Epoch 87/200, Loss: 4839.9465
Epoch 88/200, Loss: 4840.1449
Epoch 89/200, Loss: 4840.2695
Epoch 90/200, Loss: 4840.0892
Epoch 91/200, Loss: 4841.3914
Epoch 92/200, Loss: 4840.5626
Epoch 93/200, Loss: 4840.3781
Epoch 94/200, Loss: 4840.0088
Epoch 95/200, Loss: 4840.0738
Epoch 96/200, Loss: 4840.5542
Epoch 97/200, Loss: 4840.4206
Epoch 98/200, Loss: 4840.2511
Epoch 99/200, Loss: 4840.1051
Epoch 100/200, Loss: 4839.8419
Epoch 101/200, Loss: 4839.4819
Epoch 102/200, Loss: 4839.7190
Epoch 103/200, Loss: 4839.9989
Epoch 104/200, Loss: 4839.6535
Epoch 105/200, Loss: 4839.7742
Epoch 106/200, Loss: 4840.2068
Epoch 107/200, Loss: 4839.9615
Epoch 108/200, Loss: 4839.9430
Epoch 109/200, Loss: 4840.0070
Epoch 110/200, Loss: 4839.8975
Epoch 111/200, Loss: 4840.2430
Epoch 112/200, Loss: 4839.5460
Epoch 113/200, Loss: 4840.6459
Epoch 114/200, Loss: 4840.3435
Epoch 115/200, Loss: 4839.9700
Epoch 116/200, Loss: 4839.8732
Epoch 117/200, Loss: 4839.9139
Epoch 118/200, Loss: 4840.1344
Epoch 119/200, Loss: 4840.2523
Epoch 120/200, Loss: 4839.9518
Epoch 121/200, Loss: 4839.9109
Epoch 122/200, Loss: 4840.0796
Epoch 123/200, Loss: 4839.8621
Epoch 124/200, Loss: 4840.0517
Epoch 125/200, Loss: 4840.0798
Epoch 126/200, Loss: 4840.0855
Epoch 127/200, Loss: 4839.8314
Epoch 128/200, Loss: 4840.4408
Epoch 129/200, Loss: 4839.9260
Epoch 130/200, Loss: 4840.2161
Epoch 131/200, Loss: 4840.5409
Epoch 132/200, Loss: 4840.0514
Epoch 133/200, Loss: 4839.7388
Epoch 134/200, Loss: 4840.0060
Epoch 135/200, Loss: 4839.9547
Epoch 136/200, Loss: 4840.1248
Epoch 137/200, Loss: 4840.3107
Epoch 138/200, Loss: 4840.8102
Epoch 139/200, Loss: 4839.9060
Epoch 140/200, Loss: 4839.6713
Epoch 141/200, Loss: 4839.9819
Epoch 142/200, Loss: 4840.3409
Epoch 143/200, Loss: 4839.6721
Epoch 144/200, Loss: 4840.5708
Epoch 145/200, Loss: 4839.9124
Epoch 146/200, Loss: 4839.9636
Epoch 147/200, Loss: 4839.9371
Epoch 148/200, Loss: 4840.4363
Epoch 149/200, Loss: 4840.2040
Epoch 150/200, Loss: 4840.1072
Epoch 151/200, Loss: 4840.3394
Epoch 152/200, Loss: 4840.1809
Epoch 153/200, Loss: 4840.4968
Epoch 154/200, Loss: 4840.1419
Epoch 155/200, Loss: 4840.1660
Epoch 156/200, Loss: 4840.0019
Epoch 157/200, Loss: 4840.3392
Epoch 158/200, Loss: 4840.2366
Epoch 159/200, Loss: 4839.9468
Epoch 160/200, Loss: 4840.2859
Epoch 161/200, Loss: 4840.0557
Epoch 162/200, Loss: 4840.1372
Epoch 163/200, Loss: 4840.3523
Epoch 164/200, Loss: 4839.9975
Epoch 165/200, Loss: 4840.2240
Epoch 166/200, Loss: 4839.8874
Epoch 167/200, Loss: 4839.9208
Epoch 168/200, Loss: 4840.2586
Epoch 169/200, Loss: 4840.2864
Epoch 170/200, Loss: 4840.4414
Epoch 171/200, Loss: 4840.4752
Epoch 172/200, Loss: 4840.2019
Epoch 173/200, Loss: 4840.3702
Epoch 174/200, Loss: 4840.1389
Epoch 175/200, Loss: 4840.1446
Epoch 176/200, Loss: 4839.9805
Epoch 177/200, Loss: 4839.8207
Epoch 178/200, Loss: 4840.3666
Epoch 179/200, Loss: 4840.4844
Epoch 180/200, Loss: 4839.8846
Epoch 181/200, Loss: 4839.9350
Epoch 182/200, Loss: 4840.2616
Epoch 183/200, Loss: 4839.9108
Epoch 184/200, Loss: 4839.6575
Epoch 185/200, Loss: 4840.0344
Epoch 186/200, Loss: 4840.2388
Epoch 187/200, Loss: 4840.2974
Epoch 188/200, Loss: 4840.2673
Epoch 189/200, Loss: 4839.9997
Epoch 190/200, Loss: 4840.7596
Epoch 191/200, Loss: 4840.1313
Epoch 192/200, Loss: 4840.2575
Epoch 193/200, Loss: 4839.8516
Epoch 194/200, Loss: 4840.5538
Epoch 195/200, Loss: 4839.9872
Epoch 196/200, Loss: 4839.8488
Epoch 197/200, Loss: 4840.0193
Epoch 198/200, Loss: 4840.0233
Epoch 199/200, Loss: 4840.4862
Epoch 200/200, Loss: 4839.8521
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 20.13% | Test Accuracy: 19.79%
Precision: 0.1414 | Recall: 0.1979 | F1-Score: 0.1371

Processing Subject 6...
Top 32 discriminative features: [26 38 46 44 14 12 41 29 25 37 18 16 42 40 30 22 45 63 61 28 34 33 21  9
 32 59 57  2  0 49 51 20]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 115812
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3075
		Class 2: 3258
		Class 3: 3600
		Class 4: 3529
		Class 5: 3488
		Class 6: 3600
		Class 7: 3522
		Class 8: 3139
		Class 9: 3600
		Class 10: 3529
		Class 11: 1723
		Class 12: 1742
		Class 13: 2226
		Class 14: 2927
		Class 15: 2530
		Class 16: 1787
		Class 17: 1694
		Class 18: 2101
		Class 19: 2829
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2607
		Class 25: 3600
		Class 26: 2288
		Class 27: 3157
		Class 28: 2704
		Class 29: 3037
		Class 30: 3298
		Class 31: 3122
		Class 32: 2590
		Class 33: 2200
		Class 34: 3600
		Class 35: 3005
		Class 36: 3023
		Class 37: 3342
		Class 38: 2560
	# of Testing Samples: 13763
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 400
		Class 2: 400
		Class 3: 400
		Class 4: 400
		Class 5: 400
		Class 6: 400
		Class 7: 349
		Class 8: 400
		Class 9: 400
		Class 10: 400
		Class 11: 208
		Class 12: 240
		Class 13: 345
		Class 14: 367
		Class 15: 318
		Class 16: 184
		Class 17: 243
		Class 18: 288
		Class 19: 364
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 329
		Class 25: 400
		Class 26: 302
		Class 27: 361
		Class 28: 369
		Class 29: 354
		Class 30: 400
		Class 31: 400
		Class 32: 300
		Class 33: 227
		Class 34: 400
		Class 35: 400
		Class 36: 400
		Class 37: 400
		Class 38: 215
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 64.65%
Precision: 0.6588 | Recall: 0.6465 | F1-Score: 0.6169


Running Deep Learning Classifier...
DataLoader: Training set - 1810 batches, Testing set - 216 batches
Epoch 1/200, Loss: 4017.8401
Epoch 2/200, Loss: 2369.5202
Epoch 3/200, Loss: 1918.5746
Epoch 4/200, Loss: 1655.8782
Epoch 5/200, Loss: 1476.4063
Epoch 6/200, Loss: 1353.6108
Epoch 7/200, Loss: 1335.3498
Epoch 8/200, Loss: 1320.8914
Epoch 9/200, Loss: 1307.8680
Epoch 10/200, Loss: 1294.6794
Epoch 11/200, Loss: 1282.6591
Epoch 12/200, Loss: 1280.7881
Epoch 13/200, Loss: 1279.3191
Epoch 14/200, Loss: 1278.1370
Epoch 15/200, Loss: 1276.7367
Epoch 16/200, Loss: 1275.3830
Epoch 17/200, Loss: 1275.1969
Epoch 18/200, Loss: 1275.1991
Epoch 19/200, Loss: 1274.9885
Epoch 20/200, Loss: 1274.9452
Epoch 21/200, Loss: 1274.5831
Epoch 22/200, Loss: 1274.5572
Epoch 23/200, Loss: 1274.5043
Epoch 24/200, Loss: 1274.5310
Epoch 25/200, Loss: 1274.8224
Epoch 26/200, Loss: 1274.6060
Epoch 27/200, Loss: 1274.5275
Epoch 28/200, Loss: 1274.5306
Epoch 29/200, Loss: 1274.5182
Epoch 30/200, Loss: 1274.5829
Epoch 31/200, Loss: 1274.6196
Epoch 32/200, Loss: 1274.6647
Epoch 33/200, Loss: 1274.6347
Epoch 34/200, Loss: 1274.6042
Epoch 35/200, Loss: 1274.5454
Epoch 36/200, Loss: 1274.6943
Epoch 37/200, Loss: 1274.4848
Epoch 38/200, Loss: 1274.6329
Epoch 39/200, Loss: 1274.5778
Epoch 40/200, Loss: 1274.5015
Epoch 41/200, Loss: 1274.5504
Epoch 42/200, Loss: 1274.5795
Epoch 43/200, Loss: 1274.5800
Epoch 44/200, Loss: 1274.5736
Epoch 45/200, Loss: 1274.6080
Epoch 46/200, Loss: 1274.6062
Epoch 47/200, Loss: 1274.5549
Epoch 48/200, Loss: 1274.6092
Epoch 49/200, Loss: 1274.5791
Epoch 50/200, Loss: 1274.6709
Epoch 51/200, Loss: 1274.5567
Epoch 52/200, Loss: 1274.6122
Epoch 53/200, Loss: 1274.5685
Epoch 54/200, Loss: 1274.4748
Epoch 55/200, Loss: 1274.5973
Epoch 56/200, Loss: 1274.5613
Epoch 57/200, Loss: 1274.5504
Epoch 58/200, Loss: 1274.6390
Epoch 59/200, Loss: 1274.6025
Epoch 60/200, Loss: 1274.5365
Epoch 61/200, Loss: 1274.7001
Epoch 62/200, Loss: 1274.5447
Epoch 63/200, Loss: 1274.5180
Epoch 64/200, Loss: 1274.6330
Epoch 65/200, Loss: 1274.6195
Epoch 66/200, Loss: 1274.6129
Epoch 67/200, Loss: 1274.7445
Epoch 68/200, Loss: 1274.5489
Epoch 69/200, Loss: 1274.5504
Epoch 70/200, Loss: 1274.5839
Epoch 71/200, Loss: 1274.6384
Epoch 72/200, Loss: 1274.5884
Epoch 73/200, Loss: 1274.6409
Epoch 74/200, Loss: 1274.7096
Epoch 75/200, Loss: 1274.6051
Epoch 76/200, Loss: 1274.6443
Epoch 77/200, Loss: 1274.5716
Epoch 78/200, Loss: 1274.6435
Epoch 79/200, Loss: 1274.5776
Epoch 80/200, Loss: 1274.6266
Epoch 81/200, Loss: 1274.6311
Epoch 82/200, Loss: 1274.6162
Epoch 83/200, Loss: 1274.5696
Epoch 84/200, Loss: 1274.6855
Epoch 85/200, Loss: 1274.6322
Epoch 86/200, Loss: 1274.6780
Epoch 87/200, Loss: 1274.5331
Epoch 88/200, Loss: 1274.8084
Epoch 89/200, Loss: 1274.6381
Epoch 90/200, Loss: 1274.5231
Epoch 91/200, Loss: 1274.6330
Epoch 92/200, Loss: 1274.6850
Epoch 93/200, Loss: 1274.5840
Epoch 94/200, Loss: 1274.6562
Epoch 95/200, Loss: 1274.4950
Epoch 96/200, Loss: 1274.5170
Epoch 97/200, Loss: 1274.5459
Epoch 98/200, Loss: 1274.5396
Epoch 99/200, Loss: 1274.6079
Epoch 100/200, Loss: 1274.5380
Epoch 101/200, Loss: 1274.5935
Epoch 102/200, Loss: 1274.7178
Epoch 103/200, Loss: 1274.6580
Epoch 104/200, Loss: 1274.5473
Epoch 105/200, Loss: 1274.7350
Epoch 106/200, Loss: 1274.5775
Epoch 107/200, Loss: 1274.5865
Epoch 108/200, Loss: 1274.5283
Epoch 109/200, Loss: 1274.6274
Epoch 110/200, Loss: 1274.5975
Epoch 111/200, Loss: 1274.5804
Epoch 112/200, Loss: 1274.5224
Epoch 113/200, Loss: 1274.6159
Epoch 114/200, Loss: 1274.6148
Epoch 115/200, Loss: 1274.5495
Epoch 116/200, Loss: 1274.4890
Epoch 117/200, Loss: 1274.6088
Epoch 118/200, Loss: 1274.5820
Epoch 119/200, Loss: 1274.7174
Epoch 120/200, Loss: 1274.6058
Epoch 121/200, Loss: 1274.7138
Epoch 122/200, Loss: 1274.6011
Epoch 123/200, Loss: 1274.6175
Epoch 124/200, Loss: 1274.5182
Epoch 125/200, Loss: 1274.4979
Epoch 126/200, Loss: 1274.6405
Epoch 127/200, Loss: 1274.5761
Epoch 128/200, Loss: 1274.6653
Epoch 129/200, Loss: 1274.6377
Epoch 130/200, Loss: 1274.6994
Epoch 131/200, Loss: 1274.6050
Epoch 132/200, Loss: 1274.5316
Epoch 133/200, Loss: 1274.6365
Epoch 134/200, Loss: 1274.6213
Epoch 135/200, Loss: 1274.5402
Epoch 136/200, Loss: 1274.6534
Epoch 137/200, Loss: 1274.6525
Epoch 138/200, Loss: 1274.6563
Epoch 139/200, Loss: 1274.5662
Epoch 140/200, Loss: 1274.6333
Epoch 141/200, Loss: 1274.5494
Epoch 142/200, Loss: 1274.5743
Epoch 143/200, Loss: 1274.7200
Epoch 144/200, Loss: 1274.6377
Epoch 145/200, Loss: 1274.7350
Epoch 146/200, Loss: 1274.5585
Epoch 147/200, Loss: 1274.5403
Epoch 148/200, Loss: 1274.6663
Epoch 149/200, Loss: 1274.5183
Epoch 150/200, Loss: 1274.5295
Epoch 151/200, Loss: 1274.5327
Epoch 152/200, Loss: 1274.5806
Epoch 153/200, Loss: 1274.6381
Epoch 154/200, Loss: 1274.5391
Epoch 155/200, Loss: 1274.6504
Epoch 156/200, Loss: 1274.5642
Epoch 157/200, Loss: 1274.5840
Epoch 158/200, Loss: 1274.6204
Epoch 159/200, Loss: 1274.6058
Epoch 160/200, Loss: 1274.5823
Epoch 161/200, Loss: 1274.5220
Epoch 162/200, Loss: 1274.5612
Epoch 163/200, Loss: 1274.5781
Epoch 164/200, Loss: 1274.4324
Epoch 165/200, Loss: 1274.5472
Epoch 166/200, Loss: 1274.6099
Epoch 167/200, Loss: 1274.5256
Epoch 168/200, Loss: 1274.7139
Epoch 169/200, Loss: 1274.6029
Epoch 170/200, Loss: 1274.6483
Epoch 171/200, Loss: 1274.4937
Epoch 172/200, Loss: 1274.4891
Epoch 173/200, Loss: 1274.5859
Epoch 174/200, Loss: 1274.6002
Epoch 175/200, Loss: 1274.5664
Epoch 176/200, Loss: 1274.5868
Epoch 177/200, Loss: 1274.5174
Epoch 178/200, Loss: 1274.7201
Epoch 179/200, Loss: 1274.7165
Epoch 180/200, Loss: 1274.6599
Epoch 181/200, Loss: 1274.6663
Epoch 182/200, Loss: 1274.5172
Epoch 183/200, Loss: 1274.6428
Epoch 184/200, Loss: 1274.5711
Epoch 185/200, Loss: 1274.5743
Epoch 186/200, Loss: 1274.5528
Epoch 187/200, Loss: 1274.6473
Epoch 188/200, Loss: 1274.5987
Epoch 189/200, Loss: 1274.5315
Epoch 190/200, Loss: 1274.6436
Epoch 191/200, Loss: 1274.6896
Epoch 192/200, Loss: 1274.8145
Epoch 193/200, Loss: 1274.5156
Epoch 194/200, Loss: 1274.5341
Epoch 195/200, Loss: 1274.5194
Epoch 196/200, Loss: 1274.6551
Epoch 197/200, Loss: 1274.6130
Epoch 198/200, Loss: 1274.5564
Epoch 199/200, Loss: 1274.6199
Epoch 200/200, Loss: 1274.5085
Train Accuracy: 77.18% | Test Accuracy: 55.75%
Precision: 0.5586 | Recall: 0.5575 | F1-Score: 0.5443
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 18.14%
Precision: 0.1909 | Recall: 0.1814 | F1-Score: 0.1811


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1810 batches, Testing set - 216 batches
Epoch 1/200, Loss: 5913.7219
Epoch 2/200, Loss: 5245.5856
Epoch 3/200, Loss: 4975.9013
Epoch 4/200, Loss: 4778.3868
Epoch 5/200, Loss: 4626.1604
Epoch 6/200, Loss: 4520.2025
Epoch 7/200, Loss: 4508.6843
Epoch 8/200, Loss: 4498.6212
Epoch 9/200, Loss: 4489.3783
Epoch 10/200, Loss: 4479.6276
Epoch 11/200, Loss: 4469.8464
Epoch 12/200, Loss: 4468.3155
Epoch 13/200, Loss: 4467.5371
Epoch 14/200, Loss: 4466.5641
Epoch 15/200, Loss: 4465.7496
Epoch 16/200, Loss: 4464.5456
Epoch 17/200, Loss: 4464.6005
Epoch 18/200, Loss: 4464.3750
Epoch 19/200, Loss: 4464.3181
Epoch 20/200, Loss: 4464.2654
Epoch 21/200, Loss: 4464.0450
Epoch 22/200, Loss: 4464.0037
Epoch 23/200, Loss: 4464.0137
Epoch 24/200, Loss: 4464.0107
Epoch 25/200, Loss: 4464.0359
Epoch 26/200, Loss: 4463.9962
Epoch 27/200, Loss: 4464.0429
Epoch 28/200, Loss: 4463.9764
Epoch 29/200, Loss: 4463.9065
Epoch 30/200, Loss: 4464.0270
Epoch 31/200, Loss: 4463.9471
Epoch 32/200, Loss: 4464.0174
Epoch 33/200, Loss: 4463.9336
Epoch 34/200, Loss: 4463.9323
Epoch 35/200, Loss: 4463.8810
Epoch 36/200, Loss: 4464.0750
Epoch 37/200, Loss: 4464.0774
Epoch 38/200, Loss: 4464.1615
Epoch 39/200, Loss: 4463.9894
Epoch 40/200, Loss: 4464.0637
Epoch 41/200, Loss: 4463.9904
Epoch 42/200, Loss: 4464.0250
Epoch 43/200, Loss: 4463.9954
Epoch 44/200, Loss: 4463.9890
Epoch 45/200, Loss: 4464.1351
Epoch 46/200, Loss: 4464.0583
Epoch 47/200, Loss: 4464.0474
Epoch 48/200, Loss: 4464.0290
Epoch 49/200, Loss: 4463.9865
Epoch 50/200, Loss: 4464.1313
Epoch 51/200, Loss: 4464.0147
Epoch 52/200, Loss: 4464.0257
Epoch 53/200, Loss: 4464.0470
Epoch 54/200, Loss: 4464.0335
Epoch 55/200, Loss: 4463.9446
Epoch 56/200, Loss: 4464.0752
Epoch 57/200, Loss: 4464.0258
Epoch 58/200, Loss: 4464.0002
Epoch 59/200, Loss: 4464.0216
Epoch 60/200, Loss: 4464.1245
Epoch 61/200, Loss: 4464.2029
Epoch 62/200, Loss: 4464.0586
Epoch 63/200, Loss: 4464.0914
Epoch 64/200, Loss: 4463.9702
Epoch 65/200, Loss: 4464.1127
Epoch 66/200, Loss: 4463.9428
Epoch 67/200, Loss: 4464.2344
Epoch 68/200, Loss: 4464.0492
Epoch 69/200, Loss: 4464.0469
Epoch 70/200, Loss: 4464.0527
Epoch 71/200, Loss: 4463.9917
Epoch 72/200, Loss: 4464.1158
Epoch 73/200, Loss: 4464.0191
Epoch 74/200, Loss: 4464.0413
Epoch 75/200, Loss: 4464.0782
Epoch 76/200, Loss: 4464.0504
Epoch 77/200, Loss: 4464.0919
Epoch 78/200, Loss: 4464.0147
Epoch 79/200, Loss: 4464.0351
Epoch 80/200, Loss: 4463.9636
Epoch 81/200, Loss: 4464.0428
Epoch 82/200, Loss: 4464.0462
Epoch 83/200, Loss: 4463.9825
Epoch 84/200, Loss: 4464.0049
Epoch 85/200, Loss: 4463.9150
Epoch 86/200, Loss: 4463.9856
Epoch 87/200, Loss: 4464.0337
Epoch 88/200, Loss: 4463.9352
Epoch 89/200, Loss: 4464.0901
Epoch 90/200, Loss: 4464.1376
Epoch 91/200, Loss: 4464.0519
Epoch 92/200, Loss: 4464.0520
Epoch 93/200, Loss: 4464.0977
Epoch 94/200, Loss: 4463.9208
Epoch 95/200, Loss: 4463.9589
Epoch 96/200, Loss: 4464.1693
Epoch 97/200, Loss: 4464.1123
Epoch 98/200, Loss: 4463.9872
Epoch 99/200, Loss: 4464.1113
Epoch 100/200, Loss: 4464.0328
Epoch 101/200, Loss: 4464.0409
Epoch 102/200, Loss: 4463.9608
Epoch 103/200, Loss: 4464.0618
Epoch 104/200, Loss: 4464.1904
Epoch 105/200, Loss: 4464.1021
Epoch 106/200, Loss: 4464.0908
Epoch 107/200, Loss: 4464.1488
Epoch 108/200, Loss: 4463.9995
Epoch 109/200, Loss: 4463.9538
Epoch 110/200, Loss: 4464.1171
Epoch 111/200, Loss: 4463.9833
Epoch 112/200, Loss: 4464.0034
Epoch 113/200, Loss: 4463.9862
Epoch 114/200, Loss: 4464.0282
Epoch 115/200, Loss: 4464.0146
Epoch 116/200, Loss: 4464.0181
Epoch 117/200, Loss: 4463.9501
Epoch 118/200, Loss: 4464.1716
Epoch 119/200, Loss: 4464.0298
Epoch 120/200, Loss: 4464.0030
Epoch 121/200, Loss: 4464.1211
Epoch 122/200, Loss: 4463.9194
Epoch 123/200, Loss: 4463.9736
Epoch 124/200, Loss: 4464.0479
Epoch 125/200, Loss: 4464.0437
Epoch 126/200, Loss: 4464.1203
Epoch 127/200, Loss: 4463.9206
Epoch 128/200, Loss: 4463.9291
Epoch 129/200, Loss: 4463.9476
Epoch 130/200, Loss: 4463.9867
Epoch 131/200, Loss: 4463.9402
Epoch 132/200, Loss: 4464.0323
Epoch 133/200, Loss: 4463.9900
Epoch 134/200, Loss: 4464.0430
Epoch 135/200, Loss: 4463.9689
Epoch 136/200, Loss: 4464.0343
Epoch 137/200, Loss: 4464.1289
Epoch 138/200, Loss: 4464.1020
Epoch 139/200, Loss: 4464.0368
Epoch 140/200, Loss: 4463.9873
Epoch 141/200, Loss: 4464.0245
Epoch 142/200, Loss: 4463.8266
Epoch 143/200, Loss: 4463.9552
Epoch 144/200, Loss: 4464.0038
Epoch 145/200, Loss: 4464.0478
Epoch 146/200, Loss: 4464.0236
Epoch 147/200, Loss: 4464.1033
Epoch 148/200, Loss: 4464.0042
Epoch 149/200, Loss: 4464.0221
Epoch 150/200, Loss: 4463.9231
Epoch 151/200, Loss: 4464.0490
Epoch 152/200, Loss: 4464.1216
Epoch 153/200, Loss: 4464.1978
Epoch 154/200, Loss: 4464.0556
Epoch 155/200, Loss: 4464.0334
Epoch 156/200, Loss: 4464.1476
Epoch 157/200, Loss: 4463.9588
Epoch 158/200, Loss: 4464.0534
Epoch 159/200, Loss: 4464.1118
Epoch 160/200, Loss: 4463.9609
Epoch 161/200, Loss: 4463.9533
Epoch 162/200, Loss: 4463.9564
Epoch 163/200, Loss: 4464.0062
Epoch 164/200, Loss: 4464.0358
Epoch 165/200, Loss: 4464.0486
Epoch 166/200, Loss: 4464.0711
Epoch 167/200, Loss: 4464.1257
Epoch 168/200, Loss: 4463.9886
Epoch 169/200, Loss: 4463.9664
Epoch 170/200, Loss: 4464.0518
Epoch 171/200, Loss: 4464.1199
Epoch 172/200, Loss: 4464.0186
Epoch 173/200, Loss: 4464.0576
Epoch 174/200, Loss: 4463.9827
Epoch 175/200, Loss: 4464.0064
Epoch 176/200, Loss: 4464.0142
Epoch 177/200, Loss: 4464.0461
Epoch 178/200, Loss: 4464.0306
Epoch 179/200, Loss: 4464.0480
Epoch 180/200, Loss: 4463.9727
Epoch 181/200, Loss: 4464.0228
Epoch 182/200, Loss: 4464.0230
Epoch 183/200, Loss: 4464.0818
Epoch 184/200, Loss: 4464.1015
Epoch 185/200, Loss: 4464.0756
Epoch 186/200, Loss: 4464.1014
Epoch 187/200, Loss: 4464.0233
Epoch 188/200, Loss: 4464.0660
Epoch 189/200, Loss: 4464.0802
Epoch 190/200, Loss: 4464.0447
Epoch 191/200, Loss: 4464.0855
Epoch 192/200, Loss: 4464.0323
Epoch 193/200, Loss: 4463.9712
Epoch 194/200, Loss: 4463.9759
Epoch 195/200, Loss: 4463.9314
Epoch 196/200, Loss: 4463.9958
Epoch 197/200, Loss: 4464.0299
Epoch 198/200, Loss: 4464.1184
Epoch 199/200, Loss: 4464.0181
Epoch 200/200, Loss: 4464.0310
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 23.79% | Test Accuracy: 22.63%
Precision: 0.2072 | Recall: 0.2263 | F1-Score: 0.1860

Processing Subject 7...
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:112: UserWarning: Features [48 52] are constant.
  warnings.warn("Features %s are constant." % constant_features_idx, UserWarning)
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide
  f = msb / msw
Top 32 discriminative features: [48 52 26 38 46 44 14 12 41 29 18 16 42 40 45 25 37 30 22 63 61 33 34 28
 21  9 59 57 32 49 51  2]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 115853
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3135
		Class 2: 3258
		Class 3: 3600
		Class 4: 3529
		Class 5: 3488
		Class 6: 3600
		Class 7: 3471
		Class 8: 3181
		Class 9: 3600
		Class 10: 3529
		Class 11: 1698
		Class 12: 1782
		Class 13: 2277
		Class 14: 2908
		Class 15: 2493
		Class 16: 1751
		Class 17: 1727
		Class 18: 2079
		Class 19: 2838
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2613
		Class 25: 3600
		Class 26: 2320
		Class 27: 3155
		Class 28: 2747
		Class 29: 3041
		Class 30: 3298
		Class 31: 3122
		Class 32: 2574
		Class 33: 2177
		Class 34: 3600
		Class 35: 3005
		Class 36: 3023
		Class 37: 3342
		Class 38: 2512
	# of Testing Samples: 13722
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 340
		Class 2: 400
		Class 3: 400
		Class 4: 400
		Class 5: 400
		Class 6: 400
		Class 7: 400
		Class 8: 358
		Class 9: 400
		Class 10: 400
		Class 11: 233
		Class 12: 200
		Class 13: 294
		Class 14: 386
		Class 15: 355
		Class 16: 220
		Class 17: 210
		Class 18: 310
		Class 19: 355
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 323
		Class 25: 400
		Class 26: 270
		Class 27: 363
		Class 28: 326
		Class 29: 350
		Class 30: 400
		Class 31: 400
		Class 32: 316
		Class 33: 250
		Class 34: 400
		Class 35: 400
		Class 36: 400
		Class 37: 400
		Class 38: 263
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 68.73%
Precision: 0.6831 | Recall: 0.6873 | F1-Score: 0.6698


Running Deep Learning Classifier...
DataLoader: Training set - 1811 batches, Testing set - 215 batches
Epoch 1/200, Loss: 4059.4044
Epoch 2/200, Loss: 2346.7351
Epoch 3/200, Loss: 1888.4883
Epoch 4/200, Loss: 1633.5784
Epoch 5/200, Loss: 1466.6170
Epoch 6/200, Loss: 1347.1660
Epoch 7/200, Loss: 1329.3032
Epoch 8/200, Loss: 1316.2322
Epoch 9/200, Loss: 1303.7633
Epoch 10/200, Loss: 1291.6346
Epoch 11/200, Loss: 1280.1631
Epoch 12/200, Loss: 1278.0684
Epoch 13/200, Loss: 1276.7574
Epoch 14/200, Loss: 1275.3862
Epoch 15/200, Loss: 1274.7703
Epoch 16/200, Loss: 1273.1409
Epoch 17/200, Loss: 1273.4880
Epoch 18/200, Loss: 1273.2471
Epoch 19/200, Loss: 1272.7481
Epoch 20/200, Loss: 1272.7472
Epoch 21/200, Loss: 1272.4562
Epoch 22/200, Loss: 1272.6478
Epoch 23/200, Loss: 1272.5272
Epoch 24/200, Loss: 1272.7810
Epoch 25/200, Loss: 1272.4978
Epoch 26/200, Loss: 1272.3677
Epoch 27/200, Loss: 1272.2773
Epoch 28/200, Loss: 1272.3919
Epoch 29/200, Loss: 1272.3516
Epoch 30/200, Loss: 1272.5325
Epoch 31/200, Loss: 1272.4769
Epoch 32/200, Loss: 1272.4335
Epoch 33/200, Loss: 1272.4316
Epoch 34/200, Loss: 1272.7346
Epoch 35/200, Loss: 1272.3124
Epoch 36/200, Loss: 1272.5354
Epoch 37/200, Loss: 1272.4222
Epoch 38/200, Loss: 1272.6438
Epoch 39/200, Loss: 1272.5181
Epoch 40/200, Loss: 1272.7510
Epoch 41/200, Loss: 1272.3477
Epoch 42/200, Loss: 1272.7575
Epoch 43/200, Loss: 1272.6665
Epoch 44/200, Loss: 1272.7698
Epoch 45/200, Loss: 1272.2201
Epoch 46/200, Loss: 1272.4528
Epoch 47/200, Loss: 1272.8437
Epoch 48/200, Loss: 1272.5137
Epoch 49/200, Loss: 1272.2337
Epoch 50/200, Loss: 1272.4312
Epoch 51/200, Loss: 1272.4183
Epoch 52/200, Loss: 1272.8311
Epoch 53/200, Loss: 1272.6859
Epoch 54/200, Loss: 1272.4161
Epoch 55/200, Loss: 1272.3852
Epoch 56/200, Loss: 1272.4075
Epoch 57/200, Loss: 1272.3155
Epoch 58/200, Loss: 1272.4230
Epoch 59/200, Loss: 1272.5972
Epoch 60/200, Loss: 1272.4729
Epoch 61/200, Loss: 1272.5042
Epoch 62/200, Loss: 1272.5781
Epoch 63/200, Loss: 1272.3441
Epoch 64/200, Loss: 1272.2800
Epoch 65/200, Loss: 1272.4230
Epoch 66/200, Loss: 1272.3153
Epoch 67/200, Loss: 1272.4002
Epoch 68/200, Loss: 1272.2672
Epoch 69/200, Loss: 1272.4731
Epoch 70/200, Loss: 1272.4287
Epoch 71/200, Loss: 1272.4570
Epoch 72/200, Loss: 1272.6859
Epoch 73/200, Loss: 1272.3696
Epoch 74/200, Loss: 1272.5437
Epoch 75/200, Loss: 1272.8941
Epoch 76/200, Loss: 1272.8143
Epoch 77/200, Loss: 1272.4368
Epoch 78/200, Loss: 1272.2613
Epoch 79/200, Loss: 1272.1775
Epoch 80/200, Loss: 1272.3760
Epoch 81/200, Loss: 1272.3287
Epoch 82/200, Loss: 1272.6123
Epoch 83/200, Loss: 1272.2979
Epoch 84/200, Loss: 1272.3254
Epoch 85/200, Loss: 1272.4590
Epoch 86/200, Loss: 1272.3770
Epoch 87/200, Loss: 1272.2363
Epoch 88/200, Loss: 1272.5556
Epoch 89/200, Loss: 1272.3655
Epoch 90/200, Loss: 1272.3725
Epoch 91/200, Loss: 1272.6302
Epoch 92/200, Loss: 1272.5497
Epoch 93/200, Loss: 1272.2695
Epoch 94/200, Loss: 1272.4343
Epoch 95/200, Loss: 1272.3992
Epoch 96/200, Loss: 1272.8169
Epoch 97/200, Loss: 1272.2340
Epoch 98/200, Loss: 1272.3301
Epoch 99/200, Loss: 1272.6739
Epoch 100/200, Loss: 1272.8993
Epoch 101/200, Loss: 1272.2758
Epoch 102/200, Loss: 1272.4158
Epoch 103/200, Loss: 1272.8469
Epoch 104/200, Loss: 1273.1345
Epoch 105/200, Loss: 1272.9150
Epoch 106/200, Loss: 1272.7667
Epoch 107/200, Loss: 1272.6680
Epoch 108/200, Loss: 1272.3880
Epoch 109/200, Loss: 1272.3128
Epoch 110/200, Loss: 1272.4805
Epoch 111/200, Loss: 1272.8470
Epoch 112/200, Loss: 1272.4419
Epoch 113/200, Loss: 1272.4844
Epoch 114/200, Loss: 1272.5762
Epoch 115/200, Loss: 1272.3162
Epoch 116/200, Loss: 1272.7739
Epoch 117/200, Loss: 1272.4438
Epoch 118/200, Loss: 1272.5089
Epoch 119/200, Loss: 1272.2485
Epoch 120/200, Loss: 1272.1812
Epoch 121/200, Loss: 1272.8181
Epoch 122/200, Loss: 1272.4225
Epoch 123/200, Loss: 1272.8483
Epoch 124/200, Loss: 1272.4509
Epoch 125/200, Loss: 1272.2616
Epoch 126/200, Loss: 1272.7631
Epoch 127/200, Loss: 1272.4214
Epoch 128/200, Loss: 1272.4626
Epoch 129/200, Loss: 1272.5819
Epoch 130/200, Loss: 1272.6633
Epoch 131/200, Loss: 1272.4013
Epoch 132/200, Loss: 1272.2815
Epoch 133/200, Loss: 1272.5379
Epoch 134/200, Loss: 1272.5645
Epoch 135/200, Loss: 1272.4879
Epoch 136/200, Loss: 1272.7378
Epoch 137/200, Loss: 1272.7036
Epoch 138/200, Loss: 1272.4881
Epoch 139/200, Loss: 1272.3833
Epoch 140/200, Loss: 1272.3523
Epoch 141/200, Loss: 1272.4491
Epoch 142/200, Loss: 1272.3048
Epoch 143/200, Loss: 1272.6773
Epoch 144/200, Loss: 1272.3352
Epoch 145/200, Loss: 1272.3021
Epoch 146/200, Loss: 1272.5949
Epoch 147/200, Loss: 1272.8178
Epoch 148/200, Loss: 1272.2264
Epoch 149/200, Loss: 1272.4588
Epoch 150/200, Loss: 1272.2493
Epoch 151/200, Loss: 1272.4277
Epoch 152/200, Loss: 1272.7931
Epoch 153/200, Loss: 1272.9667
Epoch 154/200, Loss: 1272.8049
Epoch 155/200, Loss: 1272.7905
Epoch 156/200, Loss: 1272.4325
Epoch 157/200, Loss: 1272.8674
Epoch 158/200, Loss: 1272.5461
Epoch 159/200, Loss: 1272.3398
Epoch 160/200, Loss: 1272.5412
Epoch 161/200, Loss: 1272.1887
Epoch 162/200, Loss: 1272.8929
Epoch 163/200, Loss: 1272.4442
Epoch 164/200, Loss: 1272.3078
Epoch 165/200, Loss: 1272.8363
Epoch 166/200, Loss: 1272.5107
Epoch 167/200, Loss: 1272.6701
Epoch 168/200, Loss: 1272.1301
Epoch 169/200, Loss: 1272.3721
Epoch 170/200, Loss: 1272.3755
Epoch 171/200, Loss: 1272.4796
Epoch 172/200, Loss: 1272.7046
Epoch 173/200, Loss: 1272.4119
Epoch 174/200, Loss: 1272.3228
Epoch 175/200, Loss: 1272.1535
Epoch 176/200, Loss: 1272.4212
Epoch 177/200, Loss: 1272.5279
Epoch 178/200, Loss: 1272.3442
Epoch 179/200, Loss: 1272.3705
Epoch 180/200, Loss: 1272.3611
Epoch 181/200, Loss: 1272.5652
Epoch 182/200, Loss: 1272.6149
Epoch 183/200, Loss: 1272.4106
Epoch 184/200, Loss: 1272.1882
Epoch 185/200, Loss: 1272.3027
Epoch 186/200, Loss: 1272.7962
Epoch 187/200, Loss: 1273.0238
Epoch 188/200, Loss: 1272.7024
Epoch 189/200, Loss: 1272.3738
Epoch 190/200, Loss: 1272.3498
Epoch 191/200, Loss: 1272.4646
Epoch 192/200, Loss: 1272.3090
Epoch 193/200, Loss: 1272.5993
Epoch 194/200, Loss: 1272.4687
Epoch 195/200, Loss: 1272.4802
Epoch 196/200, Loss: 1272.4832
Epoch 197/200, Loss: 1272.2505
Epoch 198/200, Loss: 1272.9009
Epoch 199/200, Loss: 1272.6007
Epoch 200/200, Loss: 1272.1503
Train Accuracy: 77.15% | Test Accuracy: 62.91%
Precision: 0.6281 | Recall: 0.6291 | F1-Score: 0.6119
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 16.08%
Precision: 0.1565 | Recall: 0.1608 | F1-Score: 0.1560


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1811 batches, Testing set - 215 batches
Epoch 1/200, Loss: 6268.0804
Epoch 2/200, Loss: 5543.5331
Epoch 3/200, Loss: 5316.6665
Epoch 4/200, Loss: 5142.4054
Epoch 5/200, Loss: 5012.4228
Epoch 6/200, Loss: 4901.6952
Epoch 7/200, Loss: 4887.5052
Epoch 8/200, Loss: 4875.1657
Epoch 9/200, Loss: 4863.6117
Epoch 10/200, Loss: 4852.5894
Epoch 11/200, Loss: 4840.9104
Epoch 12/200, Loss: 4838.6288
Epoch 13/200, Loss: 4837.5301
Epoch 14/200, Loss: 4836.2450
Epoch 15/200, Loss: 4835.7635
Epoch 16/200, Loss: 4834.0436
Epoch 17/200, Loss: 4834.0043
Epoch 18/200, Loss: 4833.4506
Epoch 19/200, Loss: 4833.4526
Epoch 20/200, Loss: 4833.8819
Epoch 21/200, Loss: 4833.5665
Epoch 22/200, Loss: 4833.4130
Epoch 23/200, Loss: 4833.3646
Epoch 24/200, Loss: 4833.0948
Epoch 25/200, Loss: 4833.8147
Epoch 26/200, Loss: 4833.2626
Epoch 27/200, Loss: 4833.7808
Epoch 28/200, Loss: 4833.5751
Epoch 29/200, Loss: 4833.4157
Epoch 30/200, Loss: 4833.7446
Epoch 31/200, Loss: 4833.4150
Epoch 32/200, Loss: 4833.3155
Epoch 33/200, Loss: 4833.4663
Epoch 34/200, Loss: 4833.4407
Epoch 35/200, Loss: 4833.1270
Epoch 36/200, Loss: 4833.3637
Epoch 37/200, Loss: 4833.5004
Epoch 38/200, Loss: 4833.5337
Epoch 39/200, Loss: 4833.3468
Epoch 40/200, Loss: 4833.4457
Epoch 41/200, Loss: 4833.6298
Epoch 42/200, Loss: 4833.3705
Epoch 43/200, Loss: 4833.2972
Epoch 44/200, Loss: 4833.4073
Epoch 45/200, Loss: 4833.5538
Epoch 46/200, Loss: 4833.5152
Epoch 47/200, Loss: 4833.3522
Epoch 48/200, Loss: 4833.7160
Epoch 49/200, Loss: 4833.5196
Epoch 50/200, Loss: 4833.2949
Epoch 51/200, Loss: 4833.2068
Epoch 52/200, Loss: 4833.3113
Epoch 53/200, Loss: 4833.3731
Epoch 54/200, Loss: 4833.0408
Epoch 55/200, Loss: 4833.3481
Epoch 56/200, Loss: 4833.5288
Epoch 57/200, Loss: 4833.3591
Epoch 58/200, Loss: 4833.3770
Epoch 59/200, Loss: 4833.4950
Epoch 60/200, Loss: 4833.0616
Epoch 61/200, Loss: 4833.2696
Epoch 62/200, Loss: 4833.4781
Epoch 63/200, Loss: 4833.6930
Epoch 64/200, Loss: 4833.3085
Epoch 65/200, Loss: 4833.4577
Epoch 66/200, Loss: 4833.4819
Epoch 67/200, Loss: 4833.5050
Epoch 68/200, Loss: 4833.4818
Epoch 69/200, Loss: 4833.3659
Epoch 70/200, Loss: 4833.5689
Epoch 71/200, Loss: 4833.3463
Epoch 72/200, Loss: 4833.3438
Epoch 73/200, Loss: 4833.1540
Epoch 74/200, Loss: 4833.3025
Epoch 75/200, Loss: 4833.3869
Epoch 76/200, Loss: 4833.3334
Epoch 77/200, Loss: 4833.1675
Epoch 78/200, Loss: 4833.1502
Epoch 79/200, Loss: 4833.2546
Epoch 80/200, Loss: 4833.3017
Epoch 81/200, Loss: 4833.2520
Epoch 82/200, Loss: 4833.5989
Epoch 83/200, Loss: 4833.4300
Epoch 84/200, Loss: 4833.3064
Epoch 85/200, Loss: 4833.3938
Epoch 86/200, Loss: 4833.4004
Epoch 87/200, Loss: 4833.1482
Epoch 88/200, Loss: 4833.2192
Epoch 89/200, Loss: 4833.3718
Epoch 90/200, Loss: 4833.4002
Epoch 91/200, Loss: 4833.3897
Epoch 92/200, Loss: 4833.2984
Epoch 93/200, Loss: 4833.5644
Epoch 94/200, Loss: 4833.2125
Epoch 95/200, Loss: 4833.4586
Epoch 96/200, Loss: 4833.0986
Epoch 97/200, Loss: 4833.3907
Epoch 98/200, Loss: 4833.5266
Epoch 99/200, Loss: 4833.5593
Epoch 100/200, Loss: 4833.0709
Epoch 101/200, Loss: 4833.4382
Epoch 102/200, Loss: 4833.5557
Epoch 103/200, Loss: 4833.5085
Epoch 104/200, Loss: 4833.4592
Epoch 105/200, Loss: 4833.0585
Epoch 106/200, Loss: 4833.2667
Epoch 107/200, Loss: 4833.5165
Epoch 108/200, Loss: 4833.1169
Epoch 109/200, Loss: 4833.2987
Epoch 110/200, Loss: 4833.4413
Epoch 111/200, Loss: 4833.4290
Epoch 112/200, Loss: 4833.2743
Epoch 113/200, Loss: 4833.3452
Epoch 114/200, Loss: 4833.5872
Epoch 115/200, Loss: 4833.3165
Epoch 116/200, Loss: 4833.2935
Epoch 117/200, Loss: 4833.1535
Epoch 118/200, Loss: 4833.2430
Epoch 119/200, Loss: 4833.0502
Epoch 120/200, Loss: 4833.6374
Epoch 121/200, Loss: 4833.1386
Epoch 122/200, Loss: 4833.6825
Epoch 123/200, Loss: 4833.1088
Epoch 124/200, Loss: 4833.3095
Epoch 125/200, Loss: 4833.4900
Epoch 126/200, Loss: 4833.4927
Epoch 127/200, Loss: 4833.4137
Epoch 128/200, Loss: 4833.1157
Epoch 129/200, Loss: 4833.4393
Epoch 130/200, Loss: 4833.4150
Epoch 131/200, Loss: 4833.4774
Epoch 132/200, Loss: 4833.5955
Epoch 133/200, Loss: 4833.4499
Epoch 134/200, Loss: 4833.5213
Epoch 135/200, Loss: 4833.5455
Epoch 136/200, Loss: 4833.1028
Epoch 137/200, Loss: 4833.5552
Epoch 138/200, Loss: 4833.7119
Epoch 139/200, Loss: 4833.2734
Epoch 140/200, Loss: 4833.6122
Epoch 141/200, Loss: 4833.3237
Epoch 142/200, Loss: 4833.5517
Epoch 143/200, Loss: 4833.6640
Epoch 144/200, Loss: 4832.9289
Epoch 145/200, Loss: 4833.4469
Epoch 146/200, Loss: 4833.7442
Epoch 147/200, Loss: 4833.4738
Epoch 148/200, Loss: 4833.2060
Epoch 149/200, Loss: 4833.3622
Epoch 150/200, Loss: 4833.1291
Epoch 151/200, Loss: 4833.6188
Epoch 152/200, Loss: 4833.3503
Epoch 153/200, Loss: 4833.5412
Epoch 154/200, Loss: 4833.2022
Epoch 155/200, Loss: 4833.3705
Epoch 156/200, Loss: 4833.2532
Epoch 157/200, Loss: 4833.5531
Epoch 158/200, Loss: 4833.2131
Epoch 159/200, Loss: 4833.3831
Epoch 160/200, Loss: 4833.4696
Epoch 161/200, Loss: 4833.3092
Epoch 162/200, Loss: 4833.5464
Epoch 163/200, Loss: 4833.5323
Epoch 164/200, Loss: 4833.6606
Epoch 165/200, Loss: 4833.1749
Epoch 166/200, Loss: 4833.2914
Epoch 167/200, Loss: 4833.5165
Epoch 168/200, Loss: 4833.5398
Epoch 169/200, Loss: 4833.3408
Epoch 170/200, Loss: 4833.4533
Epoch 171/200, Loss: 4833.1580
Epoch 172/200, Loss: 4833.7200
Epoch 173/200, Loss: 4833.3638
Epoch 174/200, Loss: 4833.6493
Epoch 175/200, Loss: 4833.8876
Epoch 176/200, Loss: 4833.4135
Epoch 177/200, Loss: 4833.3926
Epoch 178/200, Loss: 4833.1550
Epoch 179/200, Loss: 4833.7152
Epoch 180/200, Loss: 4833.2588
Epoch 181/200, Loss: 4833.5289
Epoch 182/200, Loss: 4833.2771
Epoch 183/200, Loss: 4833.8253
Epoch 184/200, Loss: 4833.2762
Epoch 185/200, Loss: 4833.3488
Epoch 186/200, Loss: 4833.2550
Epoch 187/200, Loss: 4833.1225
Epoch 188/200, Loss: 4833.3647
Epoch 189/200, Loss: 4833.7513
Epoch 190/200, Loss: 4833.4562
Epoch 191/200, Loss: 4833.2810
Epoch 192/200, Loss: 4833.8116
Epoch 193/200, Loss: 4833.3783
Epoch 194/200, Loss: 4833.3598
Epoch 195/200, Loss: 4833.4322
Epoch 196/200, Loss: 4833.2904
Epoch 197/200, Loss: 4833.5283
Epoch 198/200, Loss: 4833.9321
Epoch 199/200, Loss: 4833.3237
Epoch 200/200, Loss: 4833.5230
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 22.05% | Test Accuracy: 19.00%
Precision: 0.1211 | Recall: 0.1900 | F1-Score: 0.1281

Processing Subject 8...
Top 32 discriminative features: [26 38 46 44 14 12 41 29 18 16 42 40 25 37 22 45 30 63 61 33 34 21 59 28
  9 57 32 49 51  1  2  0]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 117979
	# per Class in Train Dataset:
		Class 0: 3536
		Class 1: 3127
		Class 2: 3331
		Class 3: 3600
		Class 4: 3534
		Class 5: 3499
		Class 6: 3600
		Class 7: 3485
		Class 8: 3211
		Class 9: 3600
		Class 10: 3529
		Class 11: 1796
		Class 12: 1772
		Class 13: 2364
		Class 14: 3021
		Class 15: 2599
		Class 16: 1806
		Class 17: 1767
		Class 18: 2178
		Class 19: 2917
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3583
		Class 24: 2680
		Class 25: 3600
		Class 26: 2429
		Class 27: 3281
		Class 28: 2828
		Class 29: 3131
		Class 30: 3353
		Class 31: 3257
		Class 32: 2649
		Class 33: 2248
		Class 34: 3600
		Class 35: 3131
		Class 36: 3195
		Class 37: 3435
		Class 38: 2537
	# of Testing Samples: 11596
	# per Class in Test Dataset:
		Class 0: 312
		Class 1: 348
		Class 2: 327
		Class 3: 400
		Class 4: 395
		Class 5: 389
		Class 6: 400
		Class 7: 386
		Class 8: 328
		Class 9: 400
		Class 10: 400
		Class 11: 135
		Class 12: 210
		Class 13: 207
		Class 14: 273
		Class 15: 249
		Class 16: 165
		Class 17: 170
		Class 18: 211
		Class 19: 276
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 379
		Class 24: 256
		Class 25: 400
		Class 26: 161
		Class 27: 237
		Class 28: 245
		Class 29: 260
		Class 30: 345
		Class 31: 265
		Class 32: 241
		Class 33: 179
		Class 34: 400
		Class 35: 274
		Class 36: 228
		Class 37: 307
		Class 38: 238
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 70.08%
Precision: 0.7441 | Recall: 0.7008 | F1-Score: 0.6945


Running Deep Learning Classifier...
DataLoader: Training set - 1844 batches, Testing set - 182 batches
Epoch 1/200, Loss: 4313.0849
Epoch 2/200, Loss: 2474.3381
Epoch 3/200, Loss: 1990.3026
Epoch 4/200, Loss: 1712.7396
Epoch 5/200, Loss: 1527.3113
Epoch 6/200, Loss: 1397.8336
Epoch 7/200, Loss: 1379.2876
Epoch 8/200, Loss: 1365.1621
Epoch 9/200, Loss: 1350.9125
Epoch 10/200, Loss: 1337.8719
Epoch 11/200, Loss: 1325.5935
Epoch 12/200, Loss: 1323.7380
Epoch 13/200, Loss: 1321.9811
Epoch 14/200, Loss: 1320.6666
Epoch 15/200, Loss: 1319.5498
Epoch 16/200, Loss: 1318.2807
Epoch 17/200, Loss: 1317.8453
Epoch 18/200, Loss: 1317.6266
Epoch 19/200, Loss: 1317.6671
Epoch 20/200, Loss: 1317.4747
Epoch 21/200, Loss: 1317.3235
Epoch 22/200, Loss: 1317.1959
Epoch 23/200, Loss: 1317.3610
Epoch 24/200, Loss: 1317.3406
Epoch 25/200, Loss: 1317.2828
Epoch 26/200, Loss: 1317.1134
Epoch 27/200, Loss: 1317.0865
Epoch 28/200, Loss: 1317.1389
Epoch 29/200, Loss: 1317.1917
Epoch 30/200, Loss: 1317.3093
Epoch 31/200, Loss: 1317.1942
Epoch 32/200, Loss: 1317.1025
Epoch 33/200, Loss: 1317.2772
Epoch 34/200, Loss: 1317.2546
Epoch 35/200, Loss: 1317.1107
Epoch 36/200, Loss: 1317.1474
Epoch 37/200, Loss: 1317.2396
Epoch 38/200, Loss: 1317.2085
Epoch 39/200, Loss: 1317.2954
Epoch 40/200, Loss: 1317.1615
Epoch 41/200, Loss: 1317.3178
Epoch 42/200, Loss: 1317.2322
Epoch 43/200, Loss: 1317.2486
Epoch 44/200, Loss: 1317.2676
Epoch 45/200, Loss: 1317.1141
Epoch 46/200, Loss: 1317.1632
Epoch 47/200, Loss: 1317.2190
Epoch 48/200, Loss: 1317.3354
Epoch 49/200, Loss: 1317.3706
Epoch 50/200, Loss: 1317.2935
Epoch 51/200, Loss: 1317.1331
Epoch 52/200, Loss: 1317.4509
Epoch 53/200, Loss: 1317.1797
Epoch 54/200, Loss: 1317.2598
Epoch 55/200, Loss: 1317.1286
Epoch 56/200, Loss: 1317.1817
Epoch 57/200, Loss: 1317.3253
Epoch 58/200, Loss: 1317.2931
Epoch 59/200, Loss: 1317.1741
Epoch 60/200, Loss: 1317.2288
Epoch 61/200, Loss: 1317.3458
Epoch 62/200, Loss: 1317.1198
Epoch 63/200, Loss: 1317.2354
Epoch 64/200, Loss: 1317.2466
Epoch 65/200, Loss: 1317.2142
Epoch 66/200, Loss: 1317.1783
Epoch 67/200, Loss: 1317.2410
Epoch 68/200, Loss: 1317.2521
Epoch 69/200, Loss: 1317.1435
Epoch 70/200, Loss: 1317.2762
Epoch 71/200, Loss: 1317.3539
Epoch 72/200, Loss: 1317.2412
Epoch 73/200, Loss: 1317.2941
Epoch 74/200, Loss: 1317.1322
Epoch 75/200, Loss: 1317.2614
Epoch 76/200, Loss: 1317.3118
Epoch 77/200, Loss: 1317.2581
Epoch 78/200, Loss: 1317.1817
Epoch 79/200, Loss: 1317.3530
Epoch 80/200, Loss: 1317.3444
Epoch 81/200, Loss: 1317.1047
Epoch 82/200, Loss: 1317.2132
Epoch 83/200, Loss: 1317.5000
Epoch 84/200, Loss: 1317.0891
Epoch 85/200, Loss: 1317.3746
Epoch 86/200, Loss: 1317.3662
Epoch 87/200, Loss: 1317.2599
Epoch 88/200, Loss: 1317.2559
Epoch 89/200, Loss: 1317.2587
Epoch 90/200, Loss: 1317.3323
Epoch 91/200, Loss: 1317.1414
Epoch 92/200, Loss: 1317.2751
Epoch 93/200, Loss: 1317.2118
Epoch 94/200, Loss: 1317.2842
Epoch 95/200, Loss: 1317.0363
Epoch 96/200, Loss: 1317.1820
Epoch 97/200, Loss: 1317.1776
Epoch 98/200, Loss: 1317.1020
Epoch 99/200, Loss: 1317.0583
Epoch 100/200, Loss: 1317.2299
Epoch 101/200, Loss: 1317.0752
Epoch 102/200, Loss: 1317.2288
Epoch 103/200, Loss: 1317.3281
Epoch 104/200, Loss: 1317.5327
Epoch 105/200, Loss: 1317.2094
Epoch 106/200, Loss: 1317.0574
Epoch 107/200, Loss: 1317.0737
Epoch 108/200, Loss: 1317.1598
Epoch 109/200, Loss: 1317.3073
Epoch 110/200, Loss: 1317.2837
Epoch 111/200, Loss: 1317.2688
Epoch 112/200, Loss: 1317.3194
Epoch 113/200, Loss: 1317.2295
Epoch 114/200, Loss: 1317.1468
Epoch 115/200, Loss: 1317.3595
Epoch 116/200, Loss: 1317.3759
Epoch 117/200, Loss: 1317.2095
Epoch 118/200, Loss: 1317.1781
Epoch 119/200, Loss: 1317.1966
Epoch 120/200, Loss: 1317.1242
Epoch 121/200, Loss: 1317.2439
Epoch 122/200, Loss: 1317.3457
Epoch 123/200, Loss: 1317.2122
Epoch 124/200, Loss: 1317.3073
Epoch 125/200, Loss: 1317.1536
Epoch 126/200, Loss: 1317.1716
Epoch 127/200, Loss: 1317.1321
Epoch 128/200, Loss: 1317.1001
Epoch 129/200, Loss: 1317.2163
Epoch 130/200, Loss: 1317.2395
Epoch 131/200, Loss: 1317.1164
Epoch 132/200, Loss: 1317.2125
Epoch 133/200, Loss: 1317.2867
Epoch 134/200, Loss: 1317.3488
Epoch 135/200, Loss: 1317.3221
Epoch 136/200, Loss: 1317.5456
Epoch 137/200, Loss: 1317.2763
Epoch 138/200, Loss: 1317.1879
Epoch 139/200, Loss: 1317.2420
Epoch 140/200, Loss: 1317.2503
Epoch 141/200, Loss: 1317.0955
Epoch 142/200, Loss: 1317.0887
Epoch 143/200, Loss: 1317.5289
Epoch 144/200, Loss: 1317.1870
Epoch 145/200, Loss: 1317.0842
Epoch 146/200, Loss: 1317.1125
Epoch 147/200, Loss: 1317.2603
Epoch 148/200, Loss: 1317.3888
Epoch 149/200, Loss: 1317.2338
Epoch 150/200, Loss: 1317.1562
Epoch 151/200, Loss: 1317.1030
Epoch 152/200, Loss: 1317.2767
Epoch 153/200, Loss: 1317.1907
Epoch 154/200, Loss: 1317.1094
Epoch 155/200, Loss: 1317.3854
Epoch 156/200, Loss: 1317.1817
Epoch 157/200, Loss: 1317.1804
Epoch 158/200, Loss: 1317.0922
Epoch 159/200, Loss: 1317.1482
Epoch 160/200, Loss: 1317.0380
Epoch 161/200, Loss: 1317.1407
Epoch 162/200, Loss: 1317.3839
Epoch 163/200, Loss: 1317.2229
Epoch 164/200, Loss: 1317.1243
Epoch 165/200, Loss: 1317.2726
Epoch 166/200, Loss: 1317.2094
Epoch 167/200, Loss: 1317.2929
Epoch 168/200, Loss: 1317.3248
Epoch 169/200, Loss: 1317.0881
Epoch 170/200, Loss: 1317.1651
Epoch 171/200, Loss: 1317.2270
Epoch 172/200, Loss: 1317.2836
Epoch 173/200, Loss: 1317.1130
Epoch 174/200, Loss: 1317.2318
Epoch 175/200, Loss: 1317.1571
Epoch 176/200, Loss: 1317.2020
Epoch 177/200, Loss: 1317.1576
Epoch 178/200, Loss: 1317.6112
Epoch 179/200, Loss: 1317.3063
Epoch 180/200, Loss: 1317.1788
Epoch 181/200, Loss: 1317.3210
Epoch 182/200, Loss: 1317.1485
Epoch 183/200, Loss: 1317.0565
Epoch 184/200, Loss: 1317.2164
Epoch 185/200, Loss: 1317.0997
Epoch 186/200, Loss: 1317.1631
Epoch 187/200, Loss: 1317.2848
Epoch 188/200, Loss: 1317.1528
Epoch 189/200, Loss: 1317.1404
Epoch 190/200, Loss: 1317.2083
Epoch 191/200, Loss: 1317.1638
Epoch 192/200, Loss: 1317.1364
Epoch 193/200, Loss: 1317.3370
Epoch 194/200, Loss: 1317.2365
Epoch 195/200, Loss: 1317.1774
Epoch 196/200, Loss: 1317.1775
Epoch 197/200, Loss: 1317.1940
Epoch 198/200, Loss: 1317.3321
Epoch 199/200, Loss: 1317.3016
Epoch 200/200, Loss: 1317.2628
Train Accuracy: 77.31% | Test Accuracy: 54.95%
Precision: 0.5956 | Recall: 0.5495 | F1-Score: 0.5439
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 17.35%
Precision: 0.1764 | Recall: 0.1735 | F1-Score: 0.1720


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1844 batches, Testing set - 182 batches
Epoch 1/200, Loss: 6226.1858
Epoch 2/200, Loss: 5673.1238
Epoch 3/200, Loss: 5456.1274
Epoch 4/200, Loss: 5313.4439
Epoch 5/200, Loss: 5188.1485
Epoch 6/200, Loss: 5056.3150
Epoch 7/200, Loss: 5034.6378
Epoch 8/200, Loss: 5019.9811
Epoch 9/200, Loss: 5005.6845
Epoch 10/200, Loss: 4991.9281
Epoch 11/200, Loss: 4978.1450
Epoch 12/200, Loss: 4975.6868
Epoch 13/200, Loss: 4974.0738
Epoch 14/200, Loss: 4972.6622
Epoch 15/200, Loss: 4971.3477
Epoch 16/200, Loss: 4969.8590
Epoch 17/200, Loss: 4969.7325
Epoch 18/200, Loss: 4969.6991
Epoch 19/200, Loss: 4969.3522
Epoch 20/200, Loss: 4969.1238
Epoch 21/200, Loss: 4969.1563
Epoch 22/200, Loss: 4969.0223
Epoch 23/200, Loss: 4969.0344
Epoch 24/200, Loss: 4969.0324
Epoch 25/200, Loss: 4968.8728
Epoch 26/200, Loss: 4968.8409
Epoch 27/200, Loss: 4969.1260
Epoch 28/200, Loss: 4969.0423
Epoch 29/200, Loss: 4968.9008
Epoch 30/200, Loss: 4968.9801
Epoch 31/200, Loss: 4968.8530
Epoch 32/200, Loss: 4968.8731
Epoch 33/200, Loss: 4968.8165
Epoch 34/200, Loss: 4968.8391
Epoch 35/200, Loss: 4968.8360
Epoch 36/200, Loss: 4968.9715
Epoch 37/200, Loss: 4968.9424
Epoch 38/200, Loss: 4968.8929
Epoch 39/200, Loss: 4968.8355
Epoch 40/200, Loss: 4968.8979
Epoch 41/200, Loss: 4969.0498
Epoch 42/200, Loss: 4968.9233
Epoch 43/200, Loss: 4968.9601
Epoch 44/200, Loss: 4968.8773
Epoch 45/200, Loss: 4968.9101
Epoch 46/200, Loss: 4969.1958
Epoch 47/200, Loss: 4969.0817
Epoch 48/200, Loss: 4968.8279
Epoch 49/200, Loss: 4968.9519
Epoch 50/200, Loss: 4968.8074
Epoch 51/200, Loss: 4968.9660
Epoch 52/200, Loss: 4968.8715
Epoch 53/200, Loss: 4968.8849
Epoch 54/200, Loss: 4969.1153
Epoch 55/200, Loss: 4968.9186
Epoch 56/200, Loss: 4968.9184
Epoch 57/200, Loss: 4968.9997
Epoch 58/200, Loss: 4968.7082
Epoch 59/200, Loss: 4968.8222
Epoch 60/200, Loss: 4968.7784
Epoch 61/200, Loss: 4969.1439
Epoch 62/200, Loss: 4968.9305
Epoch 63/200, Loss: 4968.8006
Epoch 64/200, Loss: 4968.9793
Epoch 65/200, Loss: 4968.9135
Epoch 66/200, Loss: 4968.9745
Epoch 67/200, Loss: 4968.8130
Epoch 68/200, Loss: 4968.9054
Epoch 69/200, Loss: 4969.0545
Epoch 70/200, Loss: 4969.0596
Epoch 71/200, Loss: 4968.9125
Epoch 72/200, Loss: 4968.9756
Epoch 73/200, Loss: 4968.8824
Epoch 74/200, Loss: 4968.9968
Epoch 75/200, Loss: 4968.9362
Epoch 76/200, Loss: 4968.9404
Epoch 77/200, Loss: 4968.8004
Epoch 78/200, Loss: 4969.0497
Epoch 79/200, Loss: 4968.8778
Epoch 80/200, Loss: 4968.9238
Epoch 81/200, Loss: 4968.9095
Epoch 82/200, Loss: 4968.9761
Epoch 83/200, Loss: 4968.8831
Epoch 84/200, Loss: 4969.0158
Epoch 85/200, Loss: 4968.9646
Epoch 86/200, Loss: 4968.8408
Epoch 87/200, Loss: 4968.8774
Epoch 88/200, Loss: 4968.8030
Epoch 89/200, Loss: 4969.0347
Epoch 90/200, Loss: 4968.7972
Epoch 91/200, Loss: 4969.0778
Epoch 92/200, Loss: 4968.8699
Epoch 93/200, Loss: 4968.8712
Epoch 94/200, Loss: 4968.9603
Epoch 95/200, Loss: 4968.8961
Epoch 96/200, Loss: 4968.7599
Epoch 97/200, Loss: 4968.8456
Epoch 98/200, Loss: 4969.1692
Epoch 99/200, Loss: 4968.8358
Epoch 100/200, Loss: 4968.9344
Epoch 101/200, Loss: 4968.9125
Epoch 102/200, Loss: 4969.0596
Epoch 103/200, Loss: 4968.9874
Epoch 104/200, Loss: 4968.9103
Epoch 105/200, Loss: 4969.0733
Epoch 106/200, Loss: 4968.9996
Epoch 107/200, Loss: 4968.8978
Epoch 108/200, Loss: 4968.9374
Epoch 109/200, Loss: 4968.9420
Epoch 110/200, Loss: 4968.9570
Epoch 111/200, Loss: 4968.9186
Epoch 112/200, Loss: 4968.9761
Epoch 113/200, Loss: 4968.9462
Epoch 114/200, Loss: 4968.9790
Epoch 115/200, Loss: 4968.8766
Epoch 116/200, Loss: 4968.8393
Epoch 117/200, Loss: 4969.0056
Epoch 118/200, Loss: 4968.9810
Epoch 119/200, Loss: 4968.9912
Epoch 120/200, Loss: 4968.8849
Epoch 121/200, Loss: 4968.9843
Epoch 122/200, Loss: 4969.0303
Epoch 123/200, Loss: 4968.8778
Epoch 124/200, Loss: 4968.8957
Epoch 125/200, Loss: 4968.8490
Epoch 126/200, Loss: 4969.0989
Epoch 127/200, Loss: 4968.8883
Epoch 128/200, Loss: 4968.9233
Epoch 129/200, Loss: 4968.9067
Epoch 130/200, Loss: 4968.9046
Epoch 131/200, Loss: 4968.9579
Epoch 132/200, Loss: 4969.0540
Epoch 133/200, Loss: 4968.8604
Epoch 134/200, Loss: 4968.9430
Epoch 135/200, Loss: 4969.0274
Epoch 136/200, Loss: 4969.0353
Epoch 137/200, Loss: 4969.0342
Epoch 138/200, Loss: 4968.8919
Epoch 139/200, Loss: 4968.9973
Epoch 140/200, Loss: 4969.0213
Epoch 141/200, Loss: 4969.0120
Epoch 142/200, Loss: 4968.9941
Epoch 143/200, Loss: 4968.9727
Epoch 144/200, Loss: 4968.9666
Epoch 145/200, Loss: 4969.0427
Epoch 146/200, Loss: 4969.0867
Epoch 147/200, Loss: 4968.8763
Epoch 148/200, Loss: 4969.0161
Epoch 149/200, Loss: 4968.8350
Epoch 150/200, Loss: 4968.8225
Epoch 151/200, Loss: 4968.8534
Epoch 152/200, Loss: 4969.0295
Epoch 153/200, Loss: 4968.8402
Epoch 154/200, Loss: 4969.0019
Epoch 155/200, Loss: 4968.8939
Epoch 156/200, Loss: 4969.0566
Epoch 157/200, Loss: 4968.8378
Epoch 158/200, Loss: 4968.9184
Epoch 159/200, Loss: 4968.8384
Epoch 160/200, Loss: 4968.8657
Epoch 161/200, Loss: 4968.9108
Epoch 162/200, Loss: 4968.9912
Epoch 163/200, Loss: 4968.9816
Epoch 164/200, Loss: 4968.8985
Epoch 165/200, Loss: 4969.0376
Epoch 166/200, Loss: 4968.9456
Epoch 167/200, Loss: 4968.8982
Epoch 168/200, Loss: 4968.8934
Epoch 169/200, Loss: 4968.9460
Epoch 170/200, Loss: 4968.8780
Epoch 171/200, Loss: 4968.9581
Epoch 172/200, Loss: 4968.9962
Epoch 173/200, Loss: 4969.0043
Epoch 174/200, Loss: 4968.9889
Epoch 175/200, Loss: 4968.8474
Epoch 176/200, Loss: 4968.8130
Epoch 177/200, Loss: 4968.8353
Epoch 178/200, Loss: 4968.9916
Epoch 179/200, Loss: 4969.0622
Epoch 180/200, Loss: 4969.0096
Epoch 181/200, Loss: 4969.0167
Epoch 182/200, Loss: 4968.6102
Epoch 183/200, Loss: 4968.9739
Epoch 184/200, Loss: 4968.9436
Epoch 185/200, Loss: 4968.9403
Epoch 186/200, Loss: 4968.9289
Epoch 187/200, Loss: 4968.9758
Epoch 188/200, Loss: 4968.9642
Epoch 189/200, Loss: 4968.9438
Epoch 190/200, Loss: 4968.9938
Epoch 191/200, Loss: 4968.9622
Epoch 192/200, Loss: 4968.9931
Epoch 193/200, Loss: 4968.8604
Epoch 194/200, Loss: 4969.0807
Epoch 195/200, Loss: 4968.9867
Epoch 196/200, Loss: 4969.0432
Epoch 197/200, Loss: 4968.9098
Epoch 198/200, Loss: 4969.0114
Epoch 199/200, Loss: 4969.0534
Epoch 200/200, Loss: 4968.8353
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 19.25% | Test Accuracy: 16.43%
Precision: 0.1046 | Recall: 0.1643 | F1-Score: 0.1045

Processing Subject 9...
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:112: UserWarning: Features [ 3 15 60 64] are constant.
  warnings.warn("Features %s are constant." % constant_features_idx, UserWarning)
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide
  f = msb / msw
Top 32 discriminative features: [64 60  3 15 26 38 46 44 14 12 41 29 18 16 30 42 40 22 63 28 25 37 61 45
 34 33 21 59 57  9  1 49]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 117605
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3193
		Class 2: 3330
		Class 3: 3600
		Class 4: 3529
		Class 5: 3571
		Class 6: 3600
		Class 7: 3471
		Class 8: 3235
		Class 9: 3600
		Class 10: 3567
		Class 11: 1754
		Class 12: 1859
		Class 13: 2365
		Class 14: 2991
		Class 15: 2575
		Class 16: 1815
		Class 17: 1816
		Class 18: 2211
		Class 19: 2924
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2682
		Class 25: 3600
		Class 26: 2334
		Class 27: 3171
		Class 28: 2767
		Class 29: 3076
		Class 30: 3367
		Class 31: 3248
		Class 32: 2654
		Class 33: 2237
		Class 34: 3600
		Class 35: 3116
		Class 36: 3106
		Class 37: 3360
		Class 38: 2501
	# of Testing Samples: 11970
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 282
		Class 2: 328
		Class 3: 400
		Class 4: 400
		Class 5: 317
		Class 6: 400
		Class 7: 400
		Class 8: 304
		Class 9: 400
		Class 10: 362
		Class 11: 177
		Class 12: 123
		Class 13: 206
		Class 14: 303
		Class 15: 273
		Class 16: 156
		Class 17: 121
		Class 18: 178
		Class 19: 269
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 254
		Class 25: 400
		Class 26: 256
		Class 27: 347
		Class 28: 306
		Class 29: 315
		Class 30: 331
		Class 31: 274
		Class 32: 236
		Class 33: 190
		Class 34: 400
		Class 35: 289
		Class 36: 317
		Class 37: 382
		Class 38: 274
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 68.26%
Precision: 0.7149 | Recall: 0.6826 | F1-Score: 0.6753


Running Deep Learning Classifier...
DataLoader: Training set - 1838 batches, Testing set - 188 batches
Epoch 1/200, Loss: 3916.5042
Epoch 2/200, Loss: 2357.1347
Epoch 3/200, Loss: 1898.7248
Epoch 4/200, Loss: 1644.2753
Epoch 5/200, Loss: 1473.9339
Epoch 6/200, Loss: 1355.2536
Epoch 7/200, Loss: 1338.5765
Epoch 8/200, Loss: 1325.3729
Epoch 9/200, Loss: 1312.5301
Epoch 10/200, Loss: 1300.5643
Epoch 11/200, Loss: 1289.4455
Epoch 12/200, Loss: 1287.2716
Epoch 13/200, Loss: 1285.8841
Epoch 14/200, Loss: 1284.7994
Epoch 15/200, Loss: 1283.4447
Epoch 16/200, Loss: 1282.1679
Epoch 17/200, Loss: 1282.0926
Epoch 18/200, Loss: 1281.9100
Epoch 19/200, Loss: 1281.8099
Epoch 20/200, Loss: 1281.7285
Epoch 21/200, Loss: 1281.5153
Epoch 22/200, Loss: 1281.6297
Epoch 23/200, Loss: 1281.5038
Epoch 24/200, Loss: 1281.4100
Epoch 25/200, Loss: 1281.4652
Epoch 26/200, Loss: 1281.5519
Epoch 27/200, Loss: 1281.5212
Epoch 28/200, Loss: 1281.5121
Epoch 29/200, Loss: 1281.5397
Epoch 30/200, Loss: 1281.5337
Epoch 31/200, Loss: 1281.5210
Epoch 32/200, Loss: 1281.4101
Epoch 33/200, Loss: 1281.5250
Epoch 34/200, Loss: 1281.3870
Epoch 35/200, Loss: 1281.5643
Epoch 36/200, Loss: 1281.4898
Epoch 37/200, Loss: 1281.5291
Epoch 38/200, Loss: 1281.5835
Epoch 39/200, Loss: 1281.6178
Epoch 40/200, Loss: 1281.5948
Epoch 41/200, Loss: 1281.4919
Epoch 42/200, Loss: 1281.4368
Epoch 43/200, Loss: 1281.4620
Epoch 44/200, Loss: 1281.4115
Epoch 45/200, Loss: 1281.5287
Epoch 46/200, Loss: 1281.5258
Epoch 47/200, Loss: 1281.5083
Epoch 48/200, Loss: 1281.4321
Epoch 49/200, Loss: 1281.5312
Epoch 50/200, Loss: 1281.3687
Epoch 51/200, Loss: 1281.3809
Epoch 52/200, Loss: 1281.4809
Epoch 53/200, Loss: 1281.4483
Epoch 54/200, Loss: 1281.3955
Epoch 55/200, Loss: 1281.3741
Epoch 56/200, Loss: 1281.5278
Epoch 57/200, Loss: 1281.6440
Epoch 58/200, Loss: 1281.6138
Epoch 59/200, Loss: 1281.5112
Epoch 60/200, Loss: 1281.5433
Epoch 61/200, Loss: 1281.5058
Epoch 62/200, Loss: 1281.4321
Epoch 63/200, Loss: 1281.4908
Epoch 64/200, Loss: 1281.4485
Epoch 65/200, Loss: 1281.4003
Epoch 66/200, Loss: 1281.6251
Epoch 67/200, Loss: 1281.4071
Epoch 68/200, Loss: 1281.4472
Epoch 69/200, Loss: 1281.4071
Epoch 70/200, Loss: 1281.4053
Epoch 71/200, Loss: 1281.4620
Epoch 72/200, Loss: 1281.5038
Epoch 73/200, Loss: 1281.5789
Epoch 74/200, Loss: 1281.5252
Epoch 75/200, Loss: 1281.4356
Epoch 76/200, Loss: 1281.4263
Epoch 77/200, Loss: 1281.4149
Epoch 78/200, Loss: 1281.5728
Epoch 79/200, Loss: 1281.4142
Epoch 80/200, Loss: 1281.4854
Epoch 81/200, Loss: 1281.5485
Epoch 82/200, Loss: 1281.4667
Epoch 83/200, Loss: 1281.5998
Epoch 84/200, Loss: 1281.4057
Epoch 85/200, Loss: 1281.4466
Epoch 86/200, Loss: 1281.5631
Epoch 87/200, Loss: 1281.6854
Epoch 88/200, Loss: 1281.4403
Epoch 89/200, Loss: 1281.4488
Epoch 90/200, Loss: 1281.6816
Epoch 91/200, Loss: 1281.5084
Epoch 92/200, Loss: 1281.4301
Epoch 93/200, Loss: 1281.4811
Epoch 94/200, Loss: 1281.4658
Epoch 95/200, Loss: 1281.4941
Epoch 96/200, Loss: 1281.3847
Epoch 97/200, Loss: 1281.4772
Epoch 98/200, Loss: 1281.4929
Epoch 99/200, Loss: 1281.5788
Epoch 100/200, Loss: 1281.5161
Epoch 101/200, Loss: 1281.4991
Epoch 102/200, Loss: 1281.4611
Epoch 103/200, Loss: 1281.4875
Epoch 104/200, Loss: 1281.5126
Epoch 105/200, Loss: 1281.4708
Epoch 106/200, Loss: 1281.4492
Epoch 107/200, Loss: 1281.4344
Epoch 108/200, Loss: 1281.3764
Epoch 109/200, Loss: 1281.4629
Epoch 110/200, Loss: 1281.5250
Epoch 111/200, Loss: 1281.5007
Epoch 112/200, Loss: 1281.5416
Epoch 113/200, Loss: 1281.4672
Epoch 114/200, Loss: 1281.5576
Epoch 115/200, Loss: 1281.4976
Epoch 116/200, Loss: 1281.4824
Epoch 117/200, Loss: 1281.4648
Epoch 118/200, Loss: 1281.4785
Epoch 119/200, Loss: 1281.5334
Epoch 120/200, Loss: 1281.4087
Epoch 121/200, Loss: 1281.5344
Epoch 122/200, Loss: 1281.4462
Epoch 123/200, Loss: 1281.5024
Epoch 124/200, Loss: 1281.6211
Epoch 125/200, Loss: 1281.5831
Epoch 126/200, Loss: 1281.4961
Epoch 127/200, Loss: 1281.5007
Epoch 128/200, Loss: 1281.4654
Epoch 129/200, Loss: 1281.3849
Epoch 130/200, Loss: 1281.3679
Epoch 131/200, Loss: 1281.4102
Epoch 132/200, Loss: 1281.4453
Epoch 133/200, Loss: 1281.4906
Epoch 134/200, Loss: 1281.4676
Epoch 135/200, Loss: 1281.4583
Epoch 136/200, Loss: 1281.4183
Epoch 137/200, Loss: 1281.4819
Epoch 138/200, Loss: 1281.4183
Epoch 139/200, Loss: 1281.5029
Epoch 140/200, Loss: 1281.5987
Epoch 141/200, Loss: 1281.4778
Epoch 142/200, Loss: 1281.4479
Epoch 143/200, Loss: 1281.4497
Epoch 144/200, Loss: 1281.4338
Epoch 145/200, Loss: 1281.4085
Epoch 146/200, Loss: 1281.5283
Epoch 147/200, Loss: 1281.4418
Epoch 148/200, Loss: 1281.4441
Epoch 149/200, Loss: 1281.4567
Epoch 150/200, Loss: 1281.5515
Epoch 151/200, Loss: 1281.4302
Epoch 152/200, Loss: 1281.4309
Epoch 153/200, Loss: 1281.4796
Epoch 154/200, Loss: 1281.4798
Epoch 155/200, Loss: 1281.4355
Epoch 156/200, Loss: 1281.4730
Epoch 157/200, Loss: 1281.5268
Epoch 158/200, Loss: 1281.5138
Epoch 159/200, Loss: 1281.4565
Epoch 160/200, Loss: 1281.4146
Epoch 161/200, Loss: 1281.5097
Epoch 162/200, Loss: 1281.5599
Epoch 163/200, Loss: 1281.5348
Epoch 164/200, Loss: 1281.5869
Epoch 165/200, Loss: 1281.6570
Epoch 166/200, Loss: 1281.3861
Epoch 167/200, Loss: 1281.3921
Epoch 168/200, Loss: 1281.5199
Epoch 169/200, Loss: 1281.5767
Epoch 170/200, Loss: 1281.5376
Epoch 171/200, Loss: 1281.5015
Epoch 172/200, Loss: 1281.4114
Epoch 173/200, Loss: 1281.6410
Epoch 174/200, Loss: 1281.6817
Epoch 175/200, Loss: 1281.4530
Epoch 176/200, Loss: 1281.6232
Epoch 177/200, Loss: 1281.4542
Epoch 178/200, Loss: 1281.4425
Epoch 179/200, Loss: 1281.4964
Epoch 180/200, Loss: 1281.5621
Epoch 181/200, Loss: 1281.4894
Epoch 182/200, Loss: 1281.5673
Epoch 183/200, Loss: 1281.5650
Epoch 184/200, Loss: 1281.5176
Epoch 185/200, Loss: 1281.5641
Epoch 186/200, Loss: 1281.5022
Epoch 187/200, Loss: 1281.4642
Epoch 188/200, Loss: 1281.5078
Epoch 189/200, Loss: 1281.5756
Epoch 190/200, Loss: 1281.3320
Epoch 191/200, Loss: 1281.5085
Epoch 192/200, Loss: 1281.5784
Epoch 193/200, Loss: 1281.6037
Epoch 194/200, Loss: 1281.5214
Epoch 195/200, Loss: 1281.5193
Epoch 196/200, Loss: 1281.4912
Epoch 197/200, Loss: 1281.4926
Epoch 198/200, Loss: 1281.5191
Epoch 199/200, Loss: 1281.5161
Epoch 200/200, Loss: 1281.4672
Train Accuracy: 77.44% | Test Accuracy: 57.60%
Precision: 0.6130 | Recall: 0.5760 | F1-Score: 0.5718
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 18.52%
Precision: 0.1909 | Recall: 0.1852 | F1-Score: 0.1837


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1838 batches, Testing set - 188 batches
Epoch 1/200, Loss: 6105.7267
Epoch 2/200, Loss: 5371.5539
Epoch 3/200, Loss: 5094.4587
Epoch 4/200, Loss: 4956.6602
Epoch 5/200, Loss: 4850.1671
Epoch 6/200, Loss: 4740.7369
Epoch 7/200, Loss: 4728.5512
Epoch 8/200, Loss: 4718.3179
Epoch 9/200, Loss: 4708.3743
Epoch 10/200, Loss: 4699.0544
Epoch 11/200, Loss: 4688.6377
Epoch 12/200, Loss: 4687.1312
Epoch 13/200, Loss: 4686.1498
Epoch 14/200, Loss: 4685.3530
Epoch 15/200, Loss: 4684.4194
Epoch 16/200, Loss: 4683.1865
Epoch 17/200, Loss: 4683.0687
Epoch 18/200, Loss: 4682.8658
Epoch 19/200, Loss: 4682.7741
Epoch 20/200, Loss: 4682.8412
Epoch 21/200, Loss: 4682.5071
Epoch 22/200, Loss: 4682.5325
Epoch 23/200, Loss: 4682.6514
Epoch 24/200, Loss: 4682.5988
Epoch 25/200, Loss: 4682.6201
Epoch 26/200, Loss: 4682.6551
Epoch 27/200, Loss: 4682.5646
Epoch 28/200, Loss: 4682.5581
Epoch 29/200, Loss: 4682.6434
Epoch 30/200, Loss: 4682.6119
Epoch 31/200, Loss: 4682.7188
Epoch 32/200, Loss: 4682.6194
Epoch 33/200, Loss: 4682.5954
Epoch 34/200, Loss: 4682.5664
Epoch 35/200, Loss: 4682.4994
Epoch 36/200, Loss: 4682.6296
Epoch 37/200, Loss: 4682.6607
Epoch 38/200, Loss: 4682.5680
Epoch 39/200, Loss: 4682.5900
Epoch 40/200, Loss: 4682.6038
Epoch 41/200, Loss: 4682.5262
Epoch 42/200, Loss: 4682.4676
Epoch 43/200, Loss: 4682.6267
Epoch 44/200, Loss: 4682.6778
Epoch 45/200, Loss: 4682.5199
Epoch 46/200, Loss: 4682.6370
Epoch 47/200, Loss: 4682.5765
Epoch 48/200, Loss: 4682.4884
Epoch 49/200, Loss: 4682.4511
Epoch 50/200, Loss: 4682.7346
Epoch 51/200, Loss: 4682.5460
Epoch 52/200, Loss: 4682.6242
Epoch 53/200, Loss: 4682.6308
Epoch 54/200, Loss: 4682.6155
Epoch 55/200, Loss: 4682.7108
Epoch 56/200, Loss: 4682.6477
Epoch 57/200, Loss: 4682.5859
Epoch 58/200, Loss: 4682.5635
Epoch 59/200, Loss: 4682.5904
Epoch 60/200, Loss: 4682.5873
Epoch 61/200, Loss: 4682.5049
Epoch 62/200, Loss: 4682.6233
Epoch 63/200, Loss: 4682.6303
Epoch 64/200, Loss: 4682.5975
Epoch 65/200, Loss: 4682.5890
Epoch 66/200, Loss: 4682.5704
Epoch 67/200, Loss: 4682.5851
Epoch 68/200, Loss: 4682.6460
Epoch 69/200, Loss: 4682.5336
Epoch 70/200, Loss: 4682.6558
Epoch 71/200, Loss: 4682.5974
Epoch 72/200, Loss: 4682.6239
Epoch 73/200, Loss: 4682.5301
Epoch 74/200, Loss: 4682.5496
Epoch 75/200, Loss: 4682.5300
Epoch 76/200, Loss: 4682.5954
Epoch 77/200, Loss: 4682.6340
Epoch 78/200, Loss: 4682.6195
Epoch 79/200, Loss: 4682.5481
Epoch 80/200, Loss: 4682.6423
Epoch 81/200, Loss: 4682.5678
Epoch 82/200, Loss: 4682.6488
Epoch 83/200, Loss: 4682.5766
Epoch 84/200, Loss: 4682.4608
Epoch 85/200, Loss: 4682.6037
Epoch 86/200, Loss: 4682.4960
Epoch 87/200, Loss: 4682.6581
Epoch 88/200, Loss: 4682.5175
Epoch 89/200, Loss: 4682.5902
Epoch 90/200, Loss: 4682.5896
Epoch 91/200, Loss: 4682.5310
Epoch 92/200, Loss: 4682.5244
Epoch 93/200, Loss: 4682.6788
Epoch 94/200, Loss: 4682.6293
Epoch 95/200, Loss: 4682.4096
Epoch 96/200, Loss: 4682.6498
Epoch 97/200, Loss: 4682.5766
Epoch 98/200, Loss: 4682.5653
Epoch 99/200, Loss: 4682.5208
Epoch 100/200, Loss: 4682.5507
Epoch 101/200, Loss: 4682.4842
Epoch 102/200, Loss: 4682.7365
Epoch 103/200, Loss: 4682.5875
Epoch 104/200, Loss: 4682.6593
Epoch 105/200, Loss: 4682.5671
Epoch 106/200, Loss: 4682.5939
Epoch 107/200, Loss: 4682.5394
Epoch 108/200, Loss: 4682.5670
Epoch 109/200, Loss: 4682.5904
Epoch 110/200, Loss: 4682.4951
Epoch 111/200, Loss: 4682.6022
Epoch 112/200, Loss: 4682.6397
Epoch 113/200, Loss: 4682.5674
Epoch 114/200, Loss: 4682.6508
Epoch 115/200, Loss: 4682.5350
Epoch 116/200, Loss: 4682.5236
Epoch 117/200, Loss: 4682.5247
Epoch 118/200, Loss: 4682.7389
Epoch 119/200, Loss: 4682.6267
Epoch 120/200, Loss: 4682.5321
Epoch 121/200, Loss: 4682.6280
Epoch 122/200, Loss: 4682.5321
Epoch 123/200, Loss: 4682.5520
Epoch 124/200, Loss: 4682.5562
Epoch 125/200, Loss: 4682.6266
Epoch 126/200, Loss: 4682.5940
Epoch 127/200, Loss: 4682.5153
Epoch 128/200, Loss: 4682.5641
Epoch 129/200, Loss: 4682.6165
Epoch 130/200, Loss: 4682.5078
Epoch 131/200, Loss: 4682.5502
Epoch 132/200, Loss: 4682.4644
Epoch 133/200, Loss: 4682.6077
Epoch 134/200, Loss: 4682.6870
Epoch 135/200, Loss: 4682.6128
Epoch 136/200, Loss: 4682.5783
Epoch 137/200, Loss: 4682.5927
Epoch 138/200, Loss: 4682.6955
Epoch 139/200, Loss: 4682.3946
Epoch 140/200, Loss: 4682.4826
Epoch 141/200, Loss: 4682.4996
Epoch 142/200, Loss: 4682.5963
Epoch 143/200, Loss: 4682.5834
Epoch 144/200, Loss: 4682.6377
Epoch 145/200, Loss: 4682.5614
Epoch 146/200, Loss: 4682.6539
Epoch 147/200, Loss: 4682.5985
Epoch 148/200, Loss: 4682.6877
Epoch 149/200, Loss: 4682.5249
Epoch 150/200, Loss: 4682.5079
Epoch 151/200, Loss: 4682.5314
Epoch 152/200, Loss: 4682.5391
Epoch 153/200, Loss: 4682.6388
Epoch 154/200, Loss: 4682.6853
Epoch 155/200, Loss: 4682.7671
Epoch 156/200, Loss: 4682.5511
Epoch 157/200, Loss: 4682.5477
Epoch 158/200, Loss: 4682.7042
Epoch 159/200, Loss: 4682.4915
Epoch 160/200, Loss: 4682.6465
Epoch 161/200, Loss: 4682.5193
Epoch 162/200, Loss: 4682.5691
Epoch 163/200, Loss: 4682.5046
Epoch 164/200, Loss: 4682.5745
Epoch 165/200, Loss: 4682.6190
Epoch 166/200, Loss: 4682.5043
Epoch 167/200, Loss: 4682.5212
Epoch 168/200, Loss: 4682.5789
Epoch 169/200, Loss: 4682.5859
Epoch 170/200, Loss: 4682.5896
Epoch 171/200, Loss: 4682.7431
Epoch 172/200, Loss: 4682.6077
Epoch 173/200, Loss: 4682.5863
Epoch 174/200, Loss: 4682.6491
Epoch 175/200, Loss: 4682.6568
Epoch 176/200, Loss: 4682.5171
Epoch 177/200, Loss: 4682.5365
Epoch 178/200, Loss: 4682.6659
Epoch 179/200, Loss: 4682.5132
Epoch 180/200, Loss: 4682.5621
Epoch 181/200, Loss: 4682.5024
Epoch 182/200, Loss: 4682.6400
Epoch 183/200, Loss: 4682.5820
Epoch 184/200, Loss: 4682.4319
Epoch 185/200, Loss: 4682.5170
Epoch 186/200, Loss: 4682.5232
Epoch 187/200, Loss: 4682.5851
Epoch 188/200, Loss: 4682.4609
Epoch 189/200, Loss: 4682.4788
Epoch 190/200, Loss: 4682.4105
Epoch 191/200, Loss: 4682.5515
Epoch 192/200, Loss: 4682.5014
Epoch 193/200, Loss: 4682.5753
Epoch 194/200, Loss: 4682.5869
Epoch 195/200, Loss: 4682.5527
Epoch 196/200, Loss: 4682.6580
Epoch 197/200, Loss: 4682.4996
Epoch 198/200, Loss: 4682.6388
Epoch 199/200, Loss: 4682.5126
Epoch 200/200, Loss: 4682.4303
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 22.90% | Test Accuracy: 17.97%
Precision: 0.1369 | Recall: 0.1797 | F1-Score: 0.1355

Processing Subject 10...
Top 32 discriminative features: [26 38 46 44 41 29 14 12 18 16 42 30 40 25 37 34 22 45 33 28 63 61 32  9
 21 59 57 49 51 20  1  2]

Dataset Loaded: Taiji_dataset_200.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 115725
	# per Class in Train Dataset:
		Class 0: 3448
		Class 1: 3145
		Class 2: 3258
		Class 3: 3600
		Class 4: 3529
		Class 5: 3488
		Class 6: 3600
		Class 7: 3498
		Class 8: 3139
		Class 9: 3600
		Class 10: 3529
		Class 11: 1703
		Class 12: 1748
		Class 13: 2240
		Class 14: 2940
		Class 15: 2543
		Class 16: 1735
		Class 17: 1727
		Class 18: 2117
		Class 19: 2870
		Class 20: 3591
		Class 21: 3600
		Class 22: 3579
		Class 23: 3562
		Class 24: 2645
		Class 25: 3600
		Class 26: 2301
		Class 27: 3118
		Class 28: 2721
		Class 29: 2991
		Class 30: 3298
		Class 31: 3122
		Class 32: 2548
		Class 33: 2129
		Class 34: 3600
		Class 35: 3016
		Class 36: 3023
		Class 37: 3342
		Class 38: 2482
	# of Testing Samples: 13850
	# per Class in Test Dataset:
		Class 0: 400
		Class 1: 330
		Class 2: 400
		Class 3: 400
		Class 4: 400
		Class 5: 400
		Class 6: 400
		Class 7: 373
		Class 8: 400
		Class 9: 400
		Class 10: 400
		Class 11: 228
		Class 12: 234
		Class 13: 331
		Class 14: 354
		Class 15: 305
		Class 16: 236
		Class 17: 210
		Class 18: 272
		Class 19: 323
		Class 20: 400
		Class 21: 400
		Class 22: 400
		Class 23: 400
		Class 24: 291
		Class 25: 400
		Class 26: 289
		Class 27: 400
		Class 28: 352
		Class 29: 400
		Class 30: 400
		Class 31: 400
		Class 32: 342
		Class 33: 298
		Class 34: 400
		Class 35: 389
		Class 36: 400
		Class 37: 400
		Class 38: 293
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 61.16%
Precision: 0.6304 | Recall: 0.6116 | F1-Score: 0.5886


Running Deep Learning Classifier...
DataLoader: Training set - 1809 batches, Testing set - 217 batches
Epoch 1/200, Loss: 3773.1468
Epoch 2/200, Loss: 2279.0042
Epoch 3/200, Loss: 1852.5484
Epoch 4/200, Loss: 1602.2757
Epoch 5/200, Loss: 1427.0946
Epoch 6/200, Loss: 1309.3072
Epoch 7/200, Loss: 1291.1500
Epoch 8/200, Loss: 1277.4848
Epoch 9/200, Loss: 1264.9333
Epoch 10/200, Loss: 1252.9365
Epoch 11/200, Loss: 1241.5141
Epoch 12/200, Loss: 1239.8103
Epoch 13/200, Loss: 1238.1051
Epoch 14/200, Loss: 1236.9028
Epoch 15/200, Loss: 1236.4692
Epoch 16/200, Loss: 1234.7486
Epoch 17/200, Loss: 1234.2846
Epoch 18/200, Loss: 1234.4671
Epoch 19/200, Loss: 1234.2297
Epoch 20/200, Loss: 1234.3524
Epoch 21/200, Loss: 1234.3361
Epoch 22/200, Loss: 1234.4150
Epoch 23/200, Loss: 1234.0195
Epoch 24/200, Loss: 1233.8390
Epoch 25/200, Loss: 1234.1852
Epoch 26/200, Loss: 1233.7358
Epoch 27/200, Loss: 1233.9005
Epoch 28/200, Loss: 1233.9221
Epoch 29/200, Loss: 1233.9412
Epoch 30/200, Loss: 1233.9684
Epoch 31/200, Loss: 1233.9358
Epoch 32/200, Loss: 1233.6359
Epoch 33/200, Loss: 1233.9130
Epoch 34/200, Loss: 1233.7994
Epoch 35/200, Loss: 1233.7962
Epoch 36/200, Loss: 1233.7090
Epoch 37/200, Loss: 1233.7567
Epoch 38/200, Loss: 1233.6082
Epoch 39/200, Loss: 1233.8646
Epoch 40/200, Loss: 1234.1081
Epoch 41/200, Loss: 1233.7946
Epoch 42/200, Loss: 1234.2774
Epoch 43/200, Loss: 1233.6853
Epoch 44/200, Loss: 1233.8000
Epoch 45/200, Loss: 1233.8768
Epoch 46/200, Loss: 1234.0412
Epoch 47/200, Loss: 1233.7676
Epoch 48/200, Loss: 1233.8752
Epoch 49/200, Loss: 1234.0636
Epoch 50/200, Loss: 1233.8498
Epoch 51/200, Loss: 1233.7922
Epoch 52/200, Loss: 1233.8519
Epoch 53/200, Loss: 1233.9594
Epoch 54/200, Loss: 1234.2578
Epoch 55/200, Loss: 1233.9753
Epoch 56/200, Loss: 1233.8144
Epoch 57/200, Loss: 1233.7109
Epoch 58/200, Loss: 1234.0758
Epoch 59/200, Loss: 1233.9903
Epoch 60/200, Loss: 1233.6001
Epoch 61/200, Loss: 1233.7662
Epoch 62/200, Loss: 1233.6888
Epoch 63/200, Loss: 1233.9488
Epoch 64/200, Loss: 1233.6976
Epoch 65/200, Loss: 1234.1560
Epoch 66/200, Loss: 1233.8649
Epoch 67/200, Loss: 1233.8479
Epoch 68/200, Loss: 1234.1090
Epoch 69/200, Loss: 1234.0907
Epoch 70/200, Loss: 1233.9118
Epoch 71/200, Loss: 1233.7209
Epoch 72/200, Loss: 1233.9696
Epoch 73/200, Loss: 1233.8416
Epoch 74/200, Loss: 1233.8411
Epoch 75/200, Loss: 1233.7504
Epoch 76/200, Loss: 1234.0688
Epoch 77/200, Loss: 1233.9868
Epoch 78/200, Loss: 1234.0065
Epoch 79/200, Loss: 1234.2521
Epoch 80/200, Loss: 1234.3349
Epoch 81/200, Loss: 1234.2062
Epoch 82/200, Loss: 1233.8949
Epoch 83/200, Loss: 1234.0288
Epoch 84/200, Loss: 1233.9559
Epoch 85/200, Loss: 1233.8391
Epoch 86/200, Loss: 1234.0245
Epoch 87/200, Loss: 1233.7322
Epoch 88/200, Loss: 1233.9103
Epoch 89/200, Loss: 1233.9816
Epoch 90/200, Loss: 1233.6619
Epoch 91/200, Loss: 1234.0076
Epoch 92/200, Loss: 1234.3806
Epoch 93/200, Loss: 1233.9664
Epoch 94/200, Loss: 1234.1960
Epoch 95/200, Loss: 1233.9361
Epoch 96/200, Loss: 1233.9757
Epoch 97/200, Loss: 1234.7914
Epoch 98/200, Loss: 1233.7095
Epoch 99/200, Loss: 1233.9089
Epoch 100/200, Loss: 1233.9949
Epoch 101/200, Loss: 1234.1755
Epoch 102/200, Loss: 1233.8217
Epoch 103/200, Loss: 1233.7796
Epoch 104/200, Loss: 1233.8775
Epoch 105/200, Loss: 1233.9046
Epoch 106/200, Loss: 1233.9931
Epoch 107/200, Loss: 1233.7642
Epoch 108/200, Loss: 1233.9398
Epoch 109/200, Loss: 1233.6394
Epoch 110/200, Loss: 1234.0813
Epoch 111/200, Loss: 1233.9129
Epoch 112/200, Loss: 1233.7770
Epoch 113/200, Loss: 1233.8768
Epoch 114/200, Loss: 1233.7533
Epoch 115/200, Loss: 1234.1272
Epoch 116/200, Loss: 1234.0896
Epoch 117/200, Loss: 1234.0997
Epoch 118/200, Loss: 1233.7938
Epoch 119/200, Loss: 1233.9972
Epoch 120/200, Loss: 1234.0412
Epoch 121/200, Loss: 1233.7997
Epoch 122/200, Loss: 1234.3512
Epoch 123/200, Loss: 1233.6502
Epoch 124/200, Loss: 1233.8306
Epoch 125/200, Loss: 1234.5134
Epoch 126/200, Loss: 1233.6797
Epoch 127/200, Loss: 1233.7549
Epoch 128/200, Loss: 1234.0224
Epoch 129/200, Loss: 1234.1448
Epoch 130/200, Loss: 1234.4721
Epoch 131/200, Loss: 1233.7654
Epoch 132/200, Loss: 1233.8156
Epoch 133/200, Loss: 1233.7556
Epoch 134/200, Loss: 1233.8377
Epoch 135/200, Loss: 1234.2136
Epoch 136/200, Loss: 1233.8680
Epoch 137/200, Loss: 1233.8955
Epoch 138/200, Loss: 1233.7276
Epoch 139/200, Loss: 1233.7649
Epoch 140/200, Loss: 1234.0043
Epoch 141/200, Loss: 1234.0006
Epoch 142/200, Loss: 1234.2423
Epoch 143/200, Loss: 1234.2371
Epoch 144/200, Loss: 1233.7684
Epoch 145/200, Loss: 1233.8805
Epoch 146/200, Loss: 1233.9561
Epoch 147/200, Loss: 1234.1757
Epoch 148/200, Loss: 1233.6297
Epoch 149/200, Loss: 1233.7398
Epoch 150/200, Loss: 1233.9966
Epoch 151/200, Loss: 1233.8876
Epoch 152/200, Loss: 1234.5107
Epoch 153/200, Loss: 1233.8304
Epoch 154/200, Loss: 1233.8623
Epoch 155/200, Loss: 1234.0654
Epoch 156/200, Loss: 1233.7014
Epoch 157/200, Loss: 1234.0102
Epoch 158/200, Loss: 1233.6411
Epoch 159/200, Loss: 1234.0613
Epoch 160/200, Loss: 1233.6959
Epoch 161/200, Loss: 1233.8864
Epoch 162/200, Loss: 1233.8288
Epoch 163/200, Loss: 1234.0303
Epoch 164/200, Loss: 1234.3748
Epoch 165/200, Loss: 1234.1931
Epoch 166/200, Loss: 1233.8037
Epoch 167/200, Loss: 1233.9066
Epoch 168/200, Loss: 1234.0377
Epoch 169/200, Loss: 1233.7578
Epoch 170/200, Loss: 1233.8348
Epoch 171/200, Loss: 1233.7896
Epoch 172/200, Loss: 1233.8629
Epoch 173/200, Loss: 1233.9407
Epoch 174/200, Loss: 1233.6426
Epoch 175/200, Loss: 1233.7705
Epoch 176/200, Loss: 1233.9743
Epoch 177/200, Loss: 1233.7952
Epoch 178/200, Loss: 1233.8962
Epoch 179/200, Loss: 1234.1839
Epoch 180/200, Loss: 1233.7150
Epoch 181/200, Loss: 1233.6548
Epoch 182/200, Loss: 1234.0488
Epoch 183/200, Loss: 1235.2125
Epoch 184/200, Loss: 1233.9315
Epoch 185/200, Loss: 1233.7815
Epoch 186/200, Loss: 1233.7011
Epoch 187/200, Loss: 1233.7831
Epoch 188/200, Loss: 1233.9636
Epoch 189/200, Loss: 1233.6772
Epoch 190/200, Loss: 1234.1217
Epoch 191/200, Loss: 1234.0303
Epoch 192/200, Loss: 1233.9017
Epoch 193/200, Loss: 1234.3052
Epoch 194/200, Loss: 1234.0258
Epoch 195/200, Loss: 1233.7963
Epoch 196/200, Loss: 1233.7482
Epoch 197/200, Loss: 1234.0294
Epoch 198/200, Loss: 1234.1285
Epoch 199/200, Loss: 1234.3897
Epoch 200/200, Loss: 1233.9965
Train Accuracy: 78.00% | Test Accuracy: 51.29%
Precision: 0.5570 | Recall: 0.5129 | F1-Score: 0.5006
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 18.65%
Precision: 0.1812 | Recall: 0.1865 | F1-Score: 0.1820


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 1809 batches, Testing set - 217 batches
Epoch 1/200, Loss: 6279.6485
Epoch 2/200, Loss: 5544.5456
Epoch 3/200, Loss: 5338.0970
Epoch 4/200, Loss: 5163.2953
Epoch 5/200, Loss: 5005.5629
Epoch 6/200, Loss: 4867.7695
Epoch 7/200, Loss: 4847.3860
Epoch 8/200, Loss: 4830.2228
Epoch 9/200, Loss: 4813.5057
Epoch 10/200, Loss: 4799.2514
Epoch 11/200, Loss: 4783.1719
Epoch 12/200, Loss: 4781.5387
Epoch 13/200, Loss: 4779.7849
Epoch 14/200, Loss: 4778.7440
Epoch 15/200, Loss: 4776.6926
Epoch 16/200, Loss: 4775.4217
Epoch 17/200, Loss: 4775.1082
Epoch 18/200, Loss: 4774.9054
Epoch 19/200, Loss: 4774.8393
Epoch 20/200, Loss: 4774.8350
Epoch 21/200, Loss: 4774.5123
Epoch 22/200, Loss: 4774.5789
Epoch 23/200, Loss: 4774.8612
Epoch 24/200, Loss: 4774.4809
Epoch 25/200, Loss: 4774.4824
Epoch 26/200, Loss: 4774.2470
Epoch 27/200, Loss: 4774.5324
Epoch 28/200, Loss: 4774.5714
Epoch 29/200, Loss: 4774.6654
Epoch 30/200, Loss: 4774.4256
Epoch 31/200, Loss: 4774.6258
Epoch 32/200, Loss: 4774.2972
Epoch 33/200, Loss: 4774.5316
Epoch 34/200, Loss: 4774.9048
Epoch 35/200, Loss: 4774.5304
Epoch 36/200, Loss: 4774.5507
Epoch 37/200, Loss: 4774.3352
Epoch 38/200, Loss: 4774.4841
Epoch 39/200, Loss: 4774.8880
Epoch 40/200, Loss: 4774.8878
Epoch 41/200, Loss: 4774.5394
Epoch 42/200, Loss: 4774.2516
Epoch 43/200, Loss: 4774.6389
Epoch 44/200, Loss: 4774.3806
Epoch 45/200, Loss: 4774.7135
Epoch 46/200, Loss: 4774.3507
Epoch 47/200, Loss: 4774.3159
Epoch 48/200, Loss: 4774.9256
Epoch 49/200, Loss: 4774.5559
Epoch 50/200, Loss: 4774.4497
Epoch 51/200, Loss: 4774.5420
Epoch 52/200, Loss: 4774.6380
Epoch 53/200, Loss: 4774.9277
Epoch 54/200, Loss: 4774.4645
Epoch 55/200, Loss: 4774.5182
Epoch 56/200, Loss: 4774.7253
Epoch 57/200, Loss: 4774.8704
Epoch 58/200, Loss: 4774.3163
Epoch 59/200, Loss: 4774.6276
Epoch 60/200, Loss: 4774.5729
Epoch 61/200, Loss: 4774.5045
Epoch 62/200, Loss: 4774.2933
Epoch 63/200, Loss: 4774.3855
Epoch 64/200, Loss: 4774.4133
Epoch 65/200, Loss: 4774.2651
Epoch 66/200, Loss: 4774.3096
Epoch 67/200, Loss: 4774.3470
Epoch 68/200, Loss: 4774.4335
Epoch 69/200, Loss: 4774.6952
Epoch 70/200, Loss: 4774.1395
Epoch 71/200, Loss: 4774.2442
Epoch 72/200, Loss: 4774.5435
Epoch 73/200, Loss: 4774.6005
Epoch 74/200, Loss: 4774.7147
Epoch 75/200, Loss: 4774.4034
Epoch 76/200, Loss: 4774.6338
Epoch 77/200, Loss: 4774.4538
Epoch 78/200, Loss: 4774.3122
Epoch 79/200, Loss: 4774.3255
Epoch 80/200, Loss: 4774.6505
Epoch 81/200, Loss: 4774.4576
Epoch 82/200, Loss: 4774.3555
Epoch 83/200, Loss: 4774.5568
Epoch 84/200, Loss: 4774.5246
Epoch 85/200, Loss: 4774.3330
Epoch 86/200, Loss: 4774.3894
Epoch 87/200, Loss: 4774.6847
Epoch 88/200, Loss: 4774.8536
Epoch 89/200, Loss: 4774.1679
Epoch 90/200, Loss: 4774.1471
Epoch 91/200, Loss: 4774.6118
Epoch 92/200, Loss: 4774.7207
Epoch 93/200, Loss: 4774.7581
Epoch 94/200, Loss: 4774.5704
Epoch 95/200, Loss: 4774.3189
Epoch 96/200, Loss: 4774.5589
Epoch 97/200, Loss: 4774.5119
Epoch 98/200, Loss: 4774.6131
Epoch 99/200, Loss: 4774.5227
Epoch 100/200, Loss: 4774.5653
Epoch 101/200, Loss: 4774.2514
Epoch 102/200, Loss: 4774.5830
Epoch 103/200, Loss: 4774.4136
Epoch 104/200, Loss: 4774.4540
Epoch 105/200, Loss: 4774.9652
Epoch 106/200, Loss: 4774.5904
Epoch 107/200, Loss: 4774.2676
Epoch 108/200, Loss: 4774.6383
Epoch 109/200, Loss: 4774.6268
Epoch 110/200, Loss: 4774.4402
Epoch 111/200, Loss: 4774.4089
Epoch 112/200, Loss: 4774.4179
Epoch 113/200, Loss: 4774.3225
Epoch 114/200, Loss: 4774.1586
Epoch 115/200, Loss: 4774.4405
Epoch 116/200, Loss: 4774.4404
Epoch 117/200, Loss: 4774.1186
Epoch 118/200, Loss: 4774.5702
Epoch 119/200, Loss: 4774.6380
Epoch 120/200, Loss: 4774.6575
Epoch 121/200, Loss: 4774.8013
Epoch 122/200, Loss: 4774.5340
Epoch 123/200, Loss: 4774.6031
Epoch 124/200, Loss: 4774.4105
Epoch 125/200, Loss: 4774.4913
Epoch 126/200, Loss: 4774.5154
Epoch 127/200, Loss: 4774.3753
Epoch 128/200, Loss: 4774.6055
Epoch 129/200, Loss: 4774.6452
Epoch 130/200, Loss: 4774.4546
Epoch 131/200, Loss: 4774.4437
Epoch 132/200, Loss: 4774.5319
Epoch 133/200, Loss: 4774.4441
Epoch 134/200, Loss: 4774.3561
Epoch 135/200, Loss: 4774.4551
Epoch 136/200, Loss: 4774.4469
Epoch 137/200, Loss: 4774.5157
Epoch 138/200, Loss: 4774.3828
Epoch 139/200, Loss: 4774.2970
Epoch 140/200, Loss: 4774.4911
Epoch 141/200, Loss: 4774.3343
Epoch 142/200, Loss: 4774.2162
Epoch 143/200, Loss: 4774.4983
Epoch 144/200, Loss: 4774.6073
Epoch 145/200, Loss: 4774.6201
Epoch 146/200, Loss: 4774.4239
Epoch 147/200, Loss: 4774.5930
Epoch 148/200, Loss: 4774.2868
Epoch 149/200, Loss: 4774.5482
Epoch 150/200, Loss: 4774.1273
Epoch 151/200, Loss: 4774.7002
Epoch 152/200, Loss: 4774.6249
Epoch 153/200, Loss: 4774.1348
Epoch 154/200, Loss: 4775.1411
Epoch 155/200, Loss: 4774.4621
Epoch 156/200, Loss: 4774.6490
Epoch 157/200, Loss: 4774.6833
Epoch 158/200, Loss: 4774.6333
Epoch 159/200, Loss: 4774.4809
Epoch 160/200, Loss: 4774.4789
Epoch 161/200, Loss: 4774.4755
Epoch 162/200, Loss: 4774.6473
Epoch 163/200, Loss: 4774.7472
Epoch 164/200, Loss: 4774.2963
Epoch 165/200, Loss: 4774.3174
Epoch 166/200, Loss: 4774.4593
Epoch 167/200, Loss: 4774.8028
Epoch 168/200, Loss: 4774.5999
Epoch 169/200, Loss: 4774.1890
Epoch 170/200, Loss: 4774.5040
Epoch 171/200, Loss: 4774.4331
Epoch 172/200, Loss: 4774.4110
Epoch 173/200, Loss: 4774.5869
Epoch 174/200, Loss: 4774.6051
Epoch 175/200, Loss: 4774.4747
Epoch 176/200, Loss: 4774.4234
Epoch 177/200, Loss: 4774.4977
Epoch 178/200, Loss: 4774.5900
Epoch 179/200, Loss: 4774.4405
Epoch 180/200, Loss: 4774.3036
Epoch 181/200, Loss: 4774.4184
Epoch 182/200, Loss: 4774.4298
Epoch 183/200, Loss: 4774.8600
Epoch 184/200, Loss: 4774.1042
Epoch 185/200, Loss: 4774.3604
Epoch 186/200, Loss: 4774.3427
Epoch 187/200, Loss: 4774.4538
Epoch 188/200, Loss: 4774.2873
Epoch 189/200, Loss: 4774.2987
Epoch 190/200, Loss: 4774.4961
Epoch 191/200, Loss: 4774.6145
Epoch 192/200, Loss: 4774.3867
Epoch 193/200, Loss: 4774.1741
Epoch 194/200, Loss: 4774.6673
Epoch 195/200, Loss: 4774.9295
Epoch 196/200, Loss: 4774.4935
Epoch 197/200, Loss: 4774.5986
Epoch 198/200, Loss: 4774.6785
Epoch 199/200, Loss: 4774.6943
Epoch 200/200, Loss: 4774.9303
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 20.90% | Test Accuracy: 20.03%
Precision: 0.1383 | Recall: 0.2003 | F1-Score: 0.1465

========== Running Classification on Taiji_dataset_300.csv ==========

Processing Subject 1...
Top 32 discriminative features: [26 38 46 44 14 12 42 40 41 18 16 29 30 45 22 63 61 28 59 57 37 25  2  0
 49 51 10 20  6  4  9 33]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 131441
	# per Class in Train Dataset:
		Class 0: 1401
		Class 1: 3419
		Class 2: 3628
		Class 3: 5400
		Class 4: 4099
		Class 5: 3995
		Class 6: 5400
		Class 7: 3794
		Class 8: 3392
		Class 9: 5131
		Class 10: 4180
		Class 11: 1716
		Class 12: 1763
		Class 13: 2272
		Class 14: 2877
		Class 15: 2526
		Class 16: 1769
		Class 17: 1741
		Class 18: 2093
		Class 19: 2759
		Class 20: 4666
		Class 21: 5285
		Class 22: 5179
		Class 23: 4142
		Class 24: 2565
		Class 25: 4143
		Class 26: 2315
		Class 27: 3238
		Class 28: 2801
		Class 29: 3129
		Class 30: 3501
		Class 31: 3277
		Class 32: 2610
		Class 33: 2110
		Class 34: 5248
		Class 35: 3045
		Class 36: 3408
		Class 37: 4085
		Class 38: 3339
	# of Testing Samples: 16648
	# per Class in Test Dataset:
		Class 0: 600
		Class 1: 600
		Class 2: 317
		Class 3: 600
		Class 4: 407
		Class 5: 477
		Class 6: 600
		Class 7: 420
		Class 8: 497
		Class 9: 600
		Class 10: 530
		Class 11: 155
		Class 12: 177
		Class 13: 255
		Class 14: 400
		Class 15: 330
		Class 16: 138
		Class 17: 182
		Class 18: 250
		Class 19: 421
		Class 20: 600
		Class 21: 600
		Class 22: 600
		Class 23: 550
		Class 24: 338
		Class 25: 547
		Class 26: 239
		Class 27: 434
		Class 28: 375
		Class 29: 363
		Class 30: 414
		Class 31: 407
		Class 32: 364
		Class 33: 271
		Class 34: 600
		Class 35: 492
		Class 36: 503
		Class 37: 547
		Class 38: 448
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 67.49%
Precision: 0.6786 | Recall: 0.6749 | F1-Score: 0.6534


Running Deep Learning Classifier...
DataLoader: Training set - 2054 batches, Testing set - 261 batches
Epoch 1/200, Loss: 4588.5147
Epoch 2/200, Loss: 2887.8695
Epoch 3/200, Loss: 2388.7960
Epoch 4/200, Loss: 2094.0885
Epoch 5/200, Loss: 1892.9528
Epoch 6/200, Loss: 1751.2784
Epoch 7/200, Loss: 1729.6336
Epoch 8/200, Loss: 1713.5261
Epoch 9/200, Loss: 1698.0339
Epoch 10/200, Loss: 1683.3394
Epoch 11/200, Loss: 1669.9525
Epoch 12/200, Loss: 1667.5490
Epoch 13/200, Loss: 1665.9456
Epoch 14/200, Loss: 1664.4382
Epoch 15/200, Loss: 1662.9767
Epoch 16/200, Loss: 1661.3386
Epoch 17/200, Loss: 1661.1473
Epoch 18/200, Loss: 1660.9934
Epoch 19/200, Loss: 1660.8385
Epoch 20/200, Loss: 1660.6694
Epoch 21/200, Loss: 1660.4717
Epoch 22/200, Loss: 1660.5191
Epoch 23/200, Loss: 1660.5232
Epoch 24/200, Loss: 1660.4365
Epoch 25/200, Loss: 1660.4927
Epoch 26/200, Loss: 1660.5278
Epoch 27/200, Loss: 1660.4602
Epoch 28/200, Loss: 1660.5340
Epoch 29/200, Loss: 1660.5118
Epoch 30/200, Loss: 1660.4629
Epoch 31/200, Loss: 1660.4518
Epoch 32/200, Loss: 1660.4404
Epoch 33/200, Loss: 1660.4614
Epoch 34/200, Loss: 1660.4484
Epoch 35/200, Loss: 1660.4972
Epoch 36/200, Loss: 1660.4921
Epoch 37/200, Loss: 1660.4891
Epoch 38/200, Loss: 1660.5223
Epoch 39/200, Loss: 1660.4297
Epoch 40/200, Loss: 1660.4835
Epoch 41/200, Loss: 1660.4699
Epoch 42/200, Loss: 1660.4841
Epoch 43/200, Loss: 1660.4154
Epoch 44/200, Loss: 1660.4835
Epoch 45/200, Loss: 1660.4723
Epoch 46/200, Loss: 1660.4855
Epoch 47/200, Loss: 1660.4770
Epoch 48/200, Loss: 1660.4990
Epoch 49/200, Loss: 1660.4956
Epoch 50/200, Loss: 1660.4600
Epoch 51/200, Loss: 1660.4550
Epoch 52/200, Loss: 1660.4462
Epoch 53/200, Loss: 1660.4751
Epoch 54/200, Loss: 1660.4636
Epoch 55/200, Loss: 1660.4366
Epoch 56/200, Loss: 1660.4984
Epoch 57/200, Loss: 1660.5267
Epoch 58/200, Loss: 1660.4354
Epoch 59/200, Loss: 1660.4201
Epoch 60/200, Loss: 1660.5006
Epoch 61/200, Loss: 1660.4659
Epoch 62/200, Loss: 1660.4565
Epoch 63/200, Loss: 1660.4716
Epoch 64/200, Loss: 1660.4571
Epoch 65/200, Loss: 1660.4611
Epoch 66/200, Loss: 1660.4852
Epoch 67/200, Loss: 1660.5050
Epoch 68/200, Loss: 1660.4574
Epoch 69/200, Loss: 1660.4702
Epoch 70/200, Loss: 1660.4358
Epoch 71/200, Loss: 1660.4840
Epoch 72/200, Loss: 1660.4864
Epoch 73/200, Loss: 1660.5265
Epoch 74/200, Loss: 1660.4830
Epoch 75/200, Loss: 1660.5034
Epoch 76/200, Loss: 1660.4697
Epoch 77/200, Loss: 1660.5053
Epoch 78/200, Loss: 1660.4515
Epoch 79/200, Loss: 1660.4312
Epoch 80/200, Loss: 1660.4426
Epoch 81/200, Loss: 1660.4911
Epoch 82/200, Loss: 1660.5183
Epoch 83/200, Loss: 1660.4801
Epoch 84/200, Loss: 1660.4633
Epoch 85/200, Loss: 1660.4407
Epoch 86/200, Loss: 1660.4759
Epoch 87/200, Loss: 1660.5207
Epoch 88/200, Loss: 1660.4479
Epoch 89/200, Loss: 1660.4579
Epoch 90/200, Loss: 1660.4490
Epoch 91/200, Loss: 1660.5120
Epoch 92/200, Loss: 1660.5440
Epoch 93/200, Loss: 1660.4752
Epoch 94/200, Loss: 1660.4948
Epoch 95/200, Loss: 1660.5650
Epoch 96/200, Loss: 1660.3941
Epoch 97/200, Loss: 1660.4638
Epoch 98/200, Loss: 1660.4713
Epoch 99/200, Loss: 1660.5201
Epoch 100/200, Loss: 1660.4867
Epoch 101/200, Loss: 1660.5023
Epoch 102/200, Loss: 1660.5218
Epoch 103/200, Loss: 1660.4864
Epoch 104/200, Loss: 1660.4817
Epoch 105/200, Loss: 1660.4713
Epoch 106/200, Loss: 1660.4065
Epoch 107/200, Loss: 1660.4643
Epoch 108/200, Loss: 1660.4430
Epoch 109/200, Loss: 1660.4970
Epoch 110/200, Loss: 1660.4788
Epoch 111/200, Loss: 1660.5050
Epoch 112/200, Loss: 1660.4319
Epoch 113/200, Loss: 1660.4716
Epoch 114/200, Loss: 1660.5087
Epoch 115/200, Loss: 1660.5081
Epoch 116/200, Loss: 1660.4465
Epoch 117/200, Loss: 1660.4894
Epoch 118/200, Loss: 1660.4962
Epoch 119/200, Loss: 1660.4600
Epoch 120/200, Loss: 1660.4831
Epoch 121/200, Loss: 1660.4694
Epoch 122/200, Loss: 1660.5124
Epoch 123/200, Loss: 1660.4560
Epoch 124/200, Loss: 1660.4469
Epoch 125/200, Loss: 1660.4807
Epoch 126/200, Loss: 1660.5069
Epoch 127/200, Loss: 1660.4251
Epoch 128/200, Loss: 1660.4284
Epoch 129/200, Loss: 1660.4772
Epoch 130/200, Loss: 1660.4695
Epoch 131/200, Loss: 1660.4775
Epoch 132/200, Loss: 1660.4990
Epoch 133/200, Loss: 1660.4290
Epoch 134/200, Loss: 1660.4875
Epoch 135/200, Loss: 1660.4154
Epoch 136/200, Loss: 1660.5028
Epoch 137/200, Loss: 1660.5148
Epoch 138/200, Loss: 1660.5324
Epoch 139/200, Loss: 1660.5092
Epoch 140/200, Loss: 1660.4207
Epoch 141/200, Loss: 1660.4580
Epoch 142/200, Loss: 1660.4682
Epoch 143/200, Loss: 1660.5027
Epoch 144/200, Loss: 1660.4287
Epoch 145/200, Loss: 1660.4504
Epoch 146/200, Loss: 1660.4934
Epoch 147/200, Loss: 1660.4429
Epoch 148/200, Loss: 1660.4424
Epoch 149/200, Loss: 1660.5144
Epoch 150/200, Loss: 1660.4855
Epoch 151/200, Loss: 1660.4824
Epoch 152/200, Loss: 1660.4962
Epoch 153/200, Loss: 1660.4849
Epoch 154/200, Loss: 1660.4194
Epoch 155/200, Loss: 1660.4995
Epoch 156/200, Loss: 1660.4560
Epoch 157/200, Loss: 1660.4506
Epoch 158/200, Loss: 1660.4239
Epoch 159/200, Loss: 1660.4870
Epoch 160/200, Loss: 1660.4721
Epoch 161/200, Loss: 1660.4714
Epoch 162/200, Loss: 1660.4737
Epoch 163/200, Loss: 1660.4803
Epoch 164/200, Loss: 1660.5185
Epoch 165/200, Loss: 1660.4107
Epoch 166/200, Loss: 1660.5352
Epoch 167/200, Loss: 1660.4329
Epoch 168/200, Loss: 1660.4554
Epoch 169/200, Loss: 1660.4207
Epoch 170/200, Loss: 1660.4902
Epoch 171/200, Loss: 1660.4812
Epoch 172/200, Loss: 1660.4277
Epoch 173/200, Loss: 1660.4534
Epoch 174/200, Loss: 1660.4819
Epoch 175/200, Loss: 1660.4138
Epoch 176/200, Loss: 1660.4243
Epoch 177/200, Loss: 1660.4924
Epoch 178/200, Loss: 1660.4765
Epoch 179/200, Loss: 1660.4826
Epoch 180/200, Loss: 1660.4951
Epoch 181/200, Loss: 1660.4278
Epoch 182/200, Loss: 1660.4215
Epoch 183/200, Loss: 1660.4812
Epoch 184/200, Loss: 1660.4362
Epoch 185/200, Loss: 1660.5109
Epoch 186/200, Loss: 1660.4660
Epoch 187/200, Loss: 1660.4315
Epoch 188/200, Loss: 1660.4553
Epoch 189/200, Loss: 1660.4593
Epoch 190/200, Loss: 1660.4775
Epoch 191/200, Loss: 1660.4688
Epoch 192/200, Loss: 1660.4779
Epoch 193/200, Loss: 1660.4663
Epoch 194/200, Loss: 1660.4984
Epoch 195/200, Loss: 1660.4522
Epoch 196/200, Loss: 1660.4957
Epoch 197/200, Loss: 1660.4631
Epoch 198/200, Loss: 1660.5247
Epoch 199/200, Loss: 1660.4500
Epoch 200/200, Loss: 1660.4271
Train Accuracy: 74.14% | Test Accuracy: 54.13%
Precision: 0.5294 | Recall: 0.5413 | F1-Score: 0.5208
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 15.10%
Precision: 0.1521 | Recall: 0.1510 | F1-Score: 0.1485


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2054 batches, Testing set - 261 batches
Epoch 1/200, Loss: 7205.0250
Epoch 2/200, Loss: 6453.7543
Epoch 3/200, Loss: 6294.4154
Epoch 4/200, Loss: 6181.8089
Epoch 5/200, Loss: 6090.3217
Epoch 6/200, Loss: 5989.6852
Epoch 7/200, Loss: 5977.2965
Epoch 8/200, Loss: 5967.9322
Epoch 9/200, Loss: 5955.2988
Epoch 10/200, Loss: 5944.4895
Epoch 11/200, Loss: 5933.5422
Epoch 12/200, Loss: 5931.6342
Epoch 13/200, Loss: 5930.5980
Epoch 14/200, Loss: 5929.7307
Epoch 15/200, Loss: 5928.7148
Epoch 16/200, Loss: 5927.6152
Epoch 17/200, Loss: 5927.3245
Epoch 18/200, Loss: 5927.1748
Epoch 19/200, Loss: 5927.0556
Epoch 20/200, Loss: 5927.0088
Epoch 21/200, Loss: 5926.8507
Epoch 22/200, Loss: 5926.8269
Epoch 23/200, Loss: 5926.8209
Epoch 24/200, Loss: 5926.8041
Epoch 25/200, Loss: 5926.8007
Epoch 26/200, Loss: 5926.7708
Epoch 27/200, Loss: 5926.7497
Epoch 28/200, Loss: 5926.7123
Epoch 29/200, Loss: 5926.8145
Epoch 30/200, Loss: 5926.7662
Epoch 31/200, Loss: 5926.7498
Epoch 32/200, Loss: 5926.7928
Epoch 33/200, Loss: 5926.8158
Epoch 34/200, Loss: 5926.8039
Epoch 35/200, Loss: 5926.7747
Epoch 36/200, Loss: 5926.7621
Epoch 37/200, Loss: 5926.7581
Epoch 38/200, Loss: 5926.8311
Epoch 39/200, Loss: 5926.7917
Epoch 40/200, Loss: 5926.8184
Epoch 41/200, Loss: 5926.7706
Epoch 42/200, Loss: 5926.7631
Epoch 43/200, Loss: 5926.8286
Epoch 44/200, Loss: 5926.8062
Epoch 45/200, Loss: 5926.7723
Epoch 46/200, Loss: 5926.7507
Epoch 47/200, Loss: 5926.8277
Epoch 48/200, Loss: 5926.7632
Epoch 49/200, Loss: 5926.7702
Epoch 50/200, Loss: 5926.8305
Epoch 51/200, Loss: 5926.8297
Epoch 52/200, Loss: 5926.7818
Epoch 53/200, Loss: 5926.7967
Epoch 54/200, Loss: 5926.8353
Epoch 55/200, Loss: 5926.7850
Epoch 56/200, Loss: 5926.7802
Epoch 57/200, Loss: 5926.7738
Epoch 58/200, Loss: 5926.7767
Epoch 59/200, Loss: 5926.7988
Epoch 60/200, Loss: 5926.7822
Epoch 61/200, Loss: 5926.7802
Epoch 62/200, Loss: 5926.8648
Epoch 63/200, Loss: 5926.7291
Epoch 64/200, Loss: 5926.7912
Epoch 65/200, Loss: 5926.7755
Epoch 66/200, Loss: 5926.7446
Epoch 67/200, Loss: 5926.7949
Epoch 68/200, Loss: 5926.8196
Epoch 69/200, Loss: 5926.8106
Epoch 70/200, Loss: 5926.7621
Epoch 71/200, Loss: 5926.7629
Epoch 72/200, Loss: 5926.7880
Epoch 73/200, Loss: 5926.7984
Epoch 74/200, Loss: 5926.7889
Epoch 75/200, Loss: 5926.7932
Epoch 76/200, Loss: 5926.8141
Epoch 77/200, Loss: 5926.7523
Epoch 78/200, Loss: 5926.7888
Epoch 79/200, Loss: 5926.7878
Epoch 80/200, Loss: 5926.8012
Epoch 81/200, Loss: 5926.8209
Epoch 82/200, Loss: 5926.7859
Epoch 83/200, Loss: 5926.7840
Epoch 84/200, Loss: 5926.7662
Epoch 85/200, Loss: 5926.7720
Epoch 86/200, Loss: 5926.7798
Epoch 87/200, Loss: 5926.7661
Epoch 88/200, Loss: 5926.8034
Epoch 89/200, Loss: 5926.7640
Epoch 90/200, Loss: 5926.7901
Epoch 91/200, Loss: 5926.7790
Epoch 92/200, Loss: 5926.7616
Epoch 93/200, Loss: 5926.7962
Epoch 94/200, Loss: 5926.7794
Epoch 95/200, Loss: 5926.7674
Epoch 96/200, Loss: 5926.7657
Epoch 97/200, Loss: 5926.7522
Epoch 98/200, Loss: 5926.7706
Epoch 99/200, Loss: 5926.7554
Epoch 100/200, Loss: 5926.7851
Epoch 101/200, Loss: 5926.8377
Epoch 102/200, Loss: 5926.7706
Epoch 103/200, Loss: 5926.7480
Epoch 104/200, Loss: 5926.8101
Epoch 105/200, Loss: 5926.7119
Epoch 106/200, Loss: 5926.7849
Epoch 107/200, Loss: 5926.7692
Epoch 108/200, Loss: 5926.8218
Epoch 109/200, Loss: 5926.7619
Epoch 110/200, Loss: 5926.7858
Epoch 111/200, Loss: 5926.7722
Epoch 112/200, Loss: 5926.8028
Epoch 113/200, Loss: 5926.7631
Epoch 114/200, Loss: 5926.7822
Epoch 115/200, Loss: 5926.8082
Epoch 116/200, Loss: 5926.7626
Epoch 117/200, Loss: 5926.8063
Epoch 118/200, Loss: 5926.7704
Epoch 119/200, Loss: 5926.7972
Epoch 120/200, Loss: 5926.7976
Epoch 121/200, Loss: 5926.7587
Epoch 122/200, Loss: 5926.7906
Epoch 123/200, Loss: 5926.7735
Epoch 124/200, Loss: 5926.7537
Epoch 125/200, Loss: 5926.7969
Epoch 126/200, Loss: 5926.8340
Epoch 127/200, Loss: 5926.8100
Epoch 128/200, Loss: 5926.7645
Epoch 129/200, Loss: 5926.7424
Epoch 130/200, Loss: 5926.7963
Epoch 131/200, Loss: 5926.8038
Epoch 132/200, Loss: 5926.7647
Epoch 133/200, Loss: 5926.7934
Epoch 134/200, Loss: 5926.8152
Epoch 135/200, Loss: 5926.7527
Epoch 136/200, Loss: 5926.8156
Epoch 137/200, Loss: 5926.8283
Epoch 138/200, Loss: 5926.7705
Epoch 139/200, Loss: 5926.7548
Epoch 140/200, Loss: 5926.7961
Epoch 141/200, Loss: 5926.7751
Epoch 142/200, Loss: 5926.7818
Epoch 143/200, Loss: 5926.7745
Epoch 144/200, Loss: 5926.7494
Epoch 145/200, Loss: 5926.7855
Epoch 146/200, Loss: 5926.7681
Epoch 147/200, Loss: 5926.7810
Epoch 148/200, Loss: 5926.7504
Epoch 149/200, Loss: 5926.8329
Epoch 150/200, Loss: 5926.7866
Epoch 151/200, Loss: 5926.7975
Epoch 152/200, Loss: 5926.7746
Epoch 153/200, Loss: 5926.7922
Epoch 154/200, Loss: 5926.7792
Epoch 155/200, Loss: 5926.7771
Epoch 156/200, Loss: 5926.7660
Epoch 157/200, Loss: 5926.7935
Epoch 158/200, Loss: 5926.7595
Epoch 159/200, Loss: 5926.8024
Epoch 160/200, Loss: 5926.8365
Epoch 161/200, Loss: 5926.7640
Epoch 162/200, Loss: 5926.7557
Epoch 163/200, Loss: 5926.8008
Epoch 164/200, Loss: 5926.8076
Epoch 165/200, Loss: 5926.7635
Epoch 166/200, Loss: 5926.8063
Epoch 167/200, Loss: 5926.7618
Epoch 168/200, Loss: 5926.7313
Epoch 169/200, Loss: 5926.7678
Epoch 170/200, Loss: 5926.7900
Epoch 171/200, Loss: 5926.8007
Epoch 172/200, Loss: 5926.7628
Epoch 173/200, Loss: 5926.7612
Epoch 174/200, Loss: 5926.7611
Epoch 175/200, Loss: 5926.7593
Epoch 176/200, Loss: 5926.8018
Epoch 177/200, Loss: 5926.7931
Epoch 178/200, Loss: 5926.7694
Epoch 179/200, Loss: 5926.7846
Epoch 180/200, Loss: 5926.7867
Epoch 181/200, Loss: 5926.7441
Epoch 182/200, Loss: 5926.7746
Epoch 183/200, Loss: 5926.7540
Epoch 184/200, Loss: 5926.7656
Epoch 185/200, Loss: 5926.7541
Epoch 186/200, Loss: 5926.8230
Epoch 187/200, Loss: 5926.7562
Epoch 188/200, Loss: 5926.7817
Epoch 189/200, Loss: 5926.8035
Epoch 190/200, Loss: 5926.7858
Epoch 191/200, Loss: 5926.7457
Epoch 192/200, Loss: 5926.7752
Epoch 193/200, Loss: 5926.8033
Epoch 194/200, Loss: 5926.7700
Epoch 195/200, Loss: 5926.8064
Epoch 196/200, Loss: 5926.7772
Epoch 197/200, Loss: 5926.7572
Epoch 198/200, Loss: 5926.7986
Epoch 199/200, Loss: 5926.7994
Epoch 200/200, Loss: 5926.8062
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 13.68% | Test Accuracy: 13.16%
Precision: 0.0635 | Recall: 0.1316 | F1-Score: 0.0716

Processing Subject 2...
Top 32 discriminative features: [26 38 46 44 14 12 41 18 42 16 40 22 29 30 45 63 61  2  0 49 51 28 59 57
 21 20  9 10 37 25 33  6]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 134318
	# per Class in Train Dataset:
		Class 0: 1984
		Class 1: 3626
		Class 2: 3531
		Class 3: 5400
		Class 4: 4041
		Class 5: 4082
		Class 6: 5400
		Class 7: 3774
		Class 8: 3541
		Class 9: 5151
		Class 10: 4247
		Class 11: 1663
		Class 12: 1752
		Class 13: 2319
		Class 14: 3007
		Class 15: 2580
		Class 16: 1708
		Class 17: 1737
		Class 18: 2134
		Class 19: 2891
		Class 20: 4736
		Class 21: 5298
		Class 22: 5179
		Class 23: 4270
		Class 24: 2653
		Class 25: 4278
		Class 26: 2269
		Class 27: 3377
		Class 28: 2943
		Class 29: 3175
		Class 30: 3620
		Class 31: 3349
		Class 32: 2703
		Class 33: 2119
		Class 34: 5248
		Class 35: 3247
		Class 36: 3586
		Class 37: 4322
		Class 38: 3378
	# of Testing Samples: 13771
	# per Class in Test Dataset:
		Class 0: 17
		Class 1: 393
		Class 2: 414
		Class 3: 600
		Class 4: 465
		Class 5: 390
		Class 6: 600
		Class 7: 440
		Class 8: 348
		Class 9: 580
		Class 10: 463
		Class 11: 208
		Class 12: 188
		Class 13: 208
		Class 14: 270
		Class 15: 276
		Class 16: 199
		Class 17: 186
		Class 18: 209
		Class 19: 289
		Class 20: 530
		Class 21: 587
		Class 22: 600
		Class 23: 422
		Class 24: 250
		Class 25: 412
		Class 26: 285
		Class 27: 295
		Class 28: 233
		Class 29: 317
		Class 30: 295
		Class 31: 335
		Class 32: 271
		Class 33: 262
		Class 34: 600
		Class 35: 290
		Class 36: 325
		Class 37: 310
		Class 38: 409
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 55.73%
Precision: 0.5949 | Recall: 0.5573 | F1-Score: 0.5403


Running Deep Learning Classifier...
DataLoader: Training set - 2099 batches, Testing set - 216 batches
Epoch 1/200, Loss: 4531.7376
Epoch 2/200, Loss: 2859.5642
Epoch 3/200, Loss: 2358.0815
Epoch 4/200, Loss: 2049.2306
Epoch 5/200, Loss: 1834.4694
Epoch 6/200, Loss: 1680.8446
Epoch 7/200, Loss: 1658.8153
Epoch 8/200, Loss: 1641.4063
Epoch 9/200, Loss: 1624.1357
Epoch 10/200, Loss: 1608.3411
Epoch 11/200, Loss: 1593.3910
Epoch 12/200, Loss: 1590.9333
Epoch 13/200, Loss: 1589.1937
Epoch 14/200, Loss: 1587.4443
Epoch 15/200, Loss: 1585.8072
Epoch 16/200, Loss: 1584.2020
Epoch 17/200, Loss: 1583.8448
Epoch 18/200, Loss: 1583.7410
Epoch 19/200, Loss: 1583.5927
Epoch 20/200, Loss: 1583.4038
Epoch 21/200, Loss: 1583.2343
Epoch 22/200, Loss: 1583.2405
Epoch 23/200, Loss: 1583.1809
Epoch 24/200, Loss: 1583.1930
Epoch 25/200, Loss: 1583.1689
Epoch 26/200, Loss: 1583.1156
Epoch 27/200, Loss: 1583.1773
Epoch 28/200, Loss: 1583.1627
Epoch 29/200, Loss: 1583.1690
Epoch 30/200, Loss: 1583.1419
Epoch 31/200, Loss: 1583.1585
Epoch 32/200, Loss: 1583.1378
Epoch 33/200, Loss: 1583.1171
Epoch 34/200, Loss: 1583.1318
Epoch 35/200, Loss: 1583.1399
Epoch 36/200, Loss: 1583.1462
Epoch 37/200, Loss: 1583.1576
Epoch 38/200, Loss: 1583.1793
Epoch 39/200, Loss: 1583.1616
Epoch 40/200, Loss: 1583.1744
Epoch 41/200, Loss: 1583.1003
Epoch 42/200, Loss: 1583.1856
Epoch 43/200, Loss: 1583.1341
Epoch 44/200, Loss: 1583.1742
Epoch 45/200, Loss: 1583.1720
Epoch 46/200, Loss: 1583.0955
Epoch 47/200, Loss: 1583.2176
Epoch 48/200, Loss: 1583.1175
Epoch 49/200, Loss: 1583.1568
Epoch 50/200, Loss: 1583.1782
Epoch 51/200, Loss: 1583.2037
Epoch 52/200, Loss: 1583.2101
Epoch 53/200, Loss: 1583.1298
Epoch 54/200, Loss: 1583.1280
Epoch 55/200, Loss: 1583.1066
Epoch 56/200, Loss: 1583.2080
Epoch 57/200, Loss: 1583.1371
Epoch 58/200, Loss: 1583.0435
Epoch 59/200, Loss: 1583.1753
Epoch 60/200, Loss: 1583.1812
Epoch 61/200, Loss: 1583.1062
Epoch 62/200, Loss: 1583.1947
Epoch 63/200, Loss: 1583.1514
Epoch 64/200, Loss: 1583.1439
Epoch 65/200, Loss: 1583.1192
Epoch 66/200, Loss: 1583.1858
Epoch 67/200, Loss: 1583.1406
Epoch 68/200, Loss: 1583.0905
Epoch 69/200, Loss: 1583.1693
Epoch 70/200, Loss: 1583.1120
Epoch 71/200, Loss: 1583.1541
Epoch 72/200, Loss: 1583.1509
Epoch 73/200, Loss: 1583.1511
Epoch 74/200, Loss: 1583.1111
Epoch 75/200, Loss: 1583.0971
Epoch 76/200, Loss: 1583.1424
Epoch 77/200, Loss: 1583.1574
Epoch 78/200, Loss: 1583.1107
Epoch 79/200, Loss: 1583.2043
Epoch 80/200, Loss: 1583.1193
Epoch 81/200, Loss: 1583.1552
Epoch 82/200, Loss: 1583.1310
Epoch 83/200, Loss: 1583.1748
Epoch 84/200, Loss: 1583.1410
Epoch 85/200, Loss: 1583.1564
Epoch 86/200, Loss: 1583.1181
Epoch 87/200, Loss: 1583.1703
Epoch 88/200, Loss: 1583.1514
Epoch 89/200, Loss: 1583.0876
Epoch 90/200, Loss: 1583.1196
Epoch 91/200, Loss: 1583.1231
Epoch 92/200, Loss: 1583.2025
Epoch 93/200, Loss: 1583.1096
Epoch 94/200, Loss: 1583.2134
Epoch 95/200, Loss: 1583.1610
Epoch 96/200, Loss: 1583.1872
Epoch 97/200, Loss: 1583.1707
Epoch 98/200, Loss: 1583.1758
Epoch 99/200, Loss: 1583.1445
Epoch 100/200, Loss: 1583.1207
Epoch 101/200, Loss: 1583.1328
Epoch 102/200, Loss: 1583.1491
Epoch 103/200, Loss: 1583.0986
Epoch 104/200, Loss: 1583.1108
Epoch 105/200, Loss: 1583.0805
Epoch 106/200, Loss: 1583.1609
Epoch 107/200, Loss: 1583.1428
Epoch 108/200, Loss: 1583.1558
Epoch 109/200, Loss: 1583.2044
Epoch 110/200, Loss: 1583.1263
Epoch 111/200, Loss: 1583.2025
Epoch 112/200, Loss: 1583.1420
Epoch 113/200, Loss: 1583.1792
Epoch 114/200, Loss: 1583.2621
Epoch 115/200, Loss: 1583.1070
Epoch 116/200, Loss: 1583.1017
Epoch 117/200, Loss: 1583.1219
Epoch 118/200, Loss: 1583.1824
Epoch 119/200, Loss: 1583.0961
Epoch 120/200, Loss: 1583.1032
Epoch 121/200, Loss: 1583.1540
Epoch 122/200, Loss: 1583.1057
Epoch 123/200, Loss: 1583.1101
Epoch 124/200, Loss: 1583.1556
Epoch 125/200, Loss: 1583.1513
Epoch 126/200, Loss: 1583.1033
Epoch 127/200, Loss: 1583.1699
Epoch 128/200, Loss: 1583.1306
Epoch 129/200, Loss: 1583.1832
Epoch 130/200, Loss: 1583.1713
Epoch 131/200, Loss: 1583.1528
Epoch 132/200, Loss: 1583.1835
Epoch 133/200, Loss: 1583.1216
Epoch 134/200, Loss: 1583.1233
Epoch 135/200, Loss: 1583.1844
Epoch 136/200, Loss: 1583.1314
Epoch 137/200, Loss: 1583.1054
Epoch 138/200, Loss: 1583.1862
Epoch 139/200, Loss: 1583.1405
Epoch 140/200, Loss: 1583.1082
Epoch 141/200, Loss: 1583.2200
Epoch 142/200, Loss: 1583.1786
Epoch 143/200, Loss: 1583.1091
Epoch 144/200, Loss: 1583.1760
Epoch 145/200, Loss: 1583.2256
Epoch 146/200, Loss: 1583.1292
Epoch 147/200, Loss: 1583.1256
Epoch 148/200, Loss: 1583.1254
Epoch 149/200, Loss: 1583.1471
Epoch 150/200, Loss: 1583.1437
Epoch 151/200, Loss: 1583.1131
Epoch 152/200, Loss: 1583.1897
Epoch 153/200, Loss: 1583.1251
Epoch 154/200, Loss: 1583.1406
Epoch 155/200, Loss: 1583.1927
Epoch 156/200, Loss: 1583.1260
Epoch 157/200, Loss: 1583.1589
Epoch 158/200, Loss: 1583.1772
Epoch 159/200, Loss: 1583.2091
Epoch 160/200, Loss: 1583.1070
Epoch 161/200, Loss: 1583.1027
Epoch 162/200, Loss: 1583.1121
Epoch 163/200, Loss: 1583.1248
Epoch 164/200, Loss: 1583.2475
Epoch 165/200, Loss: 1583.1271
Epoch 166/200, Loss: 1583.1195
Epoch 167/200, Loss: 1583.1395
Epoch 168/200, Loss: 1583.1855
Epoch 169/200, Loss: 1583.1497
Epoch 170/200, Loss: 1583.1864
Epoch 171/200, Loss: 1583.1476
Epoch 172/200, Loss: 1583.1058
Epoch 173/200, Loss: 1583.1397
Epoch 174/200, Loss: 1583.1185
Epoch 175/200, Loss: 1583.1355
Epoch 176/200, Loss: 1583.0654
Epoch 177/200, Loss: 1583.2138
Epoch 178/200, Loss: 1583.1163
Epoch 179/200, Loss: 1583.1410
Epoch 180/200, Loss: 1583.1558
Epoch 181/200, Loss: 1583.1062
Epoch 182/200, Loss: 1583.1342
Epoch 183/200, Loss: 1583.1156
Epoch 184/200, Loss: 1583.2009
Epoch 185/200, Loss: 1583.2086
Epoch 186/200, Loss: 1583.1586
Epoch 187/200, Loss: 1583.1321
Epoch 188/200, Loss: 1583.1927
Epoch 189/200, Loss: 1583.1549
Epoch 190/200, Loss: 1583.1831
Epoch 191/200, Loss: 1583.1047
Epoch 192/200, Loss: 1583.1658
Epoch 193/200, Loss: 1583.1032
Epoch 194/200, Loss: 1583.1649
Epoch 195/200, Loss: 1583.1316
Epoch 196/200, Loss: 1583.1740
Epoch 197/200, Loss: 1583.1413
Epoch 198/200, Loss: 1583.2187
Epoch 199/200, Loss: 1583.1454
Epoch 200/200, Loss: 1583.1864
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 76.24% | Test Accuracy: 50.72%
Precision: 0.5259 | Recall: 0.5072 | F1-Score: 0.4944
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 14.24%
Precision: 0.1502 | Recall: 0.1424 | F1-Score: 0.1438


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2099 batches, Testing set - 216 batches
Epoch 1/200, Loss: 7209.7625
Epoch 2/200, Loss: 6612.9099
Epoch 3/200, Loss: 6500.8177
Epoch 4/200, Loss: 6439.8817
Epoch 5/200, Loss: 6390.8517
Epoch 6/200, Loss: 6311.8026
Epoch 7/200, Loss: 6303.7250
Epoch 8/200, Loss: 6296.5230
Epoch 9/200, Loss: 6289.3145
Epoch 10/200, Loss: 6281.8799
Epoch 11/200, Loss: 6274.0523
Epoch 12/200, Loss: 6272.8027
Epoch 13/200, Loss: 6272.0932
Epoch 14/200, Loss: 6271.4327
Epoch 15/200, Loss: 6270.7151
Epoch 16/200, Loss: 6269.8072
Epoch 17/200, Loss: 6269.6853
Epoch 18/200, Loss: 6269.6035
Epoch 19/200, Loss: 6269.5312
Epoch 20/200, Loss: 6269.4588
Epoch 21/200, Loss: 6269.3364
Epoch 22/200, Loss: 6269.3734
Epoch 23/200, Loss: 6269.3487
Epoch 24/200, Loss: 6269.3021
Epoch 25/200, Loss: 6269.3239
Epoch 26/200, Loss: 6269.2740
Epoch 27/200, Loss: 6269.3687
Epoch 28/200, Loss: 6269.2789
Epoch 29/200, Loss: 6269.3396
Epoch 30/200, Loss: 6269.2786
Epoch 31/200, Loss: 6269.3198
Epoch 32/200, Loss: 6269.2890
Epoch 33/200, Loss: 6269.3334
Epoch 34/200, Loss: 6269.3378
Epoch 35/200, Loss: 6269.2639
Epoch 36/200, Loss: 6269.2552
Epoch 37/200, Loss: 6269.2916
Epoch 38/200, Loss: 6269.3098
Epoch 39/200, Loss: 6269.3306
Epoch 40/200, Loss: 6269.3515
Epoch 41/200, Loss: 6269.3159
Epoch 42/200, Loss: 6269.3258
Epoch 43/200, Loss: 6269.3110
Epoch 44/200, Loss: 6269.2739
Epoch 45/200, Loss: 6269.2849
Epoch 46/200, Loss: 6269.3127
Epoch 47/200, Loss: 6269.3120
Epoch 48/200, Loss: 6269.3144
Epoch 49/200, Loss: 6269.2554
Epoch 50/200, Loss: 6269.3529
Epoch 51/200, Loss: 6269.2728
Epoch 52/200, Loss: 6269.3064
Epoch 53/200, Loss: 6269.3420
Epoch 54/200, Loss: 6269.3341
Epoch 55/200, Loss: 6269.2877
Epoch 56/200, Loss: 6269.3412
Epoch 57/200, Loss: 6269.2719
Epoch 58/200, Loss: 6269.3441
Epoch 59/200, Loss: 6269.2699
Epoch 60/200, Loss: 6269.2794
Epoch 61/200, Loss: 6269.2819
Epoch 62/200, Loss: 6269.3622
Epoch 63/200, Loss: 6269.3188
Epoch 64/200, Loss: 6269.2967
Epoch 65/200, Loss: 6269.3240
Epoch 66/200, Loss: 6269.2815
Epoch 67/200, Loss: 6269.3412
Epoch 68/200, Loss: 6269.3127
Epoch 69/200, Loss: 6269.3376
Epoch 70/200, Loss: 6269.2979
Epoch 71/200, Loss: 6269.3511
Epoch 72/200, Loss: 6269.3049
Epoch 73/200, Loss: 6269.3269
Epoch 74/200, Loss: 6269.3644
Epoch 75/200, Loss: 6269.2907
Epoch 76/200, Loss: 6269.3231
Epoch 77/200, Loss: 6269.3658
Epoch 78/200, Loss: 6269.3118
Epoch 79/200, Loss: 6269.3145
Epoch 80/200, Loss: 6269.2862
Epoch 81/200, Loss: 6269.3105
Epoch 82/200, Loss: 6269.2970
Epoch 83/200, Loss: 6269.2866
Epoch 84/200, Loss: 6269.3189
Epoch 85/200, Loss: 6269.3457
Epoch 86/200, Loss: 6269.3298
Epoch 87/200, Loss: 6269.3085
Epoch 88/200, Loss: 6269.3295
Epoch 89/200, Loss: 6269.3430
Epoch 90/200, Loss: 6269.3012
Epoch 91/200, Loss: 6269.2749
Epoch 92/200, Loss: 6269.3195
Epoch 93/200, Loss: 6269.3234
Epoch 94/200, Loss: 6269.2819
Epoch 95/200, Loss: 6269.3095
Epoch 96/200, Loss: 6269.3289
Epoch 97/200, Loss: 6269.3415
Epoch 98/200, Loss: 6269.3209
Epoch 99/200, Loss: 6269.2961
Epoch 100/200, Loss: 6269.3664
Epoch 101/200, Loss: 6269.3036
Epoch 102/200, Loss: 6269.3109
Epoch 103/200, Loss: 6269.3507
Epoch 104/200, Loss: 6269.3231
Epoch 105/200, Loss: 6269.2826
Epoch 106/200, Loss: 6269.3980
Epoch 107/200, Loss: 6269.3104
Epoch 108/200, Loss: 6269.3518
Epoch 109/200, Loss: 6269.3478
Epoch 110/200, Loss: 6269.2794
Epoch 111/200, Loss: 6269.2827
Epoch 112/200, Loss: 6269.2924
Epoch 113/200, Loss: 6269.3558
Epoch 114/200, Loss: 6269.3854
Epoch 115/200, Loss: 6269.2705
Epoch 116/200, Loss: 6269.3265
Epoch 117/200, Loss: 6269.3567
Epoch 118/200, Loss: 6269.2730
Epoch 119/200, Loss: 6269.3222
Epoch 120/200, Loss: 6269.2854
Epoch 121/200, Loss: 6269.3409
Epoch 122/200, Loss: 6269.3219
Epoch 123/200, Loss: 6269.3699
Epoch 124/200, Loss: 6269.3171
Epoch 125/200, Loss: 6269.4148
Epoch 126/200, Loss: 6269.3281
Epoch 127/200, Loss: 6269.3062
Epoch 128/200, Loss: 6269.3043
Epoch 129/200, Loss: 6269.3111
Epoch 130/200, Loss: 6269.3752
Epoch 131/200, Loss: 6269.2830
Epoch 132/200, Loss: 6269.3271
Epoch 133/200, Loss: 6269.3016
Epoch 134/200, Loss: 6269.3257
Epoch 135/200, Loss: 6269.3337
Epoch 136/200, Loss: 6269.3236
Epoch 137/200, Loss: 6269.2785
Epoch 138/200, Loss: 6269.3106
Epoch 139/200, Loss: 6269.3547
Epoch 140/200, Loss: 6269.3493
Epoch 141/200, Loss: 6269.3502
Epoch 142/200, Loss: 6269.3030
Epoch 143/200, Loss: 6269.3192
Epoch 144/200, Loss: 6269.3051
Epoch 145/200, Loss: 6269.3507
Epoch 146/200, Loss: 6269.2775
Epoch 147/200, Loss: 6269.3047
Epoch 148/200, Loss: 6269.2944
Epoch 149/200, Loss: 6269.3163
Epoch 150/200, Loss: 6269.2901
Epoch 151/200, Loss: 6269.2726
Epoch 152/200, Loss: 6269.3218
Epoch 153/200, Loss: 6269.3192
Epoch 154/200, Loss: 6269.4000
Epoch 155/200, Loss: 6269.3220
Epoch 156/200, Loss: 6269.3080
Epoch 157/200, Loss: 6269.2874
Epoch 158/200, Loss: 6269.2725
Epoch 159/200, Loss: 6269.3152
Epoch 160/200, Loss: 6269.2832
Epoch 161/200, Loss: 6269.2781
Epoch 162/200, Loss: 6269.3176
Epoch 163/200, Loss: 6269.3545
Epoch 164/200, Loss: 6269.3267
Epoch 165/200, Loss: 6269.3505
Epoch 166/200, Loss: 6269.2930
Epoch 167/200, Loss: 6269.3274
Epoch 168/200, Loss: 6269.2719
Epoch 169/200, Loss: 6269.3462
Epoch 170/200, Loss: 6269.2719
Epoch 171/200, Loss: 6269.3309
Epoch 172/200, Loss: 6269.2882
Epoch 173/200, Loss: 6269.3490
Epoch 174/200, Loss: 6269.3184
Epoch 175/200, Loss: 6269.3287
Epoch 176/200, Loss: 6269.2928
Epoch 177/200, Loss: 6269.3137
Epoch 178/200, Loss: 6269.3099
Epoch 179/200, Loss: 6269.3333
Epoch 180/200, Loss: 6269.3353
Epoch 181/200, Loss: 6269.2889
Epoch 182/200, Loss: 6269.3000
Epoch 183/200, Loss: 6269.2721
Epoch 184/200, Loss: 6269.3551
Epoch 185/200, Loss: 6269.2899
Epoch 186/200, Loss: 6269.3242
Epoch 187/200, Loss: 6269.2946
Epoch 188/200, Loss: 6269.3542
Epoch 189/200, Loss: 6269.2831
Epoch 190/200, Loss: 6269.3425
Epoch 191/200, Loss: 6269.3617
Epoch 192/200, Loss: 6269.3027
Epoch 193/200, Loss: 6269.3196
Epoch 194/200, Loss: 6269.3252
Epoch 195/200, Loss: 6269.2956
Epoch 196/200, Loss: 6269.3416
Epoch 197/200, Loss: 6269.2831
Epoch 198/200, Loss: 6269.3039
Epoch 199/200, Loss: 6269.2555
Epoch 200/200, Loss: 6269.2432
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 12.46% | Test Accuracy: 14.39%
Precision: 0.0971 | Recall: 0.1439 | F1-Score: 0.0759

Processing Subject 3...
Top 32 discriminative features: [26 38 46 44 14 12 41 42 18 40 16 29 22 30 45 63 61 28 59 57 49 51  2  0
 20  9 21 37 25 10  6 33]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 135745
	# per Class in Train Dataset:
		Class 0: 2001
		Class 1: 3704
		Class 2: 3564
		Class 3: 5400
		Class 4: 4172
		Class 5: 4080
		Class 6: 5400
		Class 7: 3831
		Class 8: 3638
		Class 9: 5293
		Class 10: 4342
		Class 11: 1693
		Class 12: 1723
		Class 13: 2311
		Class 14: 3057
		Class 15: 2658
		Class 16: 1687
		Class 17: 1704
		Class 18: 2166
		Class 19: 2967
		Class 20: 4875
		Class 21: 5371
		Class 22: 5400
		Class 23: 4309
		Class 24: 2603
		Class 25: 4253
		Class 26: 2324
		Class 27: 3254
		Class 28: 2974
		Class 29: 3210
		Class 30: 3560
		Class 31: 3342
		Class 32: 2768
		Class 33: 2213
		Class 34: 5251
		Class 35: 3275
		Class 36: 3675
		Class 37: 4289
		Class 38: 3408
	# of Testing Samples: 12344
	# per Class in Test Dataset:
		Class 1: 315
		Class 2: 381
		Class 3: 600
		Class 4: 334
		Class 5: 392
		Class 6: 600
		Class 7: 383
		Class 8: 251
		Class 9: 438
		Class 10: 368
		Class 11: 178
		Class 12: 217
		Class 13: 216
		Class 14: 220
		Class 15: 198
		Class 16: 220
		Class 17: 219
		Class 18: 177
		Class 19: 213
		Class 20: 391
		Class 21: 514
		Class 22: 379
		Class 23: 383
		Class 24: 300
		Class 25: 437
		Class 26: 230
		Class 27: 418
		Class 28: 202
		Class 29: 282
		Class 30: 355
		Class 31: 342
		Class 32: 206
		Class 33: 168
		Class 34: 597
		Class 35: 262
		Class 36: 236
		Class 37: 343
		Class 38: 379
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 57.48%
Precision: 0.6058 | Recall: 0.5748 | F1-Score: 0.5619


Running Deep Learning Classifier...
DataLoader: Training set - 2122 batches, Testing set - 193 batches
Epoch 1/200, Loss: 4643.6515
Epoch 2/200, Loss: 2848.3859
Epoch 3/200, Loss: 2331.1707
Epoch 4/200, Loss: 2022.8663
Epoch 5/200, Loss: 1804.5770
Epoch 6/200, Loss: 1651.9964
Epoch 7/200, Loss: 1629.5855
Epoch 8/200, Loss: 1612.3900
Epoch 9/200, Loss: 1596.0933
Epoch 10/200, Loss: 1579.9436
Epoch 11/200, Loss: 1566.3217
Epoch 12/200, Loss: 1563.0970
Epoch 13/200, Loss: 1562.4889
Epoch 14/200, Loss: 1560.0348
Epoch 15/200, Loss: 1561.8223
Epoch 16/200, Loss: 1556.3182
Epoch 17/200, Loss: 1556.6379
Epoch 18/200, Loss: 1557.2836
Epoch 19/200, Loss: 1556.7773
Epoch 20/200, Loss: 1556.3469
Epoch 21/200, Loss: 1558.9682
Epoch 22/200, Loss: 1558.0593
Epoch 23/200, Loss: 1555.5693
Epoch 24/200, Loss: 1555.4327
Epoch 25/200, Loss: 1555.5444
Epoch 26/200, Loss: 1556.3401
Epoch 27/200, Loss: 1557.0307
Epoch 28/200, Loss: 1557.8247
Epoch 29/200, Loss: 1555.7390
Epoch 30/200, Loss: 1555.9599
Epoch 31/200, Loss: 1555.6496
Epoch 32/200, Loss: 1555.5564
Epoch 33/200, Loss: 1555.6003
Epoch 34/200, Loss: 1556.4621
Epoch 35/200, Loss: 1556.0830
Epoch 36/200, Loss: 1556.6110
Epoch 37/200, Loss: 1555.4055
Epoch 38/200, Loss: 1555.6988
Epoch 39/200, Loss: 1555.5594
Epoch 40/200, Loss: 1555.6048
Epoch 41/200, Loss: 1556.7745
Epoch 42/200, Loss: 1555.9797
Epoch 43/200, Loss: 1555.4412
Epoch 44/200, Loss: 1556.2128
Epoch 45/200, Loss: 1555.4963
Epoch 46/200, Loss: 1555.5203
Epoch 47/200, Loss: 1556.6590
Epoch 48/200, Loss: 1555.3919
Epoch 49/200, Loss: 1556.9912
Epoch 50/200, Loss: 1555.8718
Epoch 51/200, Loss: 1555.5816
Epoch 52/200, Loss: 1555.9451
Epoch 53/200, Loss: 1556.0207
Epoch 54/200, Loss: 1555.5766
Epoch 55/200, Loss: 1555.5213
Epoch 56/200, Loss: 1556.7098
Epoch 57/200, Loss: 1555.5843
Epoch 58/200, Loss: 1555.4450
Epoch 59/200, Loss: 1555.3997
Epoch 60/200, Loss: 1555.3887
Epoch 61/200, Loss: 1557.1561
Epoch 62/200, Loss: 1556.4736
Epoch 63/200, Loss: 1555.7861
Epoch 64/200, Loss: 1555.3970
Epoch 65/200, Loss: 1555.3851
Epoch 66/200, Loss: 1556.5787
Epoch 67/200, Loss: 1558.6876
Epoch 68/200, Loss: 1557.3466
Epoch 69/200, Loss: 1555.5583
Epoch 70/200, Loss: 1555.5364
Epoch 71/200, Loss: 1555.4349
Epoch 72/200, Loss: 1555.5470
Epoch 73/200, Loss: 1555.5713
Epoch 74/200, Loss: 1555.4846
Epoch 75/200, Loss: 1555.7721
Epoch 76/200, Loss: 1556.0089
Epoch 77/200, Loss: 1555.4136
Epoch 78/200, Loss: 1555.4572
Epoch 79/200, Loss: 1555.3791
Epoch 80/200, Loss: 1556.1489
Epoch 81/200, Loss: 1555.6695
Epoch 82/200, Loss: 1555.9625
Epoch 83/200, Loss: 1556.1956
Epoch 84/200, Loss: 1555.8259
Epoch 85/200, Loss: 1555.4071
Epoch 86/200, Loss: 1558.9191
Epoch 87/200, Loss: 1555.5039
Epoch 88/200, Loss: 1556.1496
Epoch 89/200, Loss: 1556.2930
Epoch 90/200, Loss: 1556.0135
Epoch 91/200, Loss: 1556.0566
Epoch 92/200, Loss: 1555.7558
Epoch 93/200, Loss: 1556.4444
Epoch 94/200, Loss: 1555.9074
Epoch 95/200, Loss: 1555.6709
Epoch 96/200, Loss: 1558.7033
Epoch 97/200, Loss: 1556.1699
Epoch 98/200, Loss: 1555.4007
Epoch 99/200, Loss: 1557.0071
Epoch 100/200, Loss: 1555.7111
Epoch 101/200, Loss: 1556.1516
Epoch 102/200, Loss: 1555.5891
Epoch 103/200, Loss: 1555.4321
Epoch 104/200, Loss: 1555.4870
Epoch 105/200, Loss: 1555.3959
Epoch 106/200, Loss: 1555.7266
Epoch 107/200, Loss: 1555.3935
Epoch 108/200, Loss: 1555.5572
Epoch 109/200, Loss: 1555.4810
Epoch 110/200, Loss: 1555.4568
Epoch 111/200, Loss: 1555.6693
Epoch 112/200, Loss: 1558.4619
Epoch 113/200, Loss: 1555.5190
Epoch 114/200, Loss: 1555.3877
Epoch 115/200, Loss: 1558.4175
Epoch 116/200, Loss: 1555.7903
Epoch 117/200, Loss: 1555.8455
Epoch 118/200, Loss: 1555.3933
Epoch 119/200, Loss: 1556.1186
Epoch 120/200, Loss: 1555.3965
Epoch 121/200, Loss: 1556.2437
Epoch 122/200, Loss: 1555.4008
Epoch 123/200, Loss: 1555.3802
Epoch 124/200, Loss: 1555.4783
Epoch 125/200, Loss: 1558.9505
Epoch 126/200, Loss: 1556.9565
Epoch 127/200, Loss: 1555.6093
Epoch 128/200, Loss: 1556.3401
Epoch 129/200, Loss: 1555.6466
Epoch 130/200, Loss: 1556.5034
Epoch 131/200, Loss: 1556.7242
Epoch 132/200, Loss: 1556.8651
Epoch 133/200, Loss: 1555.4073
Epoch 134/200, Loss: 1556.4779
Epoch 135/200, Loss: 1555.4953
Epoch 136/200, Loss: 1556.2068
Epoch 137/200, Loss: 1556.1051
Epoch 138/200, Loss: 1555.7685
Epoch 139/200, Loss: 1555.4266
Epoch 140/200, Loss: 1555.8057
Epoch 141/200, Loss: 1555.4047
Epoch 142/200, Loss: 1555.5206
Epoch 143/200, Loss: 1555.7569
Epoch 144/200, Loss: 1556.7271
Epoch 145/200, Loss: 1556.2698
Epoch 146/200, Loss: 1555.4520
Epoch 147/200, Loss: 1556.0161
Epoch 148/200, Loss: 1557.5379
Epoch 149/200, Loss: 1556.8184
Epoch 150/200, Loss: 1555.8304
Epoch 151/200, Loss: 1555.3789
Epoch 152/200, Loss: 1555.6388
Epoch 153/200, Loss: 1555.7293
Epoch 154/200, Loss: 1555.4428
Epoch 155/200, Loss: 1555.8642
Epoch 156/200, Loss: 1555.3845
Epoch 157/200, Loss: 1555.6798
Epoch 158/200, Loss: 1555.8225
Epoch 159/200, Loss: 1559.3221
Epoch 160/200, Loss: 1555.4630
Epoch 161/200, Loss: 1558.7474
Epoch 162/200, Loss: 1555.7437
Epoch 163/200, Loss: 1555.4979
Epoch 164/200, Loss: 1555.6416
Epoch 165/200, Loss: 1555.4377
Epoch 166/200, Loss: 1555.4444
Epoch 167/200, Loss: 1555.5457
Epoch 168/200, Loss: 1556.3354
Epoch 169/200, Loss: 1555.7784
Epoch 170/200, Loss: 1556.0553
Epoch 171/200, Loss: 1555.3924
Epoch 172/200, Loss: 1555.5586
Epoch 173/200, Loss: 1555.4706
Epoch 174/200, Loss: 1556.3787
Epoch 175/200, Loss: 1555.9938
Epoch 176/200, Loss: 1555.4285
Epoch 177/200, Loss: 1555.4603
Epoch 178/200, Loss: 1556.1519
Epoch 179/200, Loss: 1555.3949
Epoch 180/200, Loss: 1556.6117
Epoch 181/200, Loss: 1555.7893
Epoch 182/200, Loss: 1555.6780
Epoch 183/200, Loss: 1555.4496
Epoch 184/200, Loss: 1555.5816
Epoch 185/200, Loss: 1555.4075
Epoch 186/200, Loss: 1555.3850
Epoch 187/200, Loss: 1555.8823
Epoch 188/200, Loss: 1557.2537
Epoch 189/200, Loss: 1557.9557
Epoch 190/200, Loss: 1555.9653
Epoch 191/200, Loss: 1555.9930
Epoch 192/200, Loss: 1555.8792
Epoch 193/200, Loss: 1556.3721
Epoch 194/200, Loss: 1565.8302
Epoch 195/200, Loss: 1556.5769
Epoch 196/200, Loss: 1555.4182
Epoch 197/200, Loss: 1555.3958
Epoch 198/200, Loss: 1557.2662
Epoch 199/200, Loss: 1555.4814
Epoch 200/200, Loss: 1558.4856
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 76.88% | Test Accuracy: 46.77%
Precision: 0.4865 | Recall: 0.4677 | F1-Score: 0.4591
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 15.14%
Precision: 0.1540 | Recall: 0.1514 | F1-Score: 0.1500


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2122 batches, Testing set - 193 batches
Epoch 1/200, Loss: 7216.6527
Epoch 2/200, Loss: 6679.0250
Epoch 3/200, Loss: 6524.1863
Epoch 4/200, Loss: 6379.3905
Epoch 5/200, Loss: 6250.1114
Epoch 6/200, Loss: 6104.5480
Epoch 7/200, Loss: 6082.8335
Epoch 8/200, Loss: 6066.6989
Epoch 9/200, Loss: 6049.7328
Epoch 10/200, Loss: 6035.2786
Epoch 11/200, Loss: 6020.5971
Epoch 12/200, Loss: 6017.5766
Epoch 13/200, Loss: 6016.8101
Epoch 14/200, Loss: 6015.6757
Epoch 15/200, Loss: 6013.8614
Epoch 16/200, Loss: 6011.3226
Epoch 17/200, Loss: 6014.2107
Epoch 18/200, Loss: 6010.4212
Epoch 19/200, Loss: 6011.5219
Epoch 20/200, Loss: 6010.9924
Epoch 21/200, Loss: 6010.9818
Epoch 22/200, Loss: 6012.0881
Epoch 23/200, Loss: 6010.5285
Epoch 24/200, Loss: 6010.7057
Epoch 25/200, Loss: 6011.0988
Epoch 26/200, Loss: 6010.9681
Epoch 27/200, Loss: 6011.0033
Epoch 28/200, Loss: 6010.6329
Epoch 29/200, Loss: 6011.5540
Epoch 30/200, Loss: 6010.9723
Epoch 31/200, Loss: 6010.5495
Epoch 32/200, Loss: 6010.2375
Epoch 33/200, Loss: 6011.1782
Epoch 34/200, Loss: 6010.1052
Epoch 35/200, Loss: 6010.6838
Epoch 36/200, Loss: 6011.0372
Epoch 37/200, Loss: 6010.9910
Epoch 38/200, Loss: 6011.3835
Epoch 39/200, Loss: 6011.1111
Epoch 40/200, Loss: 6011.1929
Epoch 41/200, Loss: 6011.1119
Epoch 42/200, Loss: 6010.9938
Epoch 43/200, Loss: 6010.7823
Epoch 44/200, Loss: 6011.1862
Epoch 45/200, Loss: 6011.2069
Epoch 46/200, Loss: 6009.9615
Epoch 47/200, Loss: 6010.0880
Epoch 48/200, Loss: 6011.6326
Epoch 49/200, Loss: 6010.3918
Epoch 50/200, Loss: 6010.7580
Epoch 51/200, Loss: 6012.8092
Epoch 52/200, Loss: 6010.2751
Epoch 53/200, Loss: 6012.2485
Epoch 54/200, Loss: 6011.1805
Epoch 55/200, Loss: 6012.2805
Epoch 56/200, Loss: 6011.0168
Epoch 57/200, Loss: 6010.0625
Epoch 58/200, Loss: 6010.7869
Epoch 59/200, Loss: 6010.8836
Epoch 60/200, Loss: 6010.4521
Epoch 61/200, Loss: 6012.0607
Epoch 62/200, Loss: 6011.4224
Epoch 63/200, Loss: 6010.7914
Epoch 64/200, Loss: 6011.2542
Epoch 65/200, Loss: 6012.8156
Epoch 66/200, Loss: 6010.8942
Epoch 67/200, Loss: 6011.0858
Epoch 68/200, Loss: 6010.9402
Epoch 69/200, Loss: 6010.8777
Epoch 70/200, Loss: 6011.5526
Epoch 71/200, Loss: 6011.1475
Epoch 72/200, Loss: 6010.7524
Epoch 73/200, Loss: 6011.9945
Epoch 74/200, Loss: 6011.3775
Epoch 75/200, Loss: 6011.0171
Epoch 76/200, Loss: 6011.1350
Epoch 77/200, Loss: 6011.2109
Epoch 78/200, Loss: 6011.2108
Epoch 79/200, Loss: 6011.6757
Epoch 80/200, Loss: 6011.3576
Epoch 81/200, Loss: 6009.8758
Epoch 82/200, Loss: 6010.2016
Epoch 83/200, Loss: 6011.0685
Epoch 84/200, Loss: 6012.6754
Epoch 85/200, Loss: 6011.0887
Epoch 86/200, Loss: 6011.0648
Epoch 87/200, Loss: 6010.5584
Epoch 88/200, Loss: 6012.1434
Epoch 89/200, Loss: 6011.1211
Epoch 90/200, Loss: 6011.6806
Epoch 91/200, Loss: 6011.3601
Epoch 92/200, Loss: 6010.9578
Epoch 93/200, Loss: 6011.1861
Epoch 94/200, Loss: 6011.6767
Epoch 95/200, Loss: 6011.4036
Epoch 96/200, Loss: 6011.0948
Epoch 97/200, Loss: 6011.4007
Epoch 98/200, Loss: 6010.4346
Epoch 99/200, Loss: 6011.2171
Epoch 100/200, Loss: 6011.0911
Epoch 101/200, Loss: 6011.0580
Epoch 102/200, Loss: 6011.9871
Epoch 103/200, Loss: 6009.9254
Epoch 104/200, Loss: 6010.6017
Epoch 105/200, Loss: 6011.0453
Epoch 106/200, Loss: 6011.0369
Epoch 107/200, Loss: 6010.7608
Epoch 108/200, Loss: 6010.4479
Epoch 109/200, Loss: 6011.5062
Epoch 110/200, Loss: 6011.1275
Epoch 111/200, Loss: 6012.2749
Epoch 112/200, Loss: 6010.7017
Epoch 113/200, Loss: 6011.7878
Epoch 114/200, Loss: 6010.9114
Epoch 115/200, Loss: 6012.9339
Epoch 116/200, Loss: 6011.7668
Epoch 117/200, Loss: 6011.0943
Epoch 118/200, Loss: 6011.2808
Epoch 119/200, Loss: 6011.0773
Epoch 120/200, Loss: 6011.8083
Epoch 121/200, Loss: 6011.6071
Epoch 122/200, Loss: 6011.7473
Epoch 123/200, Loss: 6011.0021
Epoch 124/200, Loss: 6011.2529
Epoch 125/200, Loss: 6011.0079
Epoch 126/200, Loss: 6010.3774
Epoch 127/200, Loss: 6011.0076
Epoch 128/200, Loss: 6011.1380
Epoch 129/200, Loss: 6011.1876
Epoch 130/200, Loss: 6011.2881
Epoch 131/200, Loss: 6011.8644
Epoch 132/200, Loss: 6010.2131
Epoch 133/200, Loss: 6010.8217
Epoch 134/200, Loss: 6011.3351
Epoch 135/200, Loss: 6010.7615
Epoch 136/200, Loss: 6011.3849
Epoch 137/200, Loss: 6010.0765
Epoch 138/200, Loss: 6010.6429
Epoch 139/200, Loss: 6010.5880
Epoch 140/200, Loss: 6011.2962
Epoch 141/200, Loss: 6011.9068
Epoch 142/200, Loss: 6010.6208
Epoch 143/200, Loss: 6012.0111
Epoch 144/200, Loss: 6010.4028
Epoch 145/200, Loss: 6010.5954
Epoch 146/200, Loss: 6011.9556
Epoch 147/200, Loss: 6011.5957
Epoch 148/200, Loss: 6011.2313
Epoch 149/200, Loss: 6012.4792
Epoch 150/200, Loss: 6011.0983
Epoch 151/200, Loss: 6010.9099
Epoch 152/200, Loss: 6011.5346
Epoch 153/200, Loss: 6010.9838
Epoch 154/200, Loss: 6011.0571
Epoch 155/200, Loss: 6011.3468
Epoch 156/200, Loss: 6010.6627
Epoch 157/200, Loss: 6011.6712
Epoch 158/200, Loss: 6011.4508
Epoch 159/200, Loss: 6011.2667
Epoch 160/200, Loss: 6011.7690
Epoch 161/200, Loss: 6010.8614
Epoch 162/200, Loss: 6010.7621
Epoch 163/200, Loss: 6011.4265
Epoch 164/200, Loss: 6011.1833
Epoch 165/200, Loss: 6010.0471
Epoch 166/200, Loss: 6011.0546
Epoch 167/200, Loss: 6011.0429
Epoch 168/200, Loss: 6009.4591
Epoch 169/200, Loss: 6010.9868
Epoch 170/200, Loss: 6011.3881
Epoch 171/200, Loss: 6011.0595
Epoch 172/200, Loss: 6009.9303
Epoch 173/200, Loss: 6011.4472
Epoch 174/200, Loss: 6011.0430
Epoch 175/200, Loss: 6010.4480
Epoch 176/200, Loss: 6010.8810
Epoch 177/200, Loss: 6010.6443
Epoch 178/200, Loss: 6010.7299
Epoch 179/200, Loss: 6011.8964
Epoch 180/200, Loss: 6011.5639
Epoch 181/200, Loss: 6011.0608
Epoch 182/200, Loss: 6009.9962
Epoch 183/200, Loss: 6011.3114
Epoch 184/200, Loss: 6010.5349
Epoch 185/200, Loss: 6010.9792
Epoch 186/200, Loss: 6011.4362
Epoch 187/200, Loss: 6010.6780
Epoch 188/200, Loss: 6011.2532
Epoch 189/200, Loss: 6011.2319
Epoch 190/200, Loss: 6010.4626
Epoch 191/200, Loss: 6010.7298
Epoch 192/200, Loss: 6010.9403
Epoch 193/200, Loss: 6010.1333
Epoch 194/200, Loss: 6010.8408
Epoch 195/200, Loss: 6010.8989
Epoch 196/200, Loss: 6011.4844
Epoch 197/200, Loss: 6011.6054
Epoch 198/200, Loss: 6011.1899
Epoch 199/200, Loss: 6010.7299
Epoch 200/200, Loss: 6011.6201
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 14.20% | Test Accuracy: 12.62%
Precision: 0.0792 | Recall: 0.1262 | F1-Score: 0.0822

Processing Subject 4...
Top 32 discriminative features: [26 38 46 44 14 12 41 18 42 16 40 29 30 45 22 63 61 28 49 51 59 57  2  0
 37 25 20  9 21 10 33  6]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 134144
	# per Class in Train Dataset:
		Class 0: 2001
		Class 1: 3621
		Class 2: 3619
		Class 3: 5400
		Class 4: 4102
		Class 5: 4014
		Class 6: 5400
		Class 7: 3787
		Class 8: 3539
		Class 9: 5151
		Class 10: 4295
		Class 11: 1741
		Class 12: 1783
		Class 13: 2301
		Class 14: 2953
		Class 15: 2620
		Class 16: 1770
		Class 17: 1765
		Class 18: 2136
		Class 19: 2880
		Class 20: 4754
		Class 21: 5285
		Class 22: 5179
		Class 23: 4222
		Class 24: 2641
		Class 25: 4224
		Class 26: 2275
		Class 27: 3306
		Class 28: 2898
		Class 29: 3128
		Class 30: 3530
		Class 31: 3404
		Class 32: 2697
		Class 33: 2176
		Class 34: 5282
		Class 35: 3213
		Class 36: 3581
		Class 37: 4071
		Class 38: 3400
	# of Testing Samples: 13945
	# per Class in Test Dataset:
		Class 1: 398
		Class 2: 326
		Class 3: 600
		Class 4: 404
		Class 5: 458
		Class 6: 600
		Class 7: 427
		Class 8: 350
		Class 9: 580
		Class 10: 415
		Class 11: 130
		Class 12: 157
		Class 13: 226
		Class 14: 324
		Class 15: 236
		Class 16: 137
		Class 17: 158
		Class 18: 207
		Class 19: 300
		Class 20: 512
		Class 21: 600
		Class 22: 600
		Class 23: 470
		Class 24: 262
		Class 25: 466
		Class 26: 279
		Class 27: 366
		Class 28: 278
		Class 29: 364
		Class 30: 385
		Class 31: 280
		Class 32: 277
		Class 33: 205
		Class 34: 566
		Class 35: 324
		Class 36: 330
		Class 37: 561
		Class 38: 387
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 70.64%
Precision: 0.7249 | Recall: 0.7064 | F1-Score: 0.7030


Running Deep Learning Classifier...
DataLoader: Training set - 2096 batches, Testing set - 218 batches
Epoch 1/200, Loss: 4617.9715
Epoch 2/200, Loss: 2945.5485
Epoch 3/200, Loss: 2414.5533
Epoch 4/200, Loss: 2099.3712
Epoch 5/200, Loss: 1879.3030
Epoch 6/200, Loss: 1727.3551
Epoch 7/200, Loss: 1705.1813
Epoch 8/200, Loss: 1688.0077
Epoch 9/200, Loss: 1671.6345
Epoch 10/200, Loss: 1656.7332
Epoch 11/200, Loss: 1642.1258
Epoch 12/200, Loss: 1639.6731
Epoch 13/200, Loss: 1638.0201
Epoch 14/200, Loss: 1636.4019
Epoch 15/200, Loss: 1634.9595
Epoch 16/200, Loss: 1633.2153
Epoch 17/200, Loss: 1633.0365
Epoch 18/200, Loss: 1632.8725
Epoch 19/200, Loss: 1632.7141
Epoch 20/200, Loss: 1632.5605
Epoch 21/200, Loss: 1632.3660
Epoch 22/200, Loss: 1632.3578
Epoch 23/200, Loss: 1632.3492
Epoch 24/200, Loss: 1632.3408
Epoch 25/200, Loss: 1632.3322
Epoch 26/200, Loss: 1632.3190
Epoch 27/200, Loss: 1632.3189
Epoch 28/200, Loss: 1632.3188
Epoch 29/200, Loss: 1632.3187
Epoch 30/200, Loss: 1632.3186
Epoch 31/200, Loss: 1632.3185
Epoch 32/200, Loss: 1632.3185
Epoch 33/200, Loss: 1632.3185
Epoch 34/200, Loss: 1632.3185
Epoch 35/200, Loss: 1632.3185
Epoch 36/200, Loss: 1632.3185
Epoch 37/200, Loss: 1632.3185
Epoch 38/200, Loss: 1632.3185
Epoch 39/200, Loss: 1632.3185
Epoch 40/200, Loss: 1632.3185
Epoch 41/200, Loss: 1632.3185
Epoch 42/200, Loss: 1632.3185
Epoch 43/200, Loss: 1632.3185
Epoch 44/200, Loss: 1632.3185
Epoch 45/200, Loss: 1632.3185
Epoch 46/200, Loss: 1632.3185
Epoch 47/200, Loss: 1632.3185
Epoch 48/200, Loss: 1632.3185
Epoch 49/200, Loss: 1632.3185
Epoch 50/200, Loss: 1632.3185
Epoch 51/200, Loss: 1632.3185
Epoch 52/200, Loss: 1632.3185
Epoch 53/200, Loss: 1632.3185
Epoch 54/200, Loss: 1632.3185
Epoch 55/200, Loss: 1632.3185
Epoch 56/200, Loss: 1632.3185
Epoch 57/200, Loss: 1632.3185
Epoch 58/200, Loss: 1632.3185
Epoch 59/200, Loss: 1632.3185
Epoch 60/200, Loss: 1632.3185
Epoch 61/200, Loss: 1632.3185
Epoch 62/200, Loss: 1632.3185
Epoch 63/200, Loss: 1632.3185
Epoch 64/200, Loss: 1632.3185
Epoch 65/200, Loss: 1632.3185
Epoch 66/200, Loss: 1632.3185
Epoch 67/200, Loss: 1632.3185
Epoch 68/200, Loss: 1632.3185
Epoch 69/200, Loss: 1632.3185
Epoch 70/200, Loss: 1632.3185
Epoch 71/200, Loss: 1632.3185
Epoch 72/200, Loss: 1632.3185
Epoch 73/200, Loss: 1632.3185
Epoch 74/200, Loss: 1632.3185
Epoch 75/200, Loss: 1632.3185
Epoch 76/200, Loss: 1632.3185
Epoch 77/200, Loss: 1632.3185
Epoch 78/200, Loss: 1632.3185
Epoch 79/200, Loss: 1632.3185
Epoch 80/200, Loss: 1632.3185
Epoch 81/200, Loss: 1632.3185
Epoch 82/200, Loss: 1632.3185
Epoch 83/200, Loss: 1632.3185
Epoch 84/200, Loss: 1632.3185
Epoch 85/200, Loss: 1632.3185
Epoch 86/200, Loss: 1632.3185
Epoch 87/200, Loss: 1632.3185
Epoch 88/200, Loss: 1632.3185
Epoch 89/200, Loss: 1632.3185
Epoch 90/200, Loss: 1632.3185
Epoch 91/200, Loss: 1632.3185
Epoch 92/200, Loss: 1632.3185
Epoch 93/200, Loss: 1632.3185
Epoch 94/200, Loss: 1632.3185
Epoch 95/200, Loss: 1632.3185
Epoch 96/200, Loss: 1632.3185
Epoch 97/200, Loss: 1632.3185
Epoch 98/200, Loss: 1632.3185
Epoch 99/200, Loss: 1632.3185
Epoch 100/200, Loss: 1632.3185
Epoch 101/200, Loss: 1632.3185
Epoch 102/200, Loss: 1632.3185
Epoch 103/200, Loss: 1632.3185
Epoch 104/200, Loss: 1632.3185
Epoch 105/200, Loss: 1632.3185
Epoch 106/200, Loss: 1632.3185
Epoch 107/200, Loss: 1632.3185
Epoch 108/200, Loss: 1632.3185
Epoch 109/200, Loss: 1632.3185
Epoch 110/200, Loss: 1632.3185
Epoch 111/200, Loss: 1632.3185
Epoch 112/200, Loss: 1632.3185
Epoch 113/200, Loss: 1632.3185
Epoch 114/200, Loss: 1632.3185
Epoch 115/200, Loss: 1632.3185
Epoch 116/200, Loss: 1632.3185
Epoch 117/200, Loss: 1632.3185
Epoch 118/200, Loss: 1632.3185
Epoch 119/200, Loss: 1632.3185
Epoch 120/200, Loss: 1632.3185
Epoch 121/200, Loss: 1632.3185
Epoch 122/200, Loss: 1632.3185
Epoch 123/200, Loss: 1632.3185
Epoch 124/200, Loss: 1632.3185
Epoch 125/200, Loss: 1632.3185
Epoch 126/200, Loss: 1632.3185
Epoch 127/200, Loss: 1632.3185
Epoch 128/200, Loss: 1632.3185
Epoch 129/200, Loss: 1632.3185
Epoch 130/200, Loss: 1632.3185
Epoch 131/200, Loss: 1632.3185
Epoch 132/200, Loss: 1632.3185
Epoch 133/200, Loss: 1632.3185
Epoch 134/200, Loss: 1632.3185
Epoch 135/200, Loss: 1632.3185
Epoch 136/200, Loss: 1632.3185
Epoch 137/200, Loss: 1632.3185
Epoch 138/200, Loss: 1632.3185
Epoch 139/200, Loss: 1632.3185
Epoch 140/200, Loss: 1632.3185
Epoch 141/200, Loss: 1632.3185
Epoch 142/200, Loss: 1632.3185
Epoch 143/200, Loss: 1632.3185
Epoch 144/200, Loss: 1632.3185
Epoch 145/200, Loss: 1632.3185
Epoch 146/200, Loss: 1632.3185
Epoch 147/200, Loss: 1632.3185
Epoch 148/200, Loss: 1632.3185
Epoch 149/200, Loss: 1632.3185
Epoch 150/200, Loss: 1632.3185
Epoch 151/200, Loss: 1632.3185
Epoch 152/200, Loss: 1632.3185
Epoch 153/200, Loss: 1632.3185
Epoch 154/200, Loss: 1632.3185
Epoch 155/200, Loss: 1632.3185
Epoch 156/200, Loss: 1632.3185
Epoch 157/200, Loss: 1632.3185
Epoch 158/200, Loss: 1632.3185
Epoch 159/200, Loss: 1632.3185
Epoch 160/200, Loss: 1632.3185
Epoch 161/200, Loss: 1632.3185
Epoch 162/200, Loss: 1632.3185
Epoch 163/200, Loss: 1632.3185
Epoch 164/200, Loss: 1632.3185
Epoch 165/200, Loss: 1632.3185
Epoch 166/200, Loss: 1632.3185
Epoch 167/200, Loss: 1632.3185
Epoch 168/200, Loss: 1632.3185
Epoch 169/200, Loss: 1632.3185
Epoch 170/200, Loss: 1632.3185
Epoch 171/200, Loss: 1632.3185
Epoch 172/200, Loss: 1632.3185
Epoch 173/200, Loss: 1632.3185
Epoch 174/200, Loss: 1632.3185
Epoch 175/200, Loss: 1632.3185
Epoch 176/200, Loss: 1632.3185
Epoch 177/200, Loss: 1632.3185
Epoch 178/200, Loss: 1632.3185
Epoch 179/200, Loss: 1632.3185
Epoch 180/200, Loss: 1632.3185
Epoch 181/200, Loss: 1632.3185
Epoch 182/200, Loss: 1632.3185
Epoch 183/200, Loss: 1632.3185
Epoch 184/200, Loss: 1632.3185
Epoch 185/200, Loss: 1632.3185
Epoch 186/200, Loss: 1632.3185
Epoch 187/200, Loss: 1632.3185
Epoch 188/200, Loss: 1632.3185
Epoch 189/200, Loss: 1632.3185
Epoch 190/200, Loss: 1632.3185
Epoch 191/200, Loss: 1632.3185
Epoch 192/200, Loss: 1632.3185
Epoch 193/200, Loss: 1632.3185
Epoch 194/200, Loss: 1632.3185
Epoch 195/200, Loss: 1632.3185
Epoch 196/200, Loss: 1632.3185
Epoch 197/200, Loss: 1632.3185
Epoch 198/200, Loss: 1632.3185
Epoch 199/200, Loss: 1632.3185
Epoch 200/200, Loss: 1632.3185
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 75.40% | Test Accuracy: 56.63%
Precision: 0.5734 | Recall: 0.5663 | F1-Score: 0.5552
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 14.61%
Precision: 0.1532 | Recall: 0.1461 | F1-Score: 0.1475


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2096 batches, Testing set - 218 batches
Epoch 1/200, Loss: 7073.5044
Epoch 2/200, Loss: 6560.6454
Epoch 3/200, Loss: 6381.9932
Epoch 4/200, Loss: 6260.6704
Epoch 5/200, Loss: 6159.9862
Epoch 6/200, Loss: 6059.5042
Epoch 7/200, Loss: 6044.3924
Epoch 8/200, Loss: 6031.0736
Epoch 9/200, Loss: 6020.5653
Epoch 10/200, Loss: 6011.6291
Epoch 11/200, Loss: 5999.6872
Epoch 12/200, Loss: 5998.3122
Epoch 13/200, Loss: 5997.3545
Epoch 14/200, Loss: 5996.4506
Epoch 15/200, Loss: 5995.3941
Epoch 16/200, Loss: 5994.2141
Epoch 17/200, Loss: 5994.0625
Epoch 18/200, Loss: 5993.9628
Epoch 19/200, Loss: 5993.8724
Epoch 20/200, Loss: 5993.7729
Epoch 21/200, Loss: 5993.6134
Epoch 22/200, Loss: 5993.6097
Epoch 23/200, Loss: 5993.6039
Epoch 24/200, Loss: 5993.6004
Epoch 25/200, Loss: 5993.5958
Epoch 26/200, Loss: 5993.5830
Epoch 27/200, Loss: 5993.5829
Epoch 28/200, Loss: 5993.5829
Epoch 29/200, Loss: 5993.5828
Epoch 30/200, Loss: 5993.5827
Epoch 31/200, Loss: 5993.5826
Epoch 32/200, Loss: 5993.5826
Epoch 33/200, Loss: 5993.5826
Epoch 34/200, Loss: 5993.5826
Epoch 35/200, Loss: 5993.5826
Epoch 36/200, Loss: 5993.5826
Epoch 37/200, Loss: 5993.5826
Epoch 38/200, Loss: 5993.5826
Epoch 39/200, Loss: 5993.5826
Epoch 40/200, Loss: 5993.5826
Epoch 41/200, Loss: 5993.5826
Epoch 42/200, Loss: 5993.5826
Epoch 43/200, Loss: 5993.5826
Epoch 44/200, Loss: 5993.5826
Epoch 45/200, Loss: 5993.5826
Epoch 46/200, Loss: 5993.5826
Epoch 47/200, Loss: 5993.5826
Epoch 48/200, Loss: 5993.5826
Epoch 49/200, Loss: 5993.5826
Epoch 50/200, Loss: 5993.5826
Epoch 51/200, Loss: 5993.5826
Epoch 52/200, Loss: 5993.5826
Epoch 53/200, Loss: 5993.5826
Epoch 54/200, Loss: 5993.5826
Epoch 55/200, Loss: 5993.5826
Epoch 56/200, Loss: 5993.5826
Epoch 57/200, Loss: 5993.5826
Epoch 58/200, Loss: 5993.5826
Epoch 59/200, Loss: 5993.5826
Epoch 60/200, Loss: 5993.5826
Epoch 61/200, Loss: 5993.5826
Epoch 62/200, Loss: 5993.5826
Epoch 63/200, Loss: 5993.5826
Epoch 64/200, Loss: 5993.5826
Epoch 65/200, Loss: 5993.5826
Epoch 66/200, Loss: 5993.5826
Epoch 67/200, Loss: 5993.5826
Epoch 68/200, Loss: 5993.5826
Epoch 69/200, Loss: 5993.5826
Epoch 70/200, Loss: 5993.5826
Epoch 71/200, Loss: 5993.5826
Epoch 72/200, Loss: 5993.5826
Epoch 73/200, Loss: 5993.5826
Epoch 74/200, Loss: 5993.5826
Epoch 75/200, Loss: 5993.5826
Epoch 76/200, Loss: 5993.5826
Epoch 77/200, Loss: 5993.5826
Epoch 78/200, Loss: 5993.5826
Epoch 79/200, Loss: 5993.5826
Epoch 80/200, Loss: 5993.5826
Epoch 81/200, Loss: 5993.5826
Epoch 82/200, Loss: 5993.5826
Epoch 83/200, Loss: 5993.5826
Epoch 84/200, Loss: 5993.5826
Epoch 85/200, Loss: 5993.5826
Epoch 86/200, Loss: 5993.5826
Epoch 87/200, Loss: 5993.5826
Epoch 88/200, Loss: 5993.5826
Epoch 89/200, Loss: 5993.5826
Epoch 90/200, Loss: 5993.5826
Epoch 91/200, Loss: 5993.5826
Epoch 92/200, Loss: 5993.5826
Epoch 93/200, Loss: 5993.5826
Epoch 94/200, Loss: 5993.5826
Epoch 95/200, Loss: 5993.5826
Epoch 96/200, Loss: 5993.5826
Epoch 97/200, Loss: 5993.5826
Epoch 98/200, Loss: 5993.5826
Epoch 99/200, Loss: 5993.5826
Epoch 100/200, Loss: 5993.5826
Epoch 101/200, Loss: 5993.5826
Epoch 102/200, Loss: 5993.5826
Epoch 103/200, Loss: 5993.5826
Epoch 104/200, Loss: 5993.5826
Epoch 105/200, Loss: 5993.5826
Epoch 106/200, Loss: 5993.5826
Epoch 107/200, Loss: 5993.5826
Epoch 108/200, Loss: 5993.5826
Epoch 109/200, Loss: 5993.5826
Epoch 110/200, Loss: 5993.5826
Epoch 111/200, Loss: 5993.5826
Epoch 112/200, Loss: 5993.5826
Epoch 113/200, Loss: 5993.5826
Epoch 114/200, Loss: 5993.5826
Epoch 115/200, Loss: 5993.5826
Epoch 116/200, Loss: 5993.5826
Epoch 117/200, Loss: 5993.5826
Epoch 118/200, Loss: 5993.5826
Epoch 119/200, Loss: 5993.5826
Epoch 120/200, Loss: 5993.5826
Epoch 121/200, Loss: 5993.5826
Epoch 122/200, Loss: 5993.5826
Epoch 123/200, Loss: 5993.5826
Epoch 124/200, Loss: 5993.5826
Epoch 125/200, Loss: 5993.5826
Epoch 126/200, Loss: 5993.5826
Epoch 127/200, Loss: 5993.5826
Epoch 128/200, Loss: 5993.5826
Epoch 129/200, Loss: 5993.5826
Epoch 130/200, Loss: 5993.5826
Epoch 131/200, Loss: 5993.5826
Epoch 132/200, Loss: 5993.5826
Epoch 133/200, Loss: 5993.5826
Epoch 134/200, Loss: 5993.5826
Epoch 135/200, Loss: 5993.5826
Epoch 136/200, Loss: 5993.5826
Epoch 137/200, Loss: 5993.5826
Epoch 138/200, Loss: 5993.5826
Epoch 139/200, Loss: 5993.5826
Epoch 140/200, Loss: 5993.5826
Epoch 141/200, Loss: 5993.5826
Epoch 142/200, Loss: 5993.5826
Epoch 143/200, Loss: 5993.5826
Epoch 144/200, Loss: 5993.5826
Epoch 145/200, Loss: 5993.5826
Epoch 146/200, Loss: 5993.5826
Epoch 147/200, Loss: 5993.5826
Epoch 148/200, Loss: 5993.5826
Epoch 149/200, Loss: 5993.5826
Epoch 150/200, Loss: 5993.5826
Epoch 151/200, Loss: 5993.5826
Epoch 152/200, Loss: 5993.5826
Epoch 153/200, Loss: 5993.5826
Epoch 154/200, Loss: 5993.5826
Epoch 155/200, Loss: 5993.5826
Epoch 156/200, Loss: 5993.5826
Epoch 157/200, Loss: 5993.5826
Epoch 158/200, Loss: 5993.5826
Epoch 159/200, Loss: 5993.5826
Epoch 160/200, Loss: 5993.5826
Epoch 161/200, Loss: 5993.5826
Epoch 162/200, Loss: 5993.5826
Epoch 163/200, Loss: 5993.5826
Epoch 164/200, Loss: 5993.5826
Epoch 165/200, Loss: 5993.5826
Epoch 166/200, Loss: 5993.5826
Epoch 167/200, Loss: 5993.5826
Epoch 168/200, Loss: 5993.5826
Epoch 169/200, Loss: 5993.5826
Epoch 170/200, Loss: 5993.5826
Epoch 171/200, Loss: 5993.5826
Epoch 172/200, Loss: 5993.5826
Epoch 173/200, Loss: 5993.5826
Epoch 174/200, Loss: 5993.5826
Epoch 175/200, Loss: 5993.5826
Epoch 176/200, Loss: 5993.5826
Epoch 177/200, Loss: 5993.5826
Epoch 178/200, Loss: 5993.5826
Epoch 179/200, Loss: 5993.5826
Epoch 180/200, Loss: 5993.5826
Epoch 181/200, Loss: 5993.5826
Epoch 182/200, Loss: 5993.5826
Epoch 183/200, Loss: 5993.5826
Epoch 184/200, Loss: 5993.5826
Epoch 185/200, Loss: 5993.5826
Epoch 186/200, Loss: 5993.5826
Epoch 187/200, Loss: 5993.5826
Epoch 188/200, Loss: 5993.5826
Epoch 189/200, Loss: 5993.5826
Epoch 190/200, Loss: 5993.5826
Epoch 191/200, Loss: 5993.5826
Epoch 192/200, Loss: 5993.5826
Epoch 193/200, Loss: 5993.5826
Epoch 194/200, Loss: 5993.5826
Epoch 195/200, Loss: 5993.5826
Epoch 196/200, Loss: 5993.5826
Epoch 197/200, Loss: 5993.5826
Epoch 198/200, Loss: 5993.5826
Epoch 199/200, Loss: 5993.5826
Epoch 200/200, Loss: 5993.5826
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 13.88% | Test Accuracy: 14.87%
Precision: 0.0943 | Recall: 0.1487 | F1-Score: 0.0908

Processing Subject 5...
Top 32 discriminative features: [26 38 46 44 14 12 18 42 41 16 40 30 22 45 29 63 61 49 51 28  2 59  0 57
 20 37 25  9 10  6 21  4]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 131018
	# per Class in Train Dataset:
		Class 0: 1426
		Class 1: 3554
		Class 2: 3447
		Class 3: 5400
		Class 4: 3906
		Class 5: 3885
		Class 6: 5400
		Class 7: 3626
		Class 8: 3392
		Class 9: 5156
		Class 10: 4217
		Class 11: 1652
		Class 12: 1746
		Class 13: 2288
		Class 14: 2897
		Class 15: 2540
		Class 16: 1655
		Class 17: 1699
		Class 18: 2102
		Class 19: 2810
		Class 20: 4666
		Class 21: 5285
		Class 22: 5179
		Class 23: 4235
		Class 24: 2603
		Class 25: 4203
		Class 26: 2311
		Class 27: 3336
		Class 28: 2686
		Class 29: 3126
		Class 30: 3474
		Class 31: 3225
		Class 32: 2553
		Class 33: 2050
		Class 34: 5248
		Class 35: 3127
		Class 36: 3475
		Class 37: 4032
		Class 38: 3406
	# of Testing Samples: 17071
	# per Class in Test Dataset:
		Class 0: 575
		Class 1: 465
		Class 2: 498
		Class 3: 600
		Class 4: 600
		Class 5: 587
		Class 6: 600
		Class 7: 588
		Class 8: 497
		Class 9: 575
		Class 10: 493
		Class 11: 219
		Class 12: 194
		Class 13: 239
		Class 14: 380
		Class 15: 316
		Class 16: 252
		Class 17: 224
		Class 18: 241
		Class 19: 370
		Class 20: 600
		Class 21: 600
		Class 22: 600
		Class 23: 457
		Class 24: 300
		Class 25: 487
		Class 26: 243
		Class 27: 336
		Class 28: 490
		Class 29: 366
		Class 30: 441
		Class 31: 459
		Class 32: 421
		Class 33: 331
		Class 34: 600
		Class 35: 410
		Class 36: 436
		Class 37: 600
		Class 38: 381
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 64.30%
Precision: 0.6295 | Recall: 0.6430 | F1-Score: 0.6161


Running Deep Learning Classifier...
DataLoader: Training set - 2048 batches, Testing set - 267 batches
Epoch 1/200, Loss: 4441.2783
Epoch 2/200, Loss: 2792.1982
Epoch 3/200, Loss: 2284.4426
Epoch 4/200, Loss: 1977.4555
Epoch 5/200, Loss: 1761.6194
Epoch 6/200, Loss: 1607.0381
Epoch 7/200, Loss: 1584.9278
Epoch 8/200, Loss: 1567.7380
Epoch 9/200, Loss: 1551.7396
Epoch 10/200, Loss: 1536.4242
Epoch 11/200, Loss: 1522.0673
Epoch 12/200, Loss: 1519.7312
Epoch 13/200, Loss: 1518.0212
Epoch 14/200, Loss: 1516.4319
Epoch 15/200, Loss: 1514.9389
Epoch 16/200, Loss: 1513.4144
Epoch 17/200, Loss: 1512.9568
Epoch 18/200, Loss: 1513.2595
Epoch 19/200, Loss: 1513.0108
Epoch 20/200, Loss: 1512.6000
Epoch 21/200, Loss: 1512.5311
Epoch 22/200, Loss: 1512.6319
Epoch 23/200, Loss: 1512.4570
Epoch 24/200, Loss: 1512.4147
Epoch 25/200, Loss: 1512.6420
Epoch 26/200, Loss: 1512.6217
Epoch 27/200, Loss: 1512.5482
Epoch 28/200, Loss: 1512.3739
Epoch 29/200, Loss: 1512.3618
Epoch 30/200, Loss: 1512.3200
Epoch 31/200, Loss: 1512.2834
Epoch 32/200, Loss: 1512.3972
Epoch 33/200, Loss: 1512.5139
Epoch 34/200, Loss: 1512.6314
Epoch 35/200, Loss: 1512.8135
Epoch 36/200, Loss: 1512.6333
Epoch 37/200, Loss: 1512.3931
Epoch 38/200, Loss: 1512.3384
Epoch 39/200, Loss: 1512.2140
Epoch 40/200, Loss: 1512.0552
Epoch 41/200, Loss: 1512.2913
Epoch 42/200, Loss: 1512.5656
Epoch 43/200, Loss: 1512.3314
Epoch 44/200, Loss: 1512.7541
Epoch 45/200, Loss: 1512.2613
Epoch 46/200, Loss: 1512.2216
Epoch 47/200, Loss: 1512.0807
Epoch 48/200, Loss: 1512.8205
Epoch 49/200, Loss: 1512.4168
Epoch 50/200, Loss: 1512.7983
Epoch 51/200, Loss: 1512.7270
Epoch 52/200, Loss: 1512.5071
Epoch 53/200, Loss: 1512.3767
Epoch 54/200, Loss: 1512.1527
Epoch 55/200, Loss: 1512.3484
Epoch 56/200, Loss: 1512.4061
Epoch 57/200, Loss: 1512.5265
Epoch 58/200, Loss: 1512.3486
Epoch 59/200, Loss: 1512.1300
Epoch 60/200, Loss: 1512.6021
Epoch 61/200, Loss: 1512.2421
Epoch 62/200, Loss: 1512.0483
Epoch 63/200, Loss: 1512.3104
Epoch 64/200, Loss: 1512.3829
Epoch 65/200, Loss: 1512.4090
Epoch 66/200, Loss: 1512.5407
Epoch 67/200, Loss: 1512.6461
Epoch 68/200, Loss: 1512.5844
Epoch 69/200, Loss: 1512.6689
Epoch 70/200, Loss: 1512.3295
Epoch 71/200, Loss: 1512.6448
Epoch 72/200, Loss: 1512.1931
Epoch 73/200, Loss: 1512.7411
Epoch 74/200, Loss: 1512.3024
Epoch 75/200, Loss: 1511.9479
Epoch 76/200, Loss: 1512.9164
Epoch 77/200, Loss: 1512.2788
Epoch 78/200, Loss: 1512.5332
Epoch 79/200, Loss: 1512.4187
Epoch 80/200, Loss: 1512.0502
Epoch 81/200, Loss: 1512.4952
Epoch 82/200, Loss: 1512.3028
Epoch 83/200, Loss: 1512.1706
Epoch 84/200, Loss: 1512.4173
Epoch 85/200, Loss: 1512.1286
Epoch 86/200, Loss: 1512.4819
Epoch 87/200, Loss: 1512.4739
Epoch 88/200, Loss: 1512.7083
Epoch 89/200, Loss: 1512.3066
Epoch 90/200, Loss: 1512.1186
Epoch 91/200, Loss: 1512.1357
Epoch 92/200, Loss: 1512.3058
Epoch 93/200, Loss: 1512.7771
Epoch 94/200, Loss: 1512.4245
Epoch 95/200, Loss: 1512.1001
Epoch 96/200, Loss: 1512.1329
Epoch 97/200, Loss: 1512.8055
Epoch 98/200, Loss: 1512.5628
Epoch 99/200, Loss: 1513.0665
Epoch 100/200, Loss: 1512.4215
Epoch 101/200, Loss: 1512.0746
Epoch 102/200, Loss: 1512.2680
Epoch 103/200, Loss: 1512.3050
Epoch 104/200, Loss: 1512.6683
Epoch 105/200, Loss: 1512.1540
Epoch 106/200, Loss: 1512.3430
Epoch 107/200, Loss: 1512.3594
Epoch 108/200, Loss: 1512.3864
Epoch 109/200, Loss: 1512.2497
Epoch 110/200, Loss: 1512.4238
Epoch 111/200, Loss: 1512.3129
Epoch 112/200, Loss: 1512.5531
Epoch 113/200, Loss: 1512.2793
Epoch 114/200, Loss: 1512.4344
Epoch 115/200, Loss: 1512.4171
Epoch 116/200, Loss: 1512.6574
Epoch 117/200, Loss: 1512.3978
Epoch 118/200, Loss: 1512.4623
Epoch 119/200, Loss: 1512.9287
Epoch 120/200, Loss: 1512.1230
Epoch 121/200, Loss: 1512.3595
Epoch 122/200, Loss: 1512.5633
Epoch 123/200, Loss: 1512.4162
Epoch 124/200, Loss: 1512.3186
Epoch 125/200, Loss: 1512.8883
Epoch 126/200, Loss: 1512.5124
Epoch 127/200, Loss: 1512.2868
Epoch 128/200, Loss: 1512.6091
Epoch 129/200, Loss: 1512.6505
Epoch 130/200, Loss: 1512.1904
Epoch 131/200, Loss: 1512.2166
Epoch 132/200, Loss: 1512.0976
Epoch 133/200, Loss: 1512.3080
Epoch 134/200, Loss: 1512.8011
Epoch 135/200, Loss: 1512.1294
Epoch 136/200, Loss: 1512.7387
Epoch 137/200, Loss: 1512.0776
Epoch 138/200, Loss: 1512.6423
Epoch 139/200, Loss: 1512.0430
Epoch 140/200, Loss: 1512.4410
Epoch 141/200, Loss: 1513.0719
Epoch 142/200, Loss: 1512.5924
Epoch 143/200, Loss: 1512.4702
Epoch 144/200, Loss: 1512.6663
Epoch 145/200, Loss: 1512.4957
Epoch 146/200, Loss: 1512.9503
Epoch 147/200, Loss: 1512.3843
Epoch 148/200, Loss: 1512.2553
Epoch 149/200, Loss: 1512.4466
Epoch 150/200, Loss: 1512.5964
Epoch 151/200, Loss: 1512.1056
Epoch 152/200, Loss: 1512.2345
Epoch 153/200, Loss: 1512.4315
Epoch 154/200, Loss: 1512.6639
Epoch 155/200, Loss: 1512.5573
Epoch 156/200, Loss: 1512.7169
Epoch 157/200, Loss: 1512.1080
Epoch 158/200, Loss: 1512.4323
Epoch 159/200, Loss: 1512.8714
Epoch 160/200, Loss: 1512.5042
Epoch 161/200, Loss: 1512.5167
Epoch 162/200, Loss: 1512.3252
Epoch 163/200, Loss: 1512.3724
Epoch 164/200, Loss: 1512.3984
Epoch 165/200, Loss: 1512.6304
Epoch 166/200, Loss: 1512.3316
Epoch 167/200, Loss: 1512.3100
Epoch 168/200, Loss: 1512.1350
Epoch 169/200, Loss: 1512.7181
Epoch 170/200, Loss: 1512.3482
Epoch 171/200, Loss: 1512.1664
Epoch 172/200, Loss: 1512.2128
Epoch 173/200, Loss: 1512.2101
Epoch 174/200, Loss: 1513.3008
Epoch 175/200, Loss: 1512.6429
Epoch 176/200, Loss: 1512.4787
Epoch 177/200, Loss: 1512.3672
Epoch 178/200, Loss: 1512.3887
Epoch 179/200, Loss: 1512.3421
Epoch 180/200, Loss: 1512.0910
Epoch 181/200, Loss: 1512.1504
Epoch 182/200, Loss: 1512.3498
Epoch 183/200, Loss: 1512.3745
Epoch 184/200, Loss: 1512.3394
Epoch 185/200, Loss: 1512.5246
Epoch 186/200, Loss: 1512.5458
Epoch 187/200, Loss: 1512.7377
Epoch 188/200, Loss: 1512.4030
Epoch 189/200, Loss: 1512.1215
Epoch 190/200, Loss: 1512.2089
Epoch 191/200, Loss: 1512.6750
Epoch 192/200, Loss: 1512.7474
Epoch 193/200, Loss: 1512.3745
Epoch 194/200, Loss: 1512.6562
Epoch 195/200, Loss: 1512.5580
Epoch 196/200, Loss: 1512.5276
Epoch 197/200, Loss: 1512.7408
Epoch 198/200, Loss: 1512.0881
Epoch 199/200, Loss: 1512.8256
Epoch 200/200, Loss: 1512.2570
Train Accuracy: 76.28% | Test Accuracy: 53.62%
Precision: 0.5515 | Recall: 0.5362 | F1-Score: 0.5081
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 12.79%
Precision: 0.1263 | Recall: 0.1279 | F1-Score: 0.1250


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2048 batches, Testing set - 267 batches
Epoch 1/200, Loss: 6848.1558
Epoch 2/200, Loss: 6349.4430
Epoch 3/200, Loss: 6231.9386
Epoch 4/200, Loss: 6121.8404
Epoch 5/200, Loss: 6001.9271
Epoch 6/200, Loss: 5867.6126
Epoch 7/200, Loss: 5852.4374
Epoch 8/200, Loss: 5838.9113
Epoch 9/200, Loss: 5824.2031
Epoch 10/200, Loss: 5812.8927
Epoch 11/200, Loss: 5798.5843
Epoch 12/200, Loss: 5796.7664
Epoch 13/200, Loss: 5795.2001
Epoch 14/200, Loss: 5794.5932
Epoch 15/200, Loss: 5793.1317
Epoch 16/200, Loss: 5791.3122
Epoch 17/200, Loss: 5791.4055
Epoch 18/200, Loss: 5791.2121
Epoch 19/200, Loss: 5791.1520
Epoch 20/200, Loss: 5791.3651
Epoch 21/200, Loss: 5790.9196
Epoch 22/200, Loss: 5790.9048
Epoch 23/200, Loss: 5790.8237
Epoch 24/200, Loss: 5791.1360
Epoch 25/200, Loss: 5790.7649
Epoch 26/200, Loss: 5790.9349
Epoch 27/200, Loss: 5790.8171
Epoch 28/200, Loss: 5790.9523
Epoch 29/200, Loss: 5790.8449
Epoch 30/200, Loss: 5790.7353
Epoch 31/200, Loss: 5790.9511
Epoch 32/200, Loss: 5790.9272
Epoch 33/200, Loss: 5790.5220
Epoch 34/200, Loss: 5790.9224
Epoch 35/200, Loss: 5791.4443
Epoch 36/200, Loss: 5790.3862
Epoch 37/200, Loss: 5790.9597
Epoch 38/200, Loss: 5790.6967
Epoch 39/200, Loss: 5790.7517
Epoch 40/200, Loss: 5790.5854
Epoch 41/200, Loss: 5791.0659
Epoch 42/200, Loss: 5790.3814
Epoch 43/200, Loss: 5791.3177
Epoch 44/200, Loss: 5790.9301
Epoch 45/200, Loss: 5790.7259
Epoch 46/200, Loss: 5790.6708
Epoch 47/200, Loss: 5790.9228
Epoch 48/200, Loss: 5791.4589
Epoch 49/200, Loss: 5790.8613
Epoch 50/200, Loss: 5790.9611
Epoch 51/200, Loss: 5790.9524
Epoch 52/200, Loss: 5790.6170
Epoch 53/200, Loss: 5790.7046
Epoch 54/200, Loss: 5790.8852
Epoch 55/200, Loss: 5790.7489
Epoch 56/200, Loss: 5791.0697
Epoch 57/200, Loss: 5790.7694
Epoch 58/200, Loss: 5790.6909
Epoch 59/200, Loss: 5790.6880
Epoch 60/200, Loss: 5790.6005
Epoch 61/200, Loss: 5790.9472
Epoch 62/200, Loss: 5790.6763
Epoch 63/200, Loss: 5791.0263
Epoch 64/200, Loss: 5790.4508
Epoch 65/200, Loss: 5790.7260
Epoch 66/200, Loss: 5790.8793
Epoch 67/200, Loss: 5791.1891
Epoch 68/200, Loss: 5790.8258
Epoch 69/200, Loss: 5790.3612
Epoch 70/200, Loss: 5790.8965
Epoch 71/200, Loss: 5790.8576
Epoch 72/200, Loss: 5791.0482
Epoch 73/200, Loss: 5791.0783
Epoch 74/200, Loss: 5790.7839
Epoch 75/200, Loss: 5791.1797
Epoch 76/200, Loss: 5790.7980
Epoch 77/200, Loss: 5790.9046
Epoch 78/200, Loss: 5791.3317
Epoch 79/200, Loss: 5790.6549
Epoch 80/200, Loss: 5791.3849
Epoch 81/200, Loss: 5790.8462
Epoch 82/200, Loss: 5790.6507
Epoch 83/200, Loss: 5790.7879
Epoch 84/200, Loss: 5791.0965
Epoch 85/200, Loss: 5791.1845
Epoch 86/200, Loss: 5790.9477
Epoch 87/200, Loss: 5790.8942
Epoch 88/200, Loss: 5791.0040
Epoch 89/200, Loss: 5790.7356
Epoch 90/200, Loss: 5791.0509
Epoch 91/200, Loss: 5791.0125
Epoch 92/200, Loss: 5790.9809
Epoch 93/200, Loss: 5790.8790
Epoch 94/200, Loss: 5790.9055
Epoch 95/200, Loss: 5790.8907
Epoch 96/200, Loss: 5790.7759
Epoch 97/200, Loss: 5790.4875
Epoch 98/200, Loss: 5790.7602
Epoch 99/200, Loss: 5790.6528
Epoch 100/200, Loss: 5791.0336
Epoch 101/200, Loss: 5790.7316
Epoch 102/200, Loss: 5790.3739
Epoch 103/200, Loss: 5791.2087
Epoch 104/200, Loss: 5790.8377
Epoch 105/200, Loss: 5790.7348
Epoch 106/200, Loss: 5790.4836
Epoch 107/200, Loss: 5791.0789
Epoch 108/200, Loss: 5790.9268
Epoch 109/200, Loss: 5791.0133
Epoch 110/200, Loss: 5790.9913
Epoch 111/200, Loss: 5790.4279
Epoch 112/200, Loss: 5791.1070
Epoch 113/200, Loss: 5790.8513
Epoch 114/200, Loss: 5790.4963
Epoch 115/200, Loss: 5791.2688
Epoch 116/200, Loss: 5790.6668
Epoch 117/200, Loss: 5790.9362
Epoch 118/200, Loss: 5790.6845
Epoch 119/200, Loss: 5790.8704
Epoch 120/200, Loss: 5790.5912
Epoch 121/200, Loss: 5790.5989
Epoch 122/200, Loss: 5790.6288
Epoch 123/200, Loss: 5791.1160
Epoch 124/200, Loss: 5790.9422
Epoch 125/200, Loss: 5790.9549
Epoch 126/200, Loss: 5790.5780
Epoch 127/200, Loss: 5791.0690
Epoch 128/200, Loss: 5790.5717
Epoch 129/200, Loss: 5791.2389
Epoch 130/200, Loss: 5791.2239
Epoch 131/200, Loss: 5790.9041
Epoch 132/200, Loss: 5790.7045
Epoch 133/200, Loss: 5790.8868
Epoch 134/200, Loss: 5790.6852
Epoch 135/200, Loss: 5790.4487
Epoch 136/200, Loss: 5790.9627
Epoch 137/200, Loss: 5790.6853
Epoch 138/200, Loss: 5790.6785
Epoch 139/200, Loss: 5791.1571
Epoch 140/200, Loss: 5790.6982
Epoch 141/200, Loss: 5790.8485
Epoch 142/200, Loss: 5790.9132
Epoch 143/200, Loss: 5790.7705
Epoch 144/200, Loss: 5790.8668
Epoch 145/200, Loss: 5790.7583
Epoch 146/200, Loss: 5790.4924
Epoch 147/200, Loss: 5790.8203
Epoch 148/200, Loss: 5790.7100
Epoch 149/200, Loss: 5791.0007
Epoch 150/200, Loss: 5790.5719
Epoch 151/200, Loss: 5790.7358
Epoch 152/200, Loss: 5790.5691
Epoch 153/200, Loss: 5790.6598
Epoch 154/200, Loss: 5790.6008
Epoch 155/200, Loss: 5790.9442
Epoch 156/200, Loss: 5790.5963
Epoch 157/200, Loss: 5790.6567
Epoch 158/200, Loss: 5790.5841
Epoch 159/200, Loss: 5790.6925
Epoch 160/200, Loss: 5790.9752
Epoch 161/200, Loss: 5790.6341
Epoch 162/200, Loss: 5790.8683
Epoch 163/200, Loss: 5790.9584
Epoch 164/200, Loss: 5790.6800
Epoch 165/200, Loss: 5790.7603
Epoch 166/200, Loss: 5790.9880
Epoch 167/200, Loss: 5790.8125
Epoch 168/200, Loss: 5791.0475
Epoch 169/200, Loss: 5790.6516
Epoch 170/200, Loss: 5791.1699
Epoch 171/200, Loss: 5790.9277
Epoch 172/200, Loss: 5790.8697
Epoch 173/200, Loss: 5790.7050
Epoch 174/200, Loss: 5790.7196
Epoch 175/200, Loss: 5790.8553
Epoch 176/200, Loss: 5791.0275
Epoch 177/200, Loss: 5790.7540
Epoch 178/200, Loss: 5790.6972
Epoch 179/200, Loss: 5790.6904
Epoch 180/200, Loss: 5791.0493
Epoch 181/200, Loss: 5790.9083
Epoch 182/200, Loss: 5790.8068
Epoch 183/200, Loss: 5790.7290
Epoch 184/200, Loss: 5790.6801
Epoch 185/200, Loss: 5790.7990
Epoch 186/200, Loss: 5791.3725
Epoch 187/200, Loss: 5790.6161
Epoch 188/200, Loss: 5790.5465
Epoch 189/200, Loss: 5790.5868
Epoch 190/200, Loss: 5790.8094
Epoch 191/200, Loss: 5790.9394
Epoch 192/200, Loss: 5790.7943
Epoch 193/200, Loss: 5790.7688
Epoch 194/200, Loss: 5790.7522
Epoch 195/200, Loss: 5790.8519
Epoch 196/200, Loss: 5790.8337
Epoch 197/200, Loss: 5791.0670
Epoch 198/200, Loss: 5791.1164
Epoch 199/200, Loss: 5791.1873
Epoch 200/200, Loss: 5790.8598
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 13.71% | Test Accuracy: 11.65%
Precision: 0.0533 | Recall: 0.1165 | F1-Score: 0.0667

Processing Subject 6...
Top 32 discriminative features: [26 38 46 44 14 12 18 42 41 16 40 30 22 29 45 28 37 25 63 61  2  0 59 57
 20  9 49 51 10 21  6  4]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 131968
	# per Class in Train Dataset:
		Class 0: 1700
		Class 1: 3509
		Class 2: 3490
		Class 3: 5400
		Class 4: 3976
		Class 5: 4009
		Class 6: 5400
		Class 7: 3865
		Class 8: 3472
		Class 9: 5131
		Class 10: 4143
		Class 11: 1663
		Class 12: 1700
		Class 13: 2182
		Class 14: 2910
		Class 15: 2538
		Class 16: 1723
		Class 17: 1680
		Class 18: 2055
		Class 19: 2816
		Class 20: 4695
		Class 21: 5285
		Class 22: 5179
		Class 23: 4126
		Class 24: 2574
		Class 25: 4255
		Class 26: 2252
		Class 27: 3311
		Class 28: 2807
		Class 29: 3138
		Class 30: 3481
		Class 31: 3271
		Class 32: 2674
		Class 33: 2154
		Class 34: 5248
		Class 35: 3134
		Class 36: 3416
		Class 37: 4134
		Class 38: 3472
	# of Testing Samples: 16121
	# per Class in Test Dataset:
		Class 0: 301
		Class 1: 510
		Class 2: 455
		Class 3: 600
		Class 4: 530
		Class 5: 463
		Class 6: 600
		Class 7: 349
		Class 8: 417
		Class 9: 600
		Class 10: 567
		Class 11: 208
		Class 12: 240
		Class 13: 345
		Class 14: 367
		Class 15: 318
		Class 16: 184
		Class 17: 243
		Class 18: 288
		Class 19: 364
		Class 20: 571
		Class 21: 600
		Class 22: 600
		Class 23: 566
		Class 24: 329
		Class 25: 435
		Class 26: 302
		Class 27: 361
		Class 28: 369
		Class 29: 354
		Class 30: 434
		Class 31: 413
		Class 32: 300
		Class 33: 227
		Class 34: 600
		Class 35: 403
		Class 36: 495
		Class 37: 498
		Class 38: 315
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 60.73%
Precision: 0.6563 | Recall: 0.6073 | F1-Score: 0.5857


Running Deep Learning Classifier...
DataLoader: Training set - 2062 batches, Testing set - 252 batches
Epoch 1/200, Loss: 4633.9079
Epoch 2/200, Loss: 2872.0352
Epoch 3/200, Loss: 2367.7586
Epoch 4/200, Loss: 2057.8279
Epoch 5/200, Loss: 1832.7077
Epoch 6/200, Loss: 1679.9723
Epoch 7/200, Loss: 1657.6111
Epoch 8/200, Loss: 1640.1274
Epoch 9/200, Loss: 1623.3382
Epoch 10/200, Loss: 1607.4868
Epoch 11/200, Loss: 1592.9702
Epoch 12/200, Loss: 1590.5214
Epoch 13/200, Loss: 1588.7940
Epoch 14/200, Loss: 1587.2058
Epoch 15/200, Loss: 1585.6022
Epoch 16/200, Loss: 1583.9335
Epoch 17/200, Loss: 1583.7490
Epoch 18/200, Loss: 1583.5715
Epoch 19/200, Loss: 1583.4043
Epoch 20/200, Loss: 1583.2398
Epoch 21/200, Loss: 1583.0494
Epoch 22/200, Loss: 1583.0400
Epoch 23/200, Loss: 1583.0309
Epoch 24/200, Loss: 1583.0221
Epoch 25/200, Loss: 1583.0128
Epoch 26/200, Loss: 1582.9999
Epoch 27/200, Loss: 1582.9998
Epoch 28/200, Loss: 1582.9997
Epoch 29/200, Loss: 1582.9996
Epoch 30/200, Loss: 1582.9995
Epoch 31/200, Loss: 1582.9994
Epoch 32/200, Loss: 1582.9994
Epoch 33/200, Loss: 1582.9994
Epoch 34/200, Loss: 1582.9994
Epoch 35/200, Loss: 1582.9994
Epoch 36/200, Loss: 1582.9994
Epoch 37/200, Loss: 1582.9994
Epoch 38/200, Loss: 1582.9994
Epoch 39/200, Loss: 1582.9994
Epoch 40/200, Loss: 1582.9994
Epoch 41/200, Loss: 1582.9994
Epoch 42/200, Loss: 1582.9994
Epoch 43/200, Loss: 1582.9994
Epoch 44/200, Loss: 1582.9994
Epoch 45/200, Loss: 1582.9994
Epoch 46/200, Loss: 1582.9994
Epoch 47/200, Loss: 1582.9994
Epoch 48/200, Loss: 1582.9994
Epoch 49/200, Loss: 1582.9994
Epoch 50/200, Loss: 1582.9994
Epoch 51/200, Loss: 1582.9994
Epoch 52/200, Loss: 1582.9994
Epoch 53/200, Loss: 1582.9994
Epoch 54/200, Loss: 1582.9994
Epoch 55/200, Loss: 1582.9994
Epoch 56/200, Loss: 1582.9994
Epoch 57/200, Loss: 1582.9994
Epoch 58/200, Loss: 1582.9994
Epoch 59/200, Loss: 1582.9994
Epoch 60/200, Loss: 1582.9994
Epoch 61/200, Loss: 1582.9994
Epoch 62/200, Loss: 1582.9994
Epoch 63/200, Loss: 1582.9994
Epoch 64/200, Loss: 1582.9994
Epoch 65/200, Loss: 1582.9994
Epoch 66/200, Loss: 1582.9994
Epoch 67/200, Loss: 1582.9994
Epoch 68/200, Loss: 1582.9994
Epoch 69/200, Loss: 1582.9994
Epoch 70/200, Loss: 1582.9994
Epoch 71/200, Loss: 1582.9994
Epoch 72/200, Loss: 1582.9994
Epoch 73/200, Loss: 1582.9994
Epoch 74/200, Loss: 1582.9994
Epoch 75/200, Loss: 1582.9994
Epoch 76/200, Loss: 1582.9994
Epoch 77/200, Loss: 1582.9994
Epoch 78/200, Loss: 1582.9994
Epoch 79/200, Loss: 1582.9994
Epoch 80/200, Loss: 1582.9994
Epoch 81/200, Loss: 1582.9994
Epoch 82/200, Loss: 1582.9994
Epoch 83/200, Loss: 1582.9994
Epoch 84/200, Loss: 1582.9994
Epoch 85/200, Loss: 1582.9994
Epoch 86/200, Loss: 1582.9994
Epoch 87/200, Loss: 1582.9994
Epoch 88/200, Loss: 1582.9994
Epoch 89/200, Loss: 1582.9994
Epoch 90/200, Loss: 1582.9994
Epoch 91/200, Loss: 1582.9994
Epoch 92/200, Loss: 1582.9994
Epoch 93/200, Loss: 1582.9994
Epoch 94/200, Loss: 1582.9994
Epoch 95/200, Loss: 1582.9994
Epoch 96/200, Loss: 1582.9994
Epoch 97/200, Loss: 1582.9994
Epoch 98/200, Loss: 1582.9994
Epoch 99/200, Loss: 1582.9994
Epoch 100/200, Loss: 1582.9994
Epoch 101/200, Loss: 1582.9994
Epoch 102/200, Loss: 1582.9994
Epoch 103/200, Loss: 1582.9994
Epoch 104/200, Loss: 1582.9994
Epoch 105/200, Loss: 1582.9994
Epoch 106/200, Loss: 1582.9994
Epoch 107/200, Loss: 1582.9994
Epoch 108/200, Loss: 1582.9994
Epoch 109/200, Loss: 1582.9994
Epoch 110/200, Loss: 1582.9994
Epoch 111/200, Loss: 1582.9994
Epoch 112/200, Loss: 1582.9994
Epoch 113/200, Loss: 1582.9994
Epoch 114/200, Loss: 1582.9994
Epoch 115/200, Loss: 1582.9994
Epoch 116/200, Loss: 1582.9994
Epoch 117/200, Loss: 1582.9994
Epoch 118/200, Loss: 1582.9994
Epoch 119/200, Loss: 1582.9994
Epoch 120/200, Loss: 1582.9994
Epoch 121/200, Loss: 1582.9994
Epoch 122/200, Loss: 1582.9994
Epoch 123/200, Loss: 1582.9994
Epoch 124/200, Loss: 1582.9994
Epoch 125/200, Loss: 1582.9994
Epoch 126/200, Loss: 1582.9994
Epoch 127/200, Loss: 1582.9994
Epoch 128/200, Loss: 1582.9994
Epoch 129/200, Loss: 1582.9994
Epoch 130/200, Loss: 1582.9994
Epoch 131/200, Loss: 1582.9994
Epoch 132/200, Loss: 1582.9994
Epoch 133/200, Loss: 1582.9994
Epoch 134/200, Loss: 1582.9994
Epoch 135/200, Loss: 1582.9994
Epoch 136/200, Loss: 1582.9994
Epoch 137/200, Loss: 1582.9994
Epoch 138/200, Loss: 1582.9994
Epoch 139/200, Loss: 1582.9994
Epoch 140/200, Loss: 1582.9994
Epoch 141/200, Loss: 1582.9994
Epoch 142/200, Loss: 1582.9994
Epoch 143/200, Loss: 1582.9994
Epoch 144/200, Loss: 1582.9994
Epoch 145/200, Loss: 1582.9994
Epoch 146/200, Loss: 1582.9994
Epoch 147/200, Loss: 1582.9994
Epoch 148/200, Loss: 1582.9994
Epoch 149/200, Loss: 1582.9994
Epoch 150/200, Loss: 1582.9994
Epoch 151/200, Loss: 1582.9994
Epoch 152/200, Loss: 1582.9994
Epoch 153/200, Loss: 1582.9994
Epoch 154/200, Loss: 1582.9994
Epoch 155/200, Loss: 1582.9994
Epoch 156/200, Loss: 1582.9994
Epoch 157/200, Loss: 1582.9994
Epoch 158/200, Loss: 1582.9994
Epoch 159/200, Loss: 1582.9994
Epoch 160/200, Loss: 1582.9994
Epoch 161/200, Loss: 1582.9994
Epoch 162/200, Loss: 1582.9994
Epoch 163/200, Loss: 1582.9994
Epoch 164/200, Loss: 1582.9994
Epoch 165/200, Loss: 1582.9994
Epoch 166/200, Loss: 1582.9994
Epoch 167/200, Loss: 1582.9994
Epoch 168/200, Loss: 1582.9994
Epoch 169/200, Loss: 1582.9994
Epoch 170/200, Loss: 1582.9994
Epoch 171/200, Loss: 1582.9994
Epoch 172/200, Loss: 1582.9994
Epoch 173/200, Loss: 1582.9994
Epoch 174/200, Loss: 1582.9994
Epoch 175/200, Loss: 1582.9994
Epoch 176/200, Loss: 1582.9994
Epoch 177/200, Loss: 1582.9994
Epoch 178/200, Loss: 1582.9994
Epoch 179/200, Loss: 1582.9994
Epoch 180/200, Loss: 1582.9994
Epoch 181/200, Loss: 1582.9994
Epoch 182/200, Loss: 1582.9994
Epoch 183/200, Loss: 1582.9994
Epoch 184/200, Loss: 1582.9994
Epoch 185/200, Loss: 1582.9994
Epoch 186/200, Loss: 1582.9994
Epoch 187/200, Loss: 1582.9994
Epoch 188/200, Loss: 1582.9994
Epoch 189/200, Loss: 1582.9994
Epoch 190/200, Loss: 1582.9994
Epoch 191/200, Loss: 1582.9994
Epoch 192/200, Loss: 1582.9994
Epoch 193/200, Loss: 1582.9994
Epoch 194/200, Loss: 1582.9994
Epoch 195/200, Loss: 1582.9994
Epoch 196/200, Loss: 1582.9994
Epoch 197/200, Loss: 1582.9994
Epoch 198/200, Loss: 1582.9994
Epoch 199/200, Loss: 1582.9994
Epoch 200/200, Loss: 1582.9994
Train Accuracy: 75.50% | Test Accuracy: 53.21%
Precision: 0.5260 | Recall: 0.5321 | F1-Score: 0.5080
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 11.85%
Precision: 0.1188 | Recall: 0.1185 | F1-Score: 0.1111


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2062 batches, Testing set - 252 batches
Epoch 1/200, Loss: 6893.0939
Epoch 2/200, Loss: 6450.9488
Epoch 3/200, Loss: 6315.7425
Epoch 4/200, Loss: 6249.1733
Epoch 5/200, Loss: 6196.9368
Epoch 6/200, Loss: 6124.4760
Epoch 7/200, Loss: 6116.3376
Epoch 8/200, Loss: 6108.8005
Epoch 9/200, Loss: 6101.5997
Epoch 10/200, Loss: 6093.7132
Epoch 11/200, Loss: 6085.3427
Epoch 12/200, Loss: 6084.0919
Epoch 13/200, Loss: 6083.3102
Epoch 14/200, Loss: 6082.5352
Epoch 15/200, Loss: 6081.6658
Epoch 16/200, Loss: 6080.8453
Epoch 17/200, Loss: 6080.6892
Epoch 18/200, Loss: 6080.5914
Epoch 19/200, Loss: 6080.5034
Epoch 20/200, Loss: 6080.4353
Epoch 21/200, Loss: 6080.3050
Epoch 22/200, Loss: 6080.3003
Epoch 23/200, Loss: 6080.2952
Epoch 24/200, Loss: 6080.2913
Epoch 25/200, Loss: 6080.2870
Epoch 26/200, Loss: 6080.2769
Epoch 27/200, Loss: 6080.2769
Epoch 28/200, Loss: 6080.2768
Epoch 29/200, Loss: 6080.2768
Epoch 30/200, Loss: 6080.2767
Epoch 31/200, Loss: 6080.2766
Epoch 32/200, Loss: 6080.2766
Epoch 33/200, Loss: 6080.2766
Epoch 34/200, Loss: 6080.2766
Epoch 35/200, Loss: 6080.2766
Epoch 36/200, Loss: 6080.2766
Epoch 37/200, Loss: 6080.2766
Epoch 38/200, Loss: 6080.2766
Epoch 39/200, Loss: 6080.2766
Epoch 40/200, Loss: 6080.2766
Epoch 41/200, Loss: 6080.2766
Epoch 42/200, Loss: 6080.2766
Epoch 43/200, Loss: 6080.2766
Epoch 44/200, Loss: 6080.2766
Epoch 45/200, Loss: 6080.2766
Epoch 46/200, Loss: 6080.2766
Epoch 47/200, Loss: 6080.2766
Epoch 48/200, Loss: 6080.2766
Epoch 49/200, Loss: 6080.2766
Epoch 50/200, Loss: 6080.2766
Epoch 51/200, Loss: 6080.2766
Epoch 52/200, Loss: 6080.2766
Epoch 53/200, Loss: 6080.2766
Epoch 54/200, Loss: 6080.2766
Epoch 55/200, Loss: 6080.2766
Epoch 56/200, Loss: 6080.2766
Epoch 57/200, Loss: 6080.2766
Epoch 58/200, Loss: 6080.2766
Epoch 59/200, Loss: 6080.2766
Epoch 60/200, Loss: 6080.2766
Epoch 61/200, Loss: 6080.2766
Epoch 62/200, Loss: 6080.2766
Epoch 63/200, Loss: 6080.2766
Epoch 64/200, Loss: 6080.2766
Epoch 65/200, Loss: 6080.2766
Epoch 66/200, Loss: 6080.2766
Epoch 67/200, Loss: 6080.2766
Epoch 68/200, Loss: 6080.2766
Epoch 69/200, Loss: 6080.2766
Epoch 70/200, Loss: 6080.2766
Epoch 71/200, Loss: 6080.2766
Epoch 72/200, Loss: 6080.2766
Epoch 73/200, Loss: 6080.2766
Epoch 74/200, Loss: 6080.2766
Epoch 75/200, Loss: 6080.2766
Epoch 76/200, Loss: 6080.2766
Epoch 77/200, Loss: 6080.2766
Epoch 78/200, Loss: 6080.2766
Epoch 79/200, Loss: 6080.2766
Epoch 80/200, Loss: 6080.2766
Epoch 81/200, Loss: 6080.2766
Epoch 82/200, Loss: 6080.2766
Epoch 83/200, Loss: 6080.2766
Epoch 84/200, Loss: 6080.2766
Epoch 85/200, Loss: 6080.2766
Epoch 86/200, Loss: 6080.2766
Epoch 87/200, Loss: 6080.2766
Epoch 88/200, Loss: 6080.2766
Epoch 89/200, Loss: 6080.2766
Epoch 90/200, Loss: 6080.2766
Epoch 91/200, Loss: 6080.2766
Epoch 92/200, Loss: 6080.2766
Epoch 93/200, Loss: 6080.2766
Epoch 94/200, Loss: 6080.2766
Epoch 95/200, Loss: 6080.2766
Epoch 96/200, Loss: 6080.2766
Epoch 97/200, Loss: 6080.2766
Epoch 98/200, Loss: 6080.2766
Epoch 99/200, Loss: 6080.2766
Epoch 100/200, Loss: 6080.2766
Epoch 101/200, Loss: 6080.2766
Epoch 102/200, Loss: 6080.2766
Epoch 103/200, Loss: 6080.2766
Epoch 104/200, Loss: 6080.2766
Epoch 105/200, Loss: 6080.2766
Epoch 106/200, Loss: 6080.2766
Epoch 107/200, Loss: 6080.2766
Epoch 108/200, Loss: 6080.2766
Epoch 109/200, Loss: 6080.2766
Epoch 110/200, Loss: 6080.2766
Epoch 111/200, Loss: 6080.2766
Epoch 112/200, Loss: 6080.2766
Epoch 113/200, Loss: 6080.2766
Epoch 114/200, Loss: 6080.2766
Epoch 115/200, Loss: 6080.2766
Epoch 116/200, Loss: 6080.2766
Epoch 117/200, Loss: 6080.2766
Epoch 118/200, Loss: 6080.2766
Epoch 119/200, Loss: 6080.2766
Epoch 120/200, Loss: 6080.2766
Epoch 121/200, Loss: 6080.2766
Epoch 122/200, Loss: 6080.2766
Epoch 123/200, Loss: 6080.2766
Epoch 124/200, Loss: 6080.2766
Epoch 125/200, Loss: 6080.2766
Epoch 126/200, Loss: 6080.2766
Epoch 127/200, Loss: 6080.2766
Epoch 128/200, Loss: 6080.2766
Epoch 129/200, Loss: 6080.2766
Epoch 130/200, Loss: 6080.2766
Epoch 131/200, Loss: 6080.2766
Epoch 132/200, Loss: 6080.2766
Epoch 133/200, Loss: 6080.2766
Epoch 134/200, Loss: 6080.2766
Epoch 135/200, Loss: 6080.2766
Epoch 136/200, Loss: 6080.2766
Epoch 137/200, Loss: 6080.2766
Epoch 138/200, Loss: 6080.2766
Epoch 139/200, Loss: 6080.2766
Epoch 140/200, Loss: 6080.2766
Epoch 141/200, Loss: 6080.2766
Epoch 142/200, Loss: 6080.2766
Epoch 143/200, Loss: 6080.2766
Epoch 144/200, Loss: 6080.2766
Epoch 145/200, Loss: 6080.2766
Epoch 146/200, Loss: 6080.2766
Epoch 147/200, Loss: 6080.2766
Epoch 148/200, Loss: 6080.2766
Epoch 149/200, Loss: 6080.2766
Epoch 150/200, Loss: 6080.2766
Epoch 151/200, Loss: 6080.2766
Epoch 152/200, Loss: 6080.2766
Epoch 153/200, Loss: 6080.2766
Epoch 154/200, Loss: 6080.2766
Epoch 155/200, Loss: 6080.2766
Epoch 156/200, Loss: 6080.2766
Epoch 157/200, Loss: 6080.2766
Epoch 158/200, Loss: 6080.2766
Epoch 159/200, Loss: 6080.2766
Epoch 160/200, Loss: 6080.2766
Epoch 161/200, Loss: 6080.2766
Epoch 162/200, Loss: 6080.2766
Epoch 163/200, Loss: 6080.2766
Epoch 164/200, Loss: 6080.2766
Epoch 165/200, Loss: 6080.2766
Epoch 166/200, Loss: 6080.2766
Epoch 167/200, Loss: 6080.2766
Epoch 168/200, Loss: 6080.2766
Epoch 169/200, Loss: 6080.2766
Epoch 170/200, Loss: 6080.2766
Epoch 171/200, Loss: 6080.2766
Epoch 172/200, Loss: 6080.2766
Epoch 173/200, Loss: 6080.2766
Epoch 174/200, Loss: 6080.2766
Epoch 175/200, Loss: 6080.2766
Epoch 176/200, Loss: 6080.2766
Epoch 177/200, Loss: 6080.2766
Epoch 178/200, Loss: 6080.2766
Epoch 179/200, Loss: 6080.2766
Epoch 180/200, Loss: 6080.2766
Epoch 181/200, Loss: 6080.2766
Epoch 182/200, Loss: 6080.2766
Epoch 183/200, Loss: 6080.2766
Epoch 184/200, Loss: 6080.2766
Epoch 185/200, Loss: 6080.2766
Epoch 186/200, Loss: 6080.2766
Epoch 187/200, Loss: 6080.2766
Epoch 188/200, Loss: 6080.2766
Epoch 189/200, Loss: 6080.2766
Epoch 190/200, Loss: 6080.2766
Epoch 191/200, Loss: 6080.2766
Epoch 192/200, Loss: 6080.2766
Epoch 193/200, Loss: 6080.2766
Epoch 194/200, Loss: 6080.2766
Epoch 195/200, Loss: 6080.2766
Epoch 196/200, Loss: 6080.2766
Epoch 197/200, Loss: 6080.2766
Epoch 198/200, Loss: 6080.2766
Epoch 199/200, Loss: 6080.2766
Epoch 200/200, Loss: 6080.2766
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 13.16% | Test Accuracy: 11.86%
Precision: 0.0605 | Recall: 0.1186 | F1-Score: 0.0636

Processing Subject 7...
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:112: UserWarning: Features [39 43 47 48 52] are constant.
  warnings.warn("Features %s are constant." % constant_features_idx, UserWarning)
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\feature_selection\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide
  f = msb / msw
Top 32 discriminative features: [43 39 47 48 52 26 38 46 44 14 12 18 41 16 42 40 45 29 30 22 63 61 28 59
 57  2  0 49 51 37 25 10]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 132018
	# per Class in Train Dataset:
		Class 0: 1731
		Class 1: 3579
		Class 2: 3536
		Class 3: 5400
		Class 4: 4045
		Class 5: 3966
		Class 6: 5400
		Class 7: 3814
		Class 8: 3531
		Class 9: 5131
		Class 10: 4228
		Class 11: 1638
		Class 12: 1740
		Class 13: 2233
		Class 14: 2891
		Class 15: 2501
		Class 16: 1687
		Class 17: 1713
		Class 18: 2033
		Class 19: 2825
		Class 20: 4666
		Class 21: 5285
		Class 22: 5179
		Class 23: 4105
		Class 24: 2580
		Class 25: 4228
		Class 26: 2284
		Class 27: 3309
		Class 28: 2850
		Class 29: 3142
		Class 30: 3480
		Class 31: 3235
		Class 32: 2658
		Class 33: 2131
		Class 34: 5248
		Class 35: 3133
		Class 36: 3311
		Class 37: 4148
		Class 38: 3424
	# of Testing Samples: 16071
	# per Class in Test Dataset:
		Class 0: 270
		Class 1: 440
		Class 2: 409
		Class 3: 600
		Class 4: 461
		Class 5: 506
		Class 6: 600
		Class 7: 400
		Class 8: 358
		Class 9: 600
		Class 10: 482
		Class 11: 233
		Class 12: 200
		Class 13: 294
		Class 14: 386
		Class 15: 355
		Class 16: 220
		Class 17: 210
		Class 18: 310
		Class 19: 355
		Class 20: 600
		Class 21: 600
		Class 22: 600
		Class 23: 587
		Class 24: 323
		Class 25: 462
		Class 26: 270
		Class 27: 363
		Class 28: 326
		Class 29: 350
		Class 30: 435
		Class 31: 449
		Class 32: 316
		Class 33: 250
		Class 34: 600
		Class 35: 404
		Class 36: 600
		Class 37: 484
		Class 38: 363
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 60.74%
Precision: 0.6071 | Recall: 0.6074 | F1-Score: 0.5836


Running Deep Learning Classifier...
DataLoader: Training set - 2063 batches, Testing set - 252 batches
Epoch 1/200, Loss: 4567.8328
Epoch 2/200, Loss: 3048.9848
Epoch 3/200, Loss: 2620.5990
Epoch 4/200, Loss: 2345.7784
Epoch 5/200, Loss: 2143.5818
Epoch 6/200, Loss: 1993.2101
Epoch 7/200, Loss: 1971.7191
Epoch 8/200, Loss: 1955.0876
Epoch 9/200, Loss: 1939.0806
Epoch 10/200, Loss: 1923.5098
Epoch 11/200, Loss: 1909.4736
Epoch 12/200, Loss: 1906.9796
Epoch 13/200, Loss: 1905.3652
Epoch 14/200, Loss: 1903.6912
Epoch 15/200, Loss: 1902.3012
Epoch 16/200, Loss: 1900.5136
Epoch 17/200, Loss: 1900.3036
Epoch 18/200, Loss: 1900.1892
Epoch 19/200, Loss: 1900.0008
Epoch 20/200, Loss: 1899.8755
Epoch 21/200, Loss: 1899.6687
Epoch 22/200, Loss: 1899.6422
Epoch 23/200, Loss: 1899.6438
Epoch 24/200, Loss: 1899.6176
Epoch 25/200, Loss: 1899.5877
Epoch 26/200, Loss: 1899.6206
Epoch 27/200, Loss: 1899.6188
Epoch 28/200, Loss: 1899.5979
Epoch 29/200, Loss: 1899.6337
Epoch 30/200, Loss: 1899.6187
Epoch 31/200, Loss: 1899.6235
Epoch 32/200, Loss: 1899.5981
Epoch 33/200, Loss: 1899.5928
Epoch 34/200, Loss: 1899.5910
Epoch 35/200, Loss: 1899.5718
Epoch 36/200, Loss: 1899.6520
Epoch 37/200, Loss: 1899.6037
Epoch 38/200, Loss: 1899.5808
Epoch 39/200, Loss: 1899.7228
Epoch 40/200, Loss: 1899.6145
Epoch 41/200, Loss: 1899.5476
Epoch 42/200, Loss: 1899.6390
Epoch 43/200, Loss: 1899.6284
Epoch 44/200, Loss: 1899.5758
Epoch 45/200, Loss: 1899.6534
Epoch 46/200, Loss: 1899.6183
Epoch 47/200, Loss: 1899.5980
Epoch 48/200, Loss: 1899.6093
Epoch 49/200, Loss: 1899.6144
Epoch 50/200, Loss: 1899.5897
Epoch 51/200, Loss: 1899.6199
Epoch 52/200, Loss: 1899.6307
Epoch 53/200, Loss: 1899.5845
Epoch 54/200, Loss: 1899.5854
Epoch 55/200, Loss: 1899.5727
Epoch 56/200, Loss: 1899.5985
Epoch 57/200, Loss: 1899.5753
Epoch 58/200, Loss: 1899.6198
Epoch 59/200, Loss: 1899.6056
Epoch 60/200, Loss: 1899.5685
Epoch 61/200, Loss: 1899.5735
Epoch 62/200, Loss: 1899.6065
Epoch 63/200, Loss: 1899.5734
Epoch 64/200, Loss: 1899.6287
Epoch 65/200, Loss: 1899.5890
Epoch 66/200, Loss: 1899.6130
Epoch 67/200, Loss: 1899.6228
Epoch 68/200, Loss: 1899.6072
Epoch 69/200, Loss: 1899.5998
Epoch 70/200, Loss: 1899.6095
Epoch 71/200, Loss: 1899.6033
Epoch 72/200, Loss: 1899.5946
Epoch 73/200, Loss: 1899.5700
Epoch 74/200, Loss: 1899.5701
Epoch 75/200, Loss: 1899.5975
Epoch 76/200, Loss: 1899.6056
Epoch 77/200, Loss: 1899.6055
Epoch 78/200, Loss: 1899.5789
Epoch 79/200, Loss: 1899.5929
Epoch 80/200, Loss: 1899.5897
Epoch 81/200, Loss: 1899.6371
Epoch 82/200, Loss: 1899.5725
Epoch 83/200, Loss: 1899.5878
Epoch 84/200, Loss: 1899.5877
Epoch 85/200, Loss: 1899.6408
Epoch 86/200, Loss: 1899.6080
Epoch 87/200, Loss: 1899.6008
Epoch 88/200, Loss: 1899.5781
Epoch 89/200, Loss: 1899.6597
Epoch 90/200, Loss: 1899.6089
Epoch 91/200, Loss: 1899.6134
Epoch 92/200, Loss: 1899.6470
Epoch 93/200, Loss: 1899.6328
Epoch 94/200, Loss: 1899.5718
Epoch 95/200, Loss: 1899.6107
Epoch 96/200, Loss: 1899.6103
Epoch 97/200, Loss: 1899.6090
Epoch 98/200, Loss: 1899.5760
Epoch 99/200, Loss: 1899.5711
Epoch 100/200, Loss: 1899.6586
Epoch 101/200, Loss: 1899.5784
Epoch 102/200, Loss: 1899.6005
Epoch 103/200, Loss: 1899.6162
Epoch 104/200, Loss: 1899.6450
Epoch 105/200, Loss: 1899.5505
Epoch 106/200, Loss: 1899.5411
Epoch 107/200, Loss: 1899.6065
Epoch 108/200, Loss: 1899.5567
Epoch 109/200, Loss: 1899.6242
Epoch 110/200, Loss: 1899.6095
Epoch 111/200, Loss: 1899.5942
Epoch 112/200, Loss: 1899.6511
Epoch 113/200, Loss: 1899.5700
Epoch 114/200, Loss: 1899.5986
Epoch 115/200, Loss: 1899.5931
Epoch 116/200, Loss: 1899.6053
Epoch 117/200, Loss: 1899.6575
Epoch 118/200, Loss: 1899.6294
Epoch 119/200, Loss: 1899.5864
Epoch 120/200, Loss: 1899.6222
Epoch 121/200, Loss: 1899.6088
Epoch 122/200, Loss: 1899.5995
Epoch 123/200, Loss: 1899.6684
Epoch 124/200, Loss: 1899.6138
Epoch 125/200, Loss: 1899.6040
Epoch 126/200, Loss: 1899.6251
Epoch 127/200, Loss: 1899.5937
Epoch 128/200, Loss: 1899.5985
Epoch 129/200, Loss: 1899.6157
Epoch 130/200, Loss: 1899.5993
Epoch 131/200, Loss: 1899.5823
Epoch 132/200, Loss: 1899.5826
Epoch 133/200, Loss: 1899.6186
Epoch 134/200, Loss: 1899.5650
Epoch 135/200, Loss: 1899.5586
Epoch 136/200, Loss: 1899.6274
Epoch 137/200, Loss: 1899.5689
Epoch 138/200, Loss: 1899.6159
Epoch 139/200, Loss: 1899.6155
Epoch 140/200, Loss: 1899.5750
Epoch 141/200, Loss: 1899.6038
Epoch 142/200, Loss: 1899.6249
Epoch 143/200, Loss: 1899.5833
Epoch 144/200, Loss: 1899.6273
Epoch 145/200, Loss: 1899.6155
Epoch 146/200, Loss: 1899.6447
Epoch 147/200, Loss: 1899.6285
Epoch 148/200, Loss: 1899.6165
Epoch 149/200, Loss: 1899.5748
Epoch 150/200, Loss: 1899.6178
Epoch 151/200, Loss: 1899.6032
Epoch 152/200, Loss: 1899.6967
Epoch 153/200, Loss: 1899.5748
Epoch 154/200, Loss: 1899.6451
Epoch 155/200, Loss: 1899.6377
Epoch 156/200, Loss: 1899.5498
Epoch 157/200, Loss: 1899.5915
Epoch 158/200, Loss: 1899.6276
Epoch 159/200, Loss: 1899.5611
Epoch 160/200, Loss: 1899.5789
Epoch 161/200, Loss: 1899.6079
Epoch 162/200, Loss: 1899.5920
Epoch 163/200, Loss: 1899.5914
Epoch 164/200, Loss: 1899.6299
Epoch 165/200, Loss: 1899.6136
Epoch 166/200, Loss: 1899.5801
Epoch 167/200, Loss: 1899.5626
Epoch 168/200, Loss: 1899.5879
Epoch 169/200, Loss: 1899.5928
Epoch 170/200, Loss: 1899.5757
Epoch 171/200, Loss: 1899.6048
Epoch 172/200, Loss: 1899.6083
Epoch 173/200, Loss: 1899.6447
Epoch 174/200, Loss: 1899.6080
Epoch 175/200, Loss: 1899.5888
Epoch 176/200, Loss: 1899.6424
Epoch 177/200, Loss: 1899.5809
Epoch 178/200, Loss: 1899.5966
Epoch 179/200, Loss: 1899.5891
Epoch 180/200, Loss: 1899.6010
Epoch 181/200, Loss: 1899.6285
Epoch 182/200, Loss: 1899.6199
Epoch 183/200, Loss: 1899.6080
Epoch 184/200, Loss: 1899.6070
Epoch 185/200, Loss: 1899.5770
Epoch 186/200, Loss: 1899.5927
Epoch 187/200, Loss: 1899.5560
Epoch 188/200, Loss: 1899.6439
Epoch 189/200, Loss: 1899.6373
Epoch 190/200, Loss: 1899.6263
Epoch 191/200, Loss: 1899.6854
Epoch 192/200, Loss: 1899.5997
Epoch 193/200, Loss: 1899.5857
Epoch 194/200, Loss: 1899.6090
Epoch 195/200, Loss: 1899.6164
Epoch 196/200, Loss: 1899.6234
Epoch 197/200, Loss: 1899.6309
Epoch 198/200, Loss: 1899.5783
Epoch 199/200, Loss: 1899.6205
Epoch 200/200, Loss: 1899.5878
Train Accuracy: 70.59% | Test Accuracy: 54.18%
Precision: 0.5569 | Recall: 0.5418 | F1-Score: 0.5238
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
Train Accuracy: 100.00% | Test Accuracy: 12.91%
Precision: 0.1262 | Recall: 0.1291 | F1-Score: 0.1259


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2063 batches, Testing set - 252 batches
Epoch 1/200, Loss: 7041.4817
Epoch 2/200, Loss: 6416.8329
Epoch 3/200, Loss: 6283.3145
Epoch 4/200, Loss: 6219.0772
Epoch 5/200, Loss: 6176.1783
Epoch 6/200, Loss: 6111.7177
Epoch 7/200, Loss: 6105.8219
Epoch 8/200, Loss: 6101.2796
Epoch 9/200, Loss: 6097.3578
Epoch 10/200, Loss: 6092.9187
Epoch 11/200, Loss: 6085.9901
Epoch 12/200, Loss: 6085.1012
Epoch 13/200, Loss: 6084.6869
Epoch 14/200, Loss: 6084.3876
Epoch 15/200, Loss: 6083.8844
Epoch 16/200, Loss: 6083.1090
Epoch 17/200, Loss: 6083.0301
Epoch 18/200, Loss: 6083.0078
Epoch 19/200, Loss: 6082.9318
Epoch 20/200, Loss: 6082.8699
Epoch 21/200, Loss: 6082.7944
Epoch 22/200, Loss: 6082.7811
Epoch 23/200, Loss: 6082.7685
Epoch 24/200, Loss: 6082.7690
Epoch 25/200, Loss: 6082.7566
Epoch 26/200, Loss: 6082.7368
Epoch 27/200, Loss: 6082.7657
Epoch 28/200, Loss: 6082.7375
Epoch 29/200, Loss: 6082.7657
Epoch 30/200, Loss: 6082.7330
Epoch 31/200, Loss: 6082.7366
Epoch 32/200, Loss: 6082.7605
Epoch 33/200, Loss: 6082.7241
Epoch 34/200, Loss: 6082.7598
Epoch 35/200, Loss: 6082.7264
Epoch 36/200, Loss: 6082.7390
Epoch 37/200, Loss: 6082.7350
Epoch 38/200, Loss: 6082.7508
Epoch 39/200, Loss: 6082.7531
Epoch 40/200, Loss: 6082.7301
Epoch 41/200, Loss: 6082.7573
Epoch 42/200, Loss: 6082.7728
Epoch 43/200, Loss: 6082.7282
Epoch 44/200, Loss: 6082.7165
Epoch 45/200, Loss: 6082.7868
Epoch 46/200, Loss: 6082.7778
Epoch 47/200, Loss: 6082.7481
Epoch 48/200, Loss: 6082.7284
Epoch 49/200, Loss: 6082.7435
Epoch 50/200, Loss: 6082.7399
Epoch 51/200, Loss: 6082.7466
Epoch 52/200, Loss: 6082.7420
Epoch 53/200, Loss: 6082.7659
Epoch 54/200, Loss: 6082.7871
Epoch 55/200, Loss: 6082.7498
Epoch 56/200, Loss: 6082.7644
Epoch 57/200, Loss: 6082.7654
Epoch 58/200, Loss: 6082.7697
Epoch 59/200, Loss: 6082.7201
Epoch 60/200, Loss: 6082.7467
Epoch 61/200, Loss: 6082.7459
Epoch 62/200, Loss: 6082.7391
Epoch 63/200, Loss: 6082.7368
Epoch 64/200, Loss: 6082.7303
Epoch 65/200, Loss: 6082.7518
Epoch 66/200, Loss: 6082.7698
Epoch 67/200, Loss: 6082.7379
Epoch 68/200, Loss: 6082.7631
Epoch 69/200, Loss: 6082.7416
Epoch 70/200, Loss: 6082.7344
Epoch 71/200, Loss: 6082.7246
Epoch 72/200, Loss: 6082.7349
Epoch 73/200, Loss: 6082.7798
Epoch 74/200, Loss: 6082.7275
Epoch 75/200, Loss: 6082.7089
Epoch 76/200, Loss: 6082.7252
Epoch 77/200, Loss: 6082.7918
Epoch 78/200, Loss: 6082.7520
Epoch 79/200, Loss: 6082.7370
Epoch 80/200, Loss: 6082.7691
Epoch 81/200, Loss: 6082.7511
Epoch 82/200, Loss: 6082.7665
Epoch 83/200, Loss: 6082.7138
Epoch 84/200, Loss: 6082.7506
Epoch 85/200, Loss: 6082.7313
Epoch 86/200, Loss: 6082.7975
Epoch 87/200, Loss: 6082.7756
Epoch 88/200, Loss: 6082.7584
Epoch 89/200, Loss: 6082.7797
Epoch 90/200, Loss: 6082.7588
Epoch 91/200, Loss: 6082.7798
Epoch 92/200, Loss: 6082.7663
Epoch 93/200, Loss: 6082.7346
Epoch 94/200, Loss: 6082.7525
Epoch 95/200, Loss: 6082.7316
Epoch 96/200, Loss: 6082.7245
Epoch 97/200, Loss: 6082.7133
Epoch 98/200, Loss: 6082.7758
Epoch 99/200, Loss: 6082.7339
Epoch 100/200, Loss: 6082.7460
Epoch 101/200, Loss: 6082.7422
Epoch 102/200, Loss: 6082.7774
Epoch 103/200, Loss: 6082.7609
Epoch 104/200, Loss: 6082.7282
Epoch 105/200, Loss: 6082.7205
Epoch 106/200, Loss: 6082.7239
Epoch 107/200, Loss: 6082.7503
Epoch 108/200, Loss: 6082.7885
Epoch 109/200, Loss: 6082.7089
Epoch 110/200, Loss: 6082.7525
Epoch 111/200, Loss: 6082.7429
Epoch 112/200, Loss: 6082.7450
Epoch 113/200, Loss: 6082.6989
Epoch 114/200, Loss: 6082.7608
Epoch 115/200, Loss: 6082.7440
Epoch 116/200, Loss: 6082.7270
Epoch 117/200, Loss: 6082.7480
Epoch 118/200, Loss: 6082.7834
Epoch 119/200, Loss: 6082.7199
Epoch 120/200, Loss: 6082.7329
Epoch 121/200, Loss: 6082.7411
Epoch 122/200, Loss: 6082.7331
Epoch 123/200, Loss: 6082.7634
Epoch 124/200, Loss: 6082.7536
Epoch 125/200, Loss: 6082.7259
Epoch 126/200, Loss: 6082.7588
Epoch 127/200, Loss: 6082.8080
Epoch 128/200, Loss: 6082.7674
Epoch 129/200, Loss: 6082.7713
Epoch 130/200, Loss: 6082.7357
Epoch 131/200, Loss: 6082.7177
Epoch 132/200, Loss: 6082.7682
Epoch 133/200, Loss: 6082.7498
Epoch 134/200, Loss: 6082.7203
Epoch 135/200, Loss: 6082.7582
Epoch 136/200, Loss: 6082.7212
Epoch 137/200, Loss: 6082.7513
Epoch 138/200, Loss: 6082.7469
Epoch 139/200, Loss: 6082.8145
Epoch 140/200, Loss: 6082.7433
Epoch 141/200, Loss: 6082.7680
Epoch 142/200, Loss: 6082.7210
Epoch 143/200, Loss: 6082.7334
Epoch 144/200, Loss: 6082.7599
Epoch 145/200, Loss: 6082.7474
Epoch 146/200, Loss: 6082.7445
Epoch 147/200, Loss: 6082.7854
Epoch 148/200, Loss: 6082.7704
Epoch 149/200, Loss: 6082.7558
Epoch 150/200, Loss: 6082.7470
Epoch 151/200, Loss: 6082.7389
Epoch 152/200, Loss: 6082.7958
Epoch 153/200, Loss: 6082.7489
Epoch 154/200, Loss: 6082.7593
Epoch 155/200, Loss: 6082.7567
Epoch 156/200, Loss: 6082.7847
Epoch 157/200, Loss: 6082.7497
Epoch 158/200, Loss: 6082.7453
Epoch 159/200, Loss: 6082.7166
Epoch 160/200, Loss: 6082.7659
Epoch 161/200, Loss: 6082.7472
Epoch 162/200, Loss: 6082.7380
Epoch 163/200, Loss: 6082.7678
Epoch 164/200, Loss: 6082.7366
Epoch 165/200, Loss: 6082.7544
Epoch 166/200, Loss: 6082.7705
Epoch 167/200, Loss: 6082.7769
Epoch 168/200, Loss: 6082.7354
Epoch 169/200, Loss: 6082.7608
Epoch 170/200, Loss: 6082.7588
Epoch 171/200, Loss: 6082.7341
Epoch 172/200, Loss: 6082.7156
Epoch 173/200, Loss: 6082.7584
Epoch 174/200, Loss: 6082.7477
Epoch 175/200, Loss: 6082.7435
Epoch 176/200, Loss: 6082.7889
Epoch 177/200, Loss: 6082.7711
Epoch 178/200, Loss: 6082.7327
Epoch 179/200, Loss: 6082.7317
Epoch 180/200, Loss: 6082.7660
Epoch 181/200, Loss: 6082.8114
Epoch 182/200, Loss: 6082.7816
Epoch 183/200, Loss: 6082.6907
Epoch 184/200, Loss: 6082.7759
Epoch 185/200, Loss: 6082.7213
Epoch 186/200, Loss: 6082.7504
Epoch 187/200, Loss: 6082.7199
Epoch 188/200, Loss: 6082.7470
Epoch 189/200, Loss: 6082.7802
Epoch 190/200, Loss: 6082.7270
Epoch 191/200, Loss: 6082.7724
Epoch 192/200, Loss: 6082.7536
Epoch 193/200, Loss: 6082.7750
Epoch 194/200, Loss: 6082.7232
Epoch 195/200, Loss: 6082.7775
Epoch 196/200, Loss: 6082.7224
Epoch 197/200, Loss: 6082.7492
Epoch 198/200, Loss: 6082.7092
Epoch 199/200, Loss: 6082.7726
Epoch 200/200, Loss: 6082.7473
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 12.85% | Test Accuracy: 9.78%
Precision: 0.0329 | Recall: 0.0978 | F1-Score: 0.0449

Processing Subject 8...
Top 32 discriminative features: [26 38 46 44 14 12 18 42 41 16 40 29 22 45 30 63 61 28 59 57 49 51  2  0
 25 37 20 10 21  9 33  6]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 135546
	# per Class in Train Dataset:
		Class 0: 2001
		Class 1: 3690
		Class 2: 3618
		Class 3: 5400
		Class 4: 4111
		Class 5: 4083
		Class 6: 5400
		Class 7: 3828
		Class 8: 3561
		Class 9: 5131
		Class 10: 4280
		Class 11: 1736
		Class 12: 1730
		Class 13: 2320
		Class 14: 3004
		Class 15: 2607
		Class 16: 1742
		Class 17: 1753
		Class 18: 2132
		Class 19: 2904
		Class 20: 4813
		Class 21: 5285
		Class 22: 5179
		Class 23: 4313
		Class 24: 2647
		Class 25: 4290
		Class 26: 2393
		Class 27: 3435
		Class 28: 2931
		Class 29: 3232
		Class 30: 3570
		Class 31: 3419
		Class 32: 2733
		Class 33: 2202
		Class 34: 5353
		Class 35: 3263
		Class 36: 3683
		Class 37: 4325
		Class 38: 3449
	# of Testing Samples: 12543
	# per Class in Test Dataset:
		Class 1: 329
		Class 2: 327
		Class 3: 600
		Class 4: 395
		Class 5: 389
		Class 6: 600
		Class 7: 386
		Class 8: 328
		Class 9: 600
		Class 10: 430
		Class 11: 135
		Class 12: 210
		Class 13: 207
		Class 14: 273
		Class 15: 249
		Class 16: 165
		Class 17: 170
		Class 18: 211
		Class 19: 276
		Class 20: 453
		Class 21: 600
		Class 22: 600
		Class 23: 379
		Class 24: 256
		Class 25: 400
		Class 26: 161
		Class 27: 237
		Class 28: 245
		Class 29: 260
		Class 30: 345
		Class 31: 265
		Class 32: 241
		Class 33: 179
		Class 34: 495
		Class 35: 274
		Class 36: 228
		Class 37: 307
		Class 38: 338
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 70.21%
Precision: 0.7124 | Recall: 0.7021 | F1-Score: 0.6940


Running Deep Learning Classifier...
DataLoader: Training set - 2118 batches, Testing set - 196 batches
Epoch 1/200, Loss: 4833.6141
Epoch 2/200, Loss: 3035.4027
Epoch 3/200, Loss: 2485.5182
Epoch 4/200, Loss: 2138.2153
Epoch 5/200, Loss: 1901.9842
Epoch 6/200, Loss: 1738.4239
Epoch 7/200, Loss: 1715.5407
Epoch 8/200, Loss: 1697.7423
Epoch 9/200, Loss: 1680.5288
Epoch 10/200, Loss: 1664.1147
Epoch 11/200, Loss: 1649.0440
Epoch 12/200, Loss: 1646.2637
Epoch 13/200, Loss: 1644.5083
Epoch 14/200, Loss: 1642.8862
Epoch 15/200, Loss: 1641.2777
Epoch 16/200, Loss: 1639.5492
Epoch 17/200, Loss: 1639.3039
Epoch 18/200, Loss: 1639.1219
Epoch 19/200, Loss: 1638.9528
Epoch 20/200, Loss: 1638.8016
Epoch 21/200, Loss: 1638.5971
Epoch 22/200, Loss: 1638.5636
Epoch 23/200, Loss: 1638.5595
Epoch 24/200, Loss: 1638.5718
Epoch 25/200, Loss: 1638.5546
Epoch 26/200, Loss: 1638.5197
Epoch 27/200, Loss: 1638.5012
Epoch 28/200, Loss: 1638.5220
Epoch 29/200, Loss: 1638.5300
Epoch 30/200, Loss: 1638.5258
Epoch 31/200, Loss: 1638.5180
Epoch 32/200, Loss: 1638.5392
Epoch 33/200, Loss: 1638.5233
Epoch 34/200, Loss: 1638.5202
Epoch 35/200, Loss: 1638.5125
Epoch 36/200, Loss: 1638.5460
Epoch 37/200, Loss: 1638.5374
Epoch 38/200, Loss: 1638.5288
Epoch 39/200, Loss: 1638.5321
Epoch 40/200, Loss: 1638.5209
Epoch 41/200, Loss: 1638.5355
Epoch 42/200, Loss: 1638.5295
Epoch 43/200, Loss: 1638.5229
Epoch 44/200, Loss: 1638.5261
Epoch 45/200, Loss: 1638.5211
Epoch 46/200, Loss: 1638.5328
Epoch 47/200, Loss: 1638.5293
Epoch 48/200, Loss: 1638.5237
Epoch 49/200, Loss: 1638.5179
Epoch 50/200, Loss: 1638.5399
Epoch 51/200, Loss: 1638.5312
Epoch 52/200, Loss: 1638.5314
Epoch 53/200, Loss: 1638.5330
Epoch 54/200, Loss: 1638.5376
Epoch 55/200, Loss: 1638.5426
Epoch 56/200, Loss: 1638.5229
Epoch 57/200, Loss: 1638.5409
Epoch 58/200, Loss: 1638.5210
Epoch 59/200, Loss: 1638.5577
Epoch 60/200, Loss: 1638.5357
Epoch 61/200, Loss: 1638.5291
Epoch 62/200, Loss: 1638.5362
Epoch 63/200, Loss: 1638.5197
Epoch 64/200, Loss: 1638.5252
Epoch 65/200, Loss: 1638.5298
Epoch 66/200, Loss: 1638.5201
Epoch 67/200, Loss: 1638.5272
Epoch 68/200, Loss: 1638.5331
Epoch 69/200, Loss: 1638.5288
Epoch 70/200, Loss: 1638.5150
Epoch 71/200, Loss: 1638.5297
Epoch 72/200, Loss: 1638.5200
Epoch 73/200, Loss: 1638.5529
Epoch 74/200, Loss: 1638.5559
Epoch 75/200, Loss: 1638.5192
Epoch 76/200, Loss: 1638.5161
Epoch 77/200, Loss: 1638.5379
Epoch 78/200, Loss: 1638.5288
Epoch 79/200, Loss: 1638.5358
Epoch 80/200, Loss: 1638.5361
Epoch 81/200, Loss: 1638.5375
Epoch 82/200, Loss: 1638.5222
Epoch 83/200, Loss: 1638.5240
Epoch 84/200, Loss: 1638.5481
Epoch 85/200, Loss: 1638.5224
Epoch 86/200, Loss: 1638.5282
Epoch 87/200, Loss: 1638.5330
Epoch 88/200, Loss: 1638.5596
Epoch 89/200, Loss: 1638.5183
Epoch 90/200, Loss: 1638.5277
Epoch 91/200, Loss: 1638.5396
Epoch 92/200, Loss: 1638.5147
Epoch 93/200, Loss: 1638.5250
Epoch 94/200, Loss: 1638.5317
Epoch 95/200, Loss: 1638.5296
Epoch 96/200, Loss: 1638.5422
Epoch 97/200, Loss: 1638.5307
Epoch 98/200, Loss: 1638.5399
Epoch 99/200, Loss: 1638.5334
Epoch 100/200, Loss: 1638.5213
Epoch 101/200, Loss: 1638.5564
Epoch 102/200, Loss: 1638.5413
Epoch 103/200, Loss: 1638.5369
Epoch 104/200, Loss: 1638.5208
Epoch 105/200, Loss: 1638.5162
Epoch 106/200, Loss: 1638.5333
Epoch 107/200, Loss: 1638.5445
Epoch 108/200, Loss: 1638.5266
Epoch 109/200, Loss: 1638.5340
Epoch 110/200, Loss: 1638.5280
Epoch 111/200, Loss: 1638.5393
Epoch 112/200, Loss: 1638.5138
Epoch 113/200, Loss: 1638.5394
Epoch 114/200, Loss: 1638.5418
Epoch 115/200, Loss: 1638.5321
Epoch 116/200, Loss: 1638.5237
Epoch 117/200, Loss: 1638.5292
Epoch 118/200, Loss: 1638.5457
Epoch 119/200, Loss: 1638.5379
Epoch 120/200, Loss: 1638.5298
Epoch 121/200, Loss: 1638.5191
Epoch 122/200, Loss: 1638.5204
Epoch 123/200, Loss: 1638.5517
Epoch 124/200, Loss: 1638.5258
Epoch 125/200, Loss: 1638.5303
Epoch 126/200, Loss: 1638.5371
Epoch 127/200, Loss: 1638.5379
Epoch 128/200, Loss: 1638.5403
Epoch 129/200, Loss: 1638.5239
Epoch 130/200, Loss: 1638.5348
Epoch 131/200, Loss: 1638.5420
Epoch 132/200, Loss: 1638.5251
Epoch 133/200, Loss: 1638.5219
Epoch 134/200, Loss: 1638.5261
Epoch 135/200, Loss: 1638.5273
Epoch 136/200, Loss: 1638.5304
Epoch 137/200, Loss: 1638.5556
Epoch 138/200, Loss: 1638.5578
Epoch 139/200, Loss: 1638.5311
Epoch 140/200, Loss: 1638.5329
Epoch 141/200, Loss: 1638.5262
Epoch 142/200, Loss: 1638.5289
Epoch 143/200, Loss: 1638.5537
Epoch 144/200, Loss: 1638.5194
Epoch 145/200, Loss: 1638.5357
Epoch 146/200, Loss: 1638.5300
Epoch 147/200, Loss: 1638.5317
Epoch 148/200, Loss: 1638.5355
Epoch 149/200, Loss: 1638.5391
Epoch 150/200, Loss: 1638.5415
Epoch 151/200, Loss: 1638.5193
Epoch 152/200, Loss: 1638.5158
Epoch 153/200, Loss: 1638.5275
Epoch 154/200, Loss: 1638.5435
Epoch 155/200, Loss: 1638.5216
Epoch 156/200, Loss: 1638.5383
Epoch 157/200, Loss: 1638.5220
Epoch 158/200, Loss: 1638.5487
Epoch 159/200, Loss: 1638.5152
Epoch 160/200, Loss: 1638.5384
Epoch 161/200, Loss: 1638.5298
Epoch 162/200, Loss: 1638.5398
Epoch 163/200, Loss: 1638.5468
Epoch 164/200, Loss: 1638.5122
Epoch 165/200, Loss: 1638.5179
Epoch 166/200, Loss: 1638.5217
Epoch 167/200, Loss: 1638.5184
Epoch 168/200, Loss: 1638.5308
Epoch 169/200, Loss: 1638.5244
Epoch 170/200, Loss: 1638.5394
Epoch 171/200, Loss: 1638.5308
Epoch 172/200, Loss: 1638.5257
Epoch 173/200, Loss: 1638.5218
Epoch 174/200, Loss: 1638.5263
Epoch 175/200, Loss: 1638.5323
Epoch 176/200, Loss: 1638.5331
Epoch 177/200, Loss: 1638.5227
Epoch 178/200, Loss: 1638.5279
Epoch 179/200, Loss: 1638.5327
Epoch 180/200, Loss: 1638.5288
Epoch 181/200, Loss: 1638.5455
Epoch 182/200, Loss: 1638.5202
Epoch 183/200, Loss: 1638.5198
Epoch 184/200, Loss: 1638.5281
Epoch 185/200, Loss: 1638.5330
Epoch 186/200, Loss: 1638.5280
Epoch 187/200, Loss: 1638.5169
Epoch 188/200, Loss: 1638.5184
Epoch 189/200, Loss: 1638.5228
Epoch 190/200, Loss: 1638.5247
Epoch 191/200, Loss: 1638.5269
Epoch 192/200, Loss: 1638.5380
Epoch 193/200, Loss: 1638.5170
Epoch 194/200, Loss: 1638.5198
Epoch 195/200, Loss: 1638.5293
Epoch 196/200, Loss: 1638.5330
Epoch 197/200, Loss: 1638.5588
Epoch 198/200, Loss: 1638.5298
Epoch 199/200, Loss: 1638.5178
Epoch 200/200, Loss: 1638.5184
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 75.15% | Test Accuracy: 60.90%
Precision: 0.6162 | Recall: 0.6090 | F1-Score: 0.5976
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 14.14%
Precision: 0.1476 | Recall: 0.1414 | F1-Score: 0.1431


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2118 batches, Testing set - 196 batches
Epoch 1/200, Loss: 7080.7916
Epoch 2/200, Loss: 6608.9077
Epoch 3/200, Loss: 6485.4170
Epoch 4/200, Loss: 6420.1812
Epoch 5/200, Loss: 6382.0559
Epoch 6/200, Loss: 6329.9385
Epoch 7/200, Loss: 6325.9283
Epoch 8/200, Loss: 6322.4993
Epoch 9/200, Loss: 6319.3242
Epoch 10/200, Loss: 6316.2363
Epoch 11/200, Loss: 6310.8458
Epoch 12/200, Loss: 6310.1639
Epoch 13/200, Loss: 6309.7433
Epoch 14/200, Loss: 6309.4635
Epoch 15/200, Loss: 6309.2064
Epoch 16/200, Loss: 6308.5562
Epoch 17/200, Loss: 6308.4828
Epoch 18/200, Loss: 6308.4506
Epoch 19/200, Loss: 6308.3932
Epoch 20/200, Loss: 6308.3998
Epoch 21/200, Loss: 6308.2960
Epoch 22/200, Loss: 6308.3005
Epoch 23/200, Loss: 6308.2995
Epoch 24/200, Loss: 6308.2888
Epoch 25/200, Loss: 6308.2842
Epoch 26/200, Loss: 6308.2824
Epoch 27/200, Loss: 6308.2943
Epoch 28/200, Loss: 6308.2959
Epoch 29/200, Loss: 6308.2645
Epoch 30/200, Loss: 6308.2803
Epoch 31/200, Loss: 6308.2782
Epoch 32/200, Loss: 6308.2893
Epoch 33/200, Loss: 6308.2862
Epoch 34/200, Loss: 6308.2928
Epoch 35/200, Loss: 6308.2945
Epoch 36/200, Loss: 6308.2830
Epoch 37/200, Loss: 6308.2973
Epoch 38/200, Loss: 6308.2942
Epoch 39/200, Loss: 6308.2794
Epoch 40/200, Loss: 6308.2823
Epoch 41/200, Loss: 6308.3041
Epoch 42/200, Loss: 6308.2921
Epoch 43/200, Loss: 6308.2950
Epoch 44/200, Loss: 6308.2776
Epoch 45/200, Loss: 6308.2812
Epoch 46/200, Loss: 6308.3079
Epoch 47/200, Loss: 6308.2843
Epoch 48/200, Loss: 6308.2883
Epoch 49/200, Loss: 6308.2804
Epoch 50/200, Loss: 6308.2798
Epoch 51/200, Loss: 6308.2937
Epoch 52/200, Loss: 6308.2832
Epoch 53/200, Loss: 6308.2786
Epoch 54/200, Loss: 6308.2841
Epoch 55/200, Loss: 6308.2923
Epoch 56/200, Loss: 6308.2636
Epoch 57/200, Loss: 6308.2881
Epoch 58/200, Loss: 6308.2852
Epoch 59/200, Loss: 6308.2849
Epoch 60/200, Loss: 6308.2795
Epoch 61/200, Loss: 6308.2799
Epoch 62/200, Loss: 6308.2839
Epoch 63/200, Loss: 6308.2856
Epoch 64/200, Loss: 6308.2851
Epoch 65/200, Loss: 6308.2874
Epoch 66/200, Loss: 6308.2643
Epoch 67/200, Loss: 6308.2922
Epoch 68/200, Loss: 6308.2960
Epoch 69/200, Loss: 6308.2776
Epoch 70/200, Loss: 6308.2838
Epoch 71/200, Loss: 6308.2913
Epoch 72/200, Loss: 6308.2882
Epoch 73/200, Loss: 6308.2623
Epoch 74/200, Loss: 6308.2912
Epoch 75/200, Loss: 6308.2945
Epoch 76/200, Loss: 6308.2692
Epoch 77/200, Loss: 6308.2831
Epoch 78/200, Loss: 6308.2899
Epoch 79/200, Loss: 6308.2910
Epoch 80/200, Loss: 6308.2875
Epoch 81/200, Loss: 6308.2801
Epoch 82/200, Loss: 6308.2824
Epoch 83/200, Loss: 6308.2717
Epoch 84/200, Loss: 6308.2699
Epoch 85/200, Loss: 6308.2808
Epoch 86/200, Loss: 6308.2770
Epoch 87/200, Loss: 6308.2801
Epoch 88/200, Loss: 6308.2826
Epoch 89/200, Loss: 6308.2609
Epoch 90/200, Loss: 6308.2775
Epoch 91/200, Loss: 6308.2986
Epoch 92/200, Loss: 6308.2809
Epoch 93/200, Loss: 6308.2790
Epoch 94/200, Loss: 6308.2853
Epoch 95/200, Loss: 6308.2722
Epoch 96/200, Loss: 6308.2694
Epoch 97/200, Loss: 6308.2913
Epoch 98/200, Loss: 6308.2804
Epoch 99/200, Loss: 6308.2863
Epoch 100/200, Loss: 6308.2832
Epoch 101/200, Loss: 6308.2989
Epoch 102/200, Loss: 6308.2764
Epoch 103/200, Loss: 6308.2947
Epoch 104/200, Loss: 6308.2889
Epoch 105/200, Loss: 6308.2609
Epoch 106/200, Loss: 6308.2766
Epoch 107/200, Loss: 6308.2760
Epoch 108/200, Loss: 6308.2945
Epoch 109/200, Loss: 6308.2922
Epoch 110/200, Loss: 6308.2590
Epoch 111/200, Loss: 6308.2790
Epoch 112/200, Loss: 6308.2710
Epoch 113/200, Loss: 6308.2764
Epoch 114/200, Loss: 6308.2947
Epoch 115/200, Loss: 6308.2772
Epoch 116/200, Loss: 6308.2859
Epoch 117/200, Loss: 6308.2830
Epoch 118/200, Loss: 6308.3047
Epoch 119/200, Loss: 6308.2865
Epoch 120/200, Loss: 6308.2866
Epoch 121/200, Loss: 6308.2880
Epoch 122/200, Loss: 6308.2903
Epoch 123/200, Loss: 6308.3024
Epoch 124/200, Loss: 6308.2790
Epoch 125/200, Loss: 6308.2847
Epoch 126/200, Loss: 6308.2865
Epoch 127/200, Loss: 6308.2904
Epoch 128/200, Loss: 6308.2804
Epoch 129/200, Loss: 6308.2952
Epoch 130/200, Loss: 6308.2785
Epoch 131/200, Loss: 6308.2764
Epoch 132/200, Loss: 6308.2824
Epoch 133/200, Loss: 6308.2924
Epoch 134/200, Loss: 6308.2932
Epoch 135/200, Loss: 6308.2874
Epoch 136/200, Loss: 6308.2947
Epoch 137/200, Loss: 6308.2906
Epoch 138/200, Loss: 6308.2752
Epoch 139/200, Loss: 6308.2876
Epoch 140/200, Loss: 6308.2775
Epoch 141/200, Loss: 6308.2958
Epoch 142/200, Loss: 6308.2675
Epoch 143/200, Loss: 6308.2794
Epoch 144/200, Loss: 6308.2815
Epoch 145/200, Loss: 6308.2875
Epoch 146/200, Loss: 6308.2673
Epoch 147/200, Loss: 6308.2751
Epoch 148/200, Loss: 6308.2712
Epoch 149/200, Loss: 6308.2951
Epoch 150/200, Loss: 6308.2766
Epoch 151/200, Loss: 6308.2814
Epoch 152/200, Loss: 6308.2977
Epoch 153/200, Loss: 6308.3022
Epoch 154/200, Loss: 6308.2783
Epoch 155/200, Loss: 6308.2867
Epoch 156/200, Loss: 6308.2774
Epoch 157/200, Loss: 6308.2817
Epoch 158/200, Loss: 6308.2747
Epoch 159/200, Loss: 6308.2922
Epoch 160/200, Loss: 6308.2809
Epoch 161/200, Loss: 6308.2841
Epoch 162/200, Loss: 6308.2905
Epoch 163/200, Loss: 6308.2850
Epoch 164/200, Loss: 6308.2971
Epoch 165/200, Loss: 6308.2823
Epoch 166/200, Loss: 6308.2739
Epoch 167/200, Loss: 6308.2745
Epoch 168/200, Loss: 6308.2859
Epoch 169/200, Loss: 6308.3095
Epoch 170/200, Loss: 6308.2910
Epoch 171/200, Loss: 6308.2712
Epoch 172/200, Loss: 6308.2900
Epoch 173/200, Loss: 6308.2717
Epoch 174/200, Loss: 6308.2848
Epoch 175/200, Loss: 6308.2928
Epoch 176/200, Loss: 6308.2914
Epoch 177/200, Loss: 6308.2746
Epoch 178/200, Loss: 6308.2870
Epoch 179/200, Loss: 6308.2835
Epoch 180/200, Loss: 6308.3095
Epoch 181/200, Loss: 6308.2734
Epoch 182/200, Loss: 6308.2831
Epoch 183/200, Loss: 6308.2925
Epoch 184/200, Loss: 6308.2719
Epoch 185/200, Loss: 6308.2852
Epoch 186/200, Loss: 6308.3053
Epoch 187/200, Loss: 6308.2723
Epoch 188/200, Loss: 6308.2770
Epoch 189/200, Loss: 6308.2851
Epoch 190/200, Loss: 6308.3017
Epoch 191/200, Loss: 6308.2772
Epoch 192/200, Loss: 6308.2919
Epoch 193/200, Loss: 6308.2821
Epoch 194/200, Loss: 6308.2708
Epoch 195/200, Loss: 6308.2943
Epoch 196/200, Loss: 6308.2950
Epoch 197/200, Loss: 6308.2831
Epoch 198/200, Loss: 6308.3081
Epoch 199/200, Loss: 6308.2846
Epoch 200/200, Loss: 6308.2793
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 12.89% | Test Accuracy: 13.35%
Precision: 0.0590 | Recall: 0.1335 | F1-Score: 0.0695

Processing Subject 9...
Top 32 discriminative features: [26 38 46 44 14 12 18 42 16 41 40 30 22 29 28 63 61 45 59 57  2  0 49 51
 20  9 10 21 37 25 33  6]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 135138
	# per Class in Train Dataset:
		Class 0: 2001
		Class 1: 3768
		Class 2: 3617
		Class 3: 5400
		Class 4: 4056
		Class 5: 4155
		Class 6: 5400
		Class 7: 3766
		Class 8: 3585
		Class 9: 5173
		Class 10: 4348
		Class 11: 1694
		Class 12: 1817
		Class 13: 2321
		Class 14: 2974
		Class 15: 2583
		Class 16: 1751
		Class 17: 1802
		Class 18: 2165
		Class 19: 2911
		Class 20: 4857
		Class 21: 5301
		Class 22: 5179
		Class 23: 4288
		Class 24: 2649
		Class 25: 4221
		Class 26: 2298
		Class 27: 3325
		Class 28: 2870
		Class 29: 3177
		Class 30: 3584
		Class 31: 3410
		Class 32: 2738
		Class 33: 2191
		Class 34: 5258
		Class 35: 3248
		Class 36: 3594
		Class 37: 4250
		Class 38: 3413
	# of Testing Samples: 12951
	# per Class in Test Dataset:
		Class 1: 251
		Class 2: 328
		Class 3: 600
		Class 4: 450
		Class 5: 317
		Class 6: 600
		Class 7: 448
		Class 8: 304
		Class 9: 558
		Class 10: 362
		Class 11: 177
		Class 12: 123
		Class 13: 206
		Class 14: 303
		Class 15: 273
		Class 16: 156
		Class 17: 121
		Class 18: 178
		Class 19: 269
		Class 20: 409
		Class 21: 584
		Class 22: 600
		Class 23: 404
		Class 24: 254
		Class 25: 469
		Class 26: 256
		Class 27: 347
		Class 28: 306
		Class 29: 315
		Class 30: 331
		Class 31: 274
		Class 32: 236
		Class 33: 190
		Class 34: 590
		Class 35: 289
		Class 36: 317
		Class 37: 382
		Class 38: 374
Before applying Fisher LDA Projection...

Running Traditional Classifier...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 67.38%
Precision: 0.6861 | Recall: 0.6738 | F1-Score: 0.6692


Running Deep Learning Classifier...
DataLoader: Training set - 2112 batches, Testing set - 203 batches
Epoch 1/200, Loss: 4601.5803
Epoch 2/200, Loss: 2886.5759
Epoch 3/200, Loss: 2369.0466
Epoch 4/200, Loss: 2046.3838
Epoch 5/200, Loss: 1826.0351
Epoch 6/200, Loss: 1675.8492
Epoch 7/200, Loss: 1652.2011
Epoch 8/200, Loss: 1634.2851
Epoch 9/200, Loss: 1617.7241
Epoch 10/200, Loss: 1601.2578
Epoch 11/200, Loss: 1587.3100
Epoch 12/200, Loss: 1584.9447
Epoch 13/200, Loss: 1583.0412
Epoch 14/200, Loss: 1581.5936
Epoch 15/200, Loss: 1580.1095
Epoch 16/200, Loss: 1578.3672
Epoch 17/200, Loss: 1578.0709
Epoch 18/200, Loss: 1578.0085
Epoch 19/200, Loss: 1577.8234
Epoch 20/200, Loss: 1577.5099
Epoch 21/200, Loss: 1577.4429
Epoch 22/200, Loss: 1577.4918
Epoch 23/200, Loss: 1577.5017
Epoch 24/200, Loss: 1577.4465
Epoch 25/200, Loss: 1577.2637
Epoch 26/200, Loss: 1577.4459
Epoch 27/200, Loss: 1577.3541
Epoch 28/200, Loss: 1577.4928
Epoch 29/200, Loss: 1577.4271
Epoch 30/200, Loss: 1577.4493
Epoch 31/200, Loss: 1577.3424
Epoch 32/200, Loss: 1577.3138
Epoch 33/200, Loss: 1577.3622
Epoch 34/200, Loss: 1577.3739
Epoch 35/200, Loss: 1577.3709
Epoch 36/200, Loss: 1577.4146
Epoch 37/200, Loss: 1577.5191
Epoch 38/200, Loss: 1577.5437
Epoch 39/200, Loss: 1577.4005
Epoch 40/200, Loss: 1577.3910
Epoch 41/200, Loss: 1577.5555
Epoch 42/200, Loss: 1577.5281
Epoch 43/200, Loss: 1577.4310
Epoch 44/200, Loss: 1577.4911
Epoch 45/200, Loss: 1577.3188
Epoch 46/200, Loss: 1577.4794
Epoch 47/200, Loss: 1577.3997
Epoch 48/200, Loss: 1577.4299
Epoch 49/200, Loss: 1577.4102
Epoch 50/200, Loss: 1577.4767
Epoch 51/200, Loss: 1577.5177
Epoch 52/200, Loss: 1577.4135
Epoch 53/200, Loss: 1577.3072
Epoch 54/200, Loss: 1577.3576
Epoch 55/200, Loss: 1577.5186
Epoch 56/200, Loss: 1577.4715
Epoch 57/200, Loss: 1577.3675
Epoch 58/200, Loss: 1577.4304
Epoch 59/200, Loss: 1577.4312
Epoch 60/200, Loss: 1577.3475
Epoch 61/200, Loss: 1577.4514
Epoch 62/200, Loss: 1577.4292
Epoch 63/200, Loss: 1577.4141
Epoch 64/200, Loss: 1577.5591
Epoch 65/200, Loss: 1577.3159
Epoch 66/200, Loss: 1577.3090
Epoch 67/200, Loss: 1577.3603
Epoch 68/200, Loss: 1577.4348
Epoch 69/200, Loss: 1577.5135
Epoch 70/200, Loss: 1577.4107
Epoch 71/200, Loss: 1577.4483
Epoch 72/200, Loss: 1577.3821
Epoch 73/200, Loss: 1577.5176
Epoch 74/200, Loss: 1577.3805
Epoch 75/200, Loss: 1577.3999
Epoch 76/200, Loss: 1577.3462
Epoch 77/200, Loss: 1577.3913
Epoch 78/200, Loss: 1577.3784
Epoch 79/200, Loss: 1577.2802
Epoch 80/200, Loss: 1577.4606
Epoch 81/200, Loss: 1577.3213
Epoch 82/200, Loss: 1577.3402
Epoch 83/200, Loss: 1577.4285
Epoch 84/200, Loss: 1577.3388
Epoch 85/200, Loss: 1577.4628
Epoch 86/200, Loss: 1577.5316
Epoch 87/200, Loss: 1577.3692
Epoch 88/200, Loss: 1577.3537
Epoch 89/200, Loss: 1577.5051
Epoch 90/200, Loss: 1577.3938
Epoch 91/200, Loss: 1577.4450
Epoch 92/200, Loss: 1577.4776
Epoch 93/200, Loss: 1577.4871
Epoch 94/200, Loss: 1577.4506
Epoch 95/200, Loss: 1577.5040
Epoch 96/200, Loss: 1577.4320
Epoch 97/200, Loss: 1577.3549
Epoch 98/200, Loss: 1577.4361
Epoch 99/200, Loss: 1577.3045
Epoch 100/200, Loss: 1577.3569
Epoch 101/200, Loss: 1577.3586
Epoch 102/200, Loss: 1577.4513
Epoch 103/200, Loss: 1577.4895
Epoch 104/200, Loss: 1577.3977
Epoch 105/200, Loss: 1577.4723
Epoch 106/200, Loss: 1577.4531
Epoch 107/200, Loss: 1577.3837
Epoch 108/200, Loss: 1577.5471
Epoch 109/200, Loss: 1577.3756
Epoch 110/200, Loss: 1577.3318
Epoch 111/200, Loss: 1577.4017
Epoch 112/200, Loss: 1577.5490
Epoch 113/200, Loss: 1577.3806
Epoch 114/200, Loss: 1577.4592
Epoch 115/200, Loss: 1577.3475
Epoch 116/200, Loss: 1577.3261
Epoch 117/200, Loss: 1577.3682
Epoch 118/200, Loss: 1577.4945
Epoch 119/200, Loss: 1577.6187
Epoch 120/200, Loss: 1577.4660
Epoch 121/200, Loss: 1577.3599
Epoch 122/200, Loss: 1577.4814
Epoch 123/200, Loss: 1577.3828
Epoch 124/200, Loss: 1577.4456
Epoch 125/200, Loss: 1577.4591
Epoch 126/200, Loss: 1577.5672
Epoch 127/200, Loss: 1577.4387
Epoch 128/200, Loss: 1577.3347
Epoch 129/200, Loss: 1577.3191
Epoch 130/200, Loss: 1577.5986
Epoch 131/200, Loss: 1577.4469
Epoch 132/200, Loss: 1577.6035
Epoch 133/200, Loss: 1577.4057
Epoch 134/200, Loss: 1577.4153
Epoch 135/200, Loss: 1577.4009
Epoch 136/200, Loss: 1577.3100
Epoch 137/200, Loss: 1577.3825
Epoch 138/200, Loss: 1577.4020
Epoch 139/200, Loss: 1577.4946
Epoch 140/200, Loss: 1577.5742
Epoch 141/200, Loss: 1577.4061
Epoch 142/200, Loss: 1577.4822
Epoch 143/200, Loss: 1577.4578
Epoch 144/200, Loss: 1577.3399
Epoch 145/200, Loss: 1577.3436
Epoch 146/200, Loss: 1577.3589
Epoch 147/200, Loss: 1577.4743
Epoch 148/200, Loss: 1577.2709
Epoch 149/200, Loss: 1577.4968
Epoch 150/200, Loss: 1577.3473
Epoch 151/200, Loss: 1577.3841
Epoch 152/200, Loss: 1577.4088
Epoch 153/200, Loss: 1577.3375
Epoch 154/200, Loss: 1577.4856
Epoch 155/200, Loss: 1577.3161
Epoch 156/200, Loss: 1577.6072
Epoch 157/200, Loss: 1577.4915
Epoch 158/200, Loss: 1577.4546
Epoch 159/200, Loss: 1577.5007
Epoch 160/200, Loss: 1577.4510
Epoch 161/200, Loss: 1577.4541
Epoch 162/200, Loss: 1577.6137
Epoch 163/200, Loss: 1577.3944
Epoch 164/200, Loss: 1577.4915
Epoch 165/200, Loss: 1577.5038
Epoch 166/200, Loss: 1577.4140
Epoch 167/200, Loss: 1577.3773
Epoch 168/200, Loss: 1577.3651
Epoch 169/200, Loss: 1577.4526
Epoch 170/200, Loss: 1577.2839
Epoch 171/200, Loss: 1577.4922
Epoch 172/200, Loss: 1577.4107
Epoch 173/200, Loss: 1577.5245
Epoch 174/200, Loss: 1577.3054
Epoch 175/200, Loss: 1577.4228
Epoch 176/200, Loss: 1577.3936
Epoch 177/200, Loss: 1577.4121
Epoch 178/200, Loss: 1577.3836
Epoch 179/200, Loss: 1577.3789
Epoch 180/200, Loss: 1577.3336
Epoch 181/200, Loss: 1577.4458
Epoch 182/200, Loss: 1577.3422
Epoch 183/200, Loss: 1577.4044
Epoch 184/200, Loss: 1577.3070
Epoch 185/200, Loss: 1577.3332
Epoch 186/200, Loss: 1577.4879
Epoch 187/200, Loss: 1577.3087
Epoch 188/200, Loss: 1577.4123
Epoch 189/200, Loss: 1577.2806
Epoch 190/200, Loss: 1577.5180
Epoch 191/200, Loss: 1577.4936
Epoch 192/200, Loss: 1577.4089
Epoch 193/200, Loss: 1577.4316
Epoch 194/200, Loss: 1577.3574
Epoch 195/200, Loss: 1577.3751
Epoch 196/200, Loss: 1577.4966
Epoch 197/200, Loss: 1577.3808
Epoch 198/200, Loss: 1577.4869
Epoch 199/200, Loss: 1577.2535
Epoch 200/200, Loss: 1577.3565
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 76.41% | Test Accuracy: 49.87%
Precision: 0.5262 | Recall: 0.4987 | F1-Score: 0.4957
Applying Fisher LDA Projection...

Running Traditional Classifier with LDA...

--- traditional_classifier Classifier ---
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 100.00% | Test Accuracy: 15.26%
Precision: 0.1540 | Recall: 0.1526 | F1-Score: 0.1511


Running Deep Learning Classifier with LDA...
DataLoader: Training set - 2112 batches, Testing set - 203 batches
Epoch 1/200, Loss: 7260.2407
Epoch 2/200, Loss: 6654.3012
Epoch 3/200, Loss: 6391.2902
Epoch 4/200, Loss: 6128.7066
Epoch 5/200, Loss: 5954.7202
Epoch 6/200, Loss: 5817.2529
Epoch 7/200, Loss: 5805.1748
Epoch 8/200, Loss: 5793.9777
Epoch 9/200, Loss: 5783.1316
Epoch 10/200, Loss: 5773.2944
Epoch 11/200, Loss: 5758.5221
Epoch 12/200, Loss: 5756.8249
Epoch 13/200, Loss: 5755.8189
Epoch 14/200, Loss: 5754.8650
Epoch 15/200, Loss: 5753.9241
Epoch 16/200, Loss: 5752.3018
Epoch 17/200, Loss: 5752.0861
Epoch 18/200, Loss: 5751.8983
Epoch 19/200, Loss: 5751.8713
Epoch 20/200, Loss: 5751.7765
Epoch 21/200, Loss: 5751.5668
Epoch 22/200, Loss: 5751.5471
Epoch 23/200, Loss: 5751.4983
Epoch 24/200, Loss: 5751.4690
Epoch 25/200, Loss: 5751.5795
Epoch 26/200, Loss: 5751.4459
Epoch 27/200, Loss: 5751.4451
Epoch 28/200, Loss: 5751.5504
Epoch 29/200, Loss: 5751.5538
Epoch 30/200, Loss: 5751.5069
Epoch 31/200, Loss: 5751.4395
Epoch 32/200, Loss: 5751.5338
Epoch 33/200, Loss: 5751.4996
Epoch 34/200, Loss: 5751.4860
Epoch 35/200, Loss: 5751.5639
Epoch 36/200, Loss: 5751.5790
Epoch 37/200, Loss: 5751.5265
Epoch 38/200, Loss: 5751.4408
Epoch 39/200, Loss: 5751.6337
Epoch 40/200, Loss: 5751.4737
Epoch 41/200, Loss: 5751.5045
Epoch 42/200, Loss: 5751.5408
Epoch 43/200, Loss: 5751.6042
Epoch 44/200, Loss: 5751.4657
Epoch 45/200, Loss: 5751.3879
Epoch 46/200, Loss: 5751.4960
Epoch 47/200, Loss: 5751.6424
Epoch 48/200, Loss: 5751.5228
Epoch 49/200, Loss: 5751.5750
Epoch 50/200, Loss: 5751.4635
Epoch 51/200, Loss: 5751.5849
Epoch 52/200, Loss: 5751.5668
Epoch 53/200, Loss: 5751.5819
Epoch 54/200, Loss: 5751.4507
Epoch 55/200, Loss: 5751.5050
Epoch 56/200, Loss: 5751.5052
Epoch 57/200, Loss: 5751.6958
Epoch 58/200, Loss: 5751.5663
Epoch 59/200, Loss: 5751.4777
Epoch 60/200, Loss: 5751.4023
Epoch 61/200, Loss: 5751.6218
Epoch 62/200, Loss: 5751.5785
Epoch 63/200, Loss: 5751.5839
Epoch 64/200, Loss: 5751.5353
Epoch 65/200, Loss: 5751.5711
Epoch 66/200, Loss: 5751.5722
Epoch 67/200, Loss: 5751.4127
Epoch 68/200, Loss: 5751.5763
Epoch 69/200, Loss: 5751.3892
Epoch 70/200, Loss: 5751.5569
Epoch 71/200, Loss: 5751.5398
Epoch 72/200, Loss: 5751.5616
Epoch 73/200, Loss: 5751.5657
Epoch 74/200, Loss: 5751.5564
Epoch 75/200, Loss: 5751.4731
Epoch 76/200, Loss: 5751.6165
Epoch 77/200, Loss: 5751.5035
Epoch 78/200, Loss: 5751.5031
Epoch 79/200, Loss: 5751.6408
Epoch 80/200, Loss: 5751.4363
Epoch 81/200, Loss: 5751.5837
Epoch 82/200, Loss: 5751.5731
Epoch 83/200, Loss: 5751.4599
Epoch 84/200, Loss: 5751.5132
Epoch 85/200, Loss: 5751.5516
Epoch 86/200, Loss: 5751.5979
Epoch 87/200, Loss: 5751.4498
Epoch 88/200, Loss: 5751.4966
Epoch 89/200, Loss: 5751.3929
Epoch 90/200, Loss: 5751.5222
Epoch 91/200, Loss: 5751.6454
Epoch 92/200, Loss: 5751.5429
Epoch 93/200, Loss: 5751.4876
Epoch 94/200, Loss: 5751.4596
Epoch 95/200, Loss: 5751.6266
Epoch 96/200, Loss: 5751.5674
Epoch 97/200, Loss: 5751.4591
Epoch 98/200, Loss: 5751.6267
Epoch 99/200, Loss: 5751.5359
Epoch 100/200, Loss: 5751.5114
Epoch 101/200, Loss: 5751.4795
Epoch 102/200, Loss: 5751.4780
Epoch 103/200, Loss: 5751.4163
Epoch 104/200, Loss: 5751.6565
Epoch 105/200, Loss: 5751.5396
Epoch 106/200, Loss: 5751.5056
Epoch 107/200, Loss: 5751.5063
Epoch 108/200, Loss: 5751.4751
Epoch 109/200, Loss: 5751.5326
Epoch 110/200, Loss: 5751.5061
Epoch 111/200, Loss: 5751.5686
Epoch 112/200, Loss: 5751.5792
Epoch 113/200, Loss: 5751.5206
Epoch 114/200, Loss: 5751.4400
Epoch 115/200, Loss: 5751.4752
Epoch 116/200, Loss: 5751.4339
Epoch 117/200, Loss: 5751.5978
Epoch 118/200, Loss: 5751.4673
Epoch 119/200, Loss: 5751.5284
Epoch 120/200, Loss: 5751.5302
Epoch 121/200, Loss: 5751.5154
Epoch 122/200, Loss: 5751.5144
Epoch 123/200, Loss: 5751.5379
Epoch 124/200, Loss: 5751.6278
Epoch 125/200, Loss: 5751.5433
Epoch 126/200, Loss: 5751.6419
Epoch 127/200, Loss: 5751.6472
Epoch 128/200, Loss: 5751.3022
Epoch 129/200, Loss: 5751.5158
Epoch 130/200, Loss: 5751.6741
Epoch 131/200, Loss: 5751.6629
Epoch 132/200, Loss: 5751.5317
Epoch 133/200, Loss: 5751.4581
Epoch 134/200, Loss: 5751.5608
Epoch 135/200, Loss: 5751.5169
Epoch 136/200, Loss: 5751.5141
Epoch 137/200, Loss: 5751.5548
Epoch 138/200, Loss: 5751.5407
Epoch 139/200, Loss: 5751.5070
Epoch 140/200, Loss: 5751.6114
Epoch 141/200, Loss: 5751.5803
Epoch 142/200, Loss: 5751.5047
Epoch 143/200, Loss: 5751.5437
Epoch 144/200, Loss: 5751.5204
Epoch 145/200, Loss: 5751.6575
Epoch 146/200, Loss: 5751.5937
Epoch 147/200, Loss: 5751.4908
Epoch 148/200, Loss: 5751.6065
Epoch 149/200, Loss: 5751.4587
Epoch 150/200, Loss: 5751.5626
Epoch 151/200, Loss: 5751.5864
Epoch 152/200, Loss: 5751.4903
Epoch 153/200, Loss: 5751.6126
Epoch 154/200, Loss: 5751.5055
Epoch 155/200, Loss: 5751.5309
Epoch 156/200, Loss: 5751.6144
Epoch 157/200, Loss: 5751.5009
Epoch 158/200, Loss: 5751.5601
Epoch 159/200, Loss: 5751.6316
Epoch 160/200, Loss: 5751.5742
Epoch 161/200, Loss: 5751.5047
Epoch 162/200, Loss: 5751.5227
Epoch 163/200, Loss: 5751.5105
Epoch 164/200, Loss: 5751.4999
Epoch 165/200, Loss: 5751.5296
Epoch 166/200, Loss: 5751.5781
Epoch 167/200, Loss: 5751.5262
Epoch 168/200, Loss: 5751.6126
Epoch 169/200, Loss: 5751.6130
Epoch 170/200, Loss: 5751.5477
Epoch 171/200, Loss: 5751.7275
Epoch 172/200, Loss: 5751.4651
Epoch 173/200, Loss: 5751.5564
Epoch 174/200, Loss: 5751.4754
Epoch 175/200, Loss: 5751.5438
Epoch 176/200, Loss: 5751.5362
Epoch 177/200, Loss: 5751.5332
Epoch 178/200, Loss: 5751.6077
Epoch 179/200, Loss: 5751.6153
Epoch 180/200, Loss: 5751.6070
Epoch 181/200, Loss: 5751.4229
Epoch 182/200, Loss: 5751.4797
Epoch 183/200, Loss: 5751.5817
Epoch 184/200, Loss: 5751.6022
Epoch 185/200, Loss: 5751.5484
Epoch 186/200, Loss: 5751.6226
Epoch 187/200, Loss: 5751.4040
Epoch 188/200, Loss: 5751.3849
Epoch 189/200, Loss: 5751.4216
Epoch 190/200, Loss: 5751.4417
Epoch 191/200, Loss: 5751.5953
Epoch 192/200, Loss: 5751.4481
Epoch 193/200, Loss: 5751.5439
Epoch 194/200, Loss: 5751.5582
Epoch 195/200, Loss: 5751.4501
Epoch 196/200, Loss: 5751.5793
Epoch 197/200, Loss: 5751.4843
Epoch 198/200, Loss: 5751.4841
Epoch 199/200, Loss: 5751.5448
Epoch 200/200, Loss: 5751.4838
C:\Users\mqm7099\.conda\envs\tl_env\lib\site-packages\sklearn\metrics\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Train Accuracy: 15.74% | Test Accuracy: 13.13%
Precision: 0.0909 | Recall: 0.1313 | F1-Score: 0.0899

Processing Subject 10...
Top 32 discriminative features: [26 38 46 44 14 12 41 42 18 40 16 29 30 45 22 28 63 61 59 57  9  2 49  0
 51 37 25 21 10 20  6  4]

Dataset Loaded: Taiji_dataset_300.csv
	# of Classes: 39
	# of Features after Selection: 32
	# of Training Samples: 131465
	# per Class in Train Dataset:
		Class 0: 1763
		Class 1: 3701
		Class 2: 3455
		Class 3: 5400
		Class 4: 4046
		Class 5: 3979
		Class 6: 5400
		Class 7: 3841
		Class 8: 3350
		Class 9: 5131
		Class 10: 4110
		Class 11: 1643
		Class 12: 1706
		Class 13: 2196
		Class 14: 2923
		Class 15: 2551
		Class 16: 1671
		Class 17: 1713
		Class 18: 2071
		Class 19: 2857
		Class 20: 4666
		Class 21: 5285
		Class 22: 5179
		Class 23: 4218
		Class 24: 2612
		Class 25: 4115




======= FINAL AVERAGE METRICS FOR Traditional Classifier w/o LDA =======
Metric         Mean      Std Dev
========================================
train_acc      100.0000  0.0000
test_acc       66.2605  5.5316
precision      0.6951  0.0617
recall         0.6626  0.0553
f1             0.6510  0.0596

======= FINAL AVERAGE METRICS FOR Deep Learning Classifier w/o LDA =======
Metric         Mean      Std Dev
========================================
train_acc      78.8203  3.5971
test_acc       59.9726  8.5448
precision      0.6136  0.0821
recall         0.5997  0.0854
f1             0.5846  0.0850

======= FINAL AVERAGE METRICS FOR Traditional Classifier with LDA =======
Metric         Mean      Std Dev
========================================
train_acc      99.9987  0.0013
test_acc       18.4723  4.9988
precision      0.1851  0.0499
recall         0.1847  0.0500
f1             0.1804  0.0472

======= FINAL AVERAGE METRICS FOR Deep Learning Classifier with LDA =======
Metric         Mean      Std Dev
========================================
train_acc      20.3725  6.1009
test_acc       18.2310  5.2664
precision      0.1259  0.0559
recall         0.1823  0.0527
f1             0.1268  0.0509

Classification process completed. Logs are saved in classification_log.txt  



